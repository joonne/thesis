\documentclass[main.tex]{thesis.tex}
\begin{document}

\chapter{Teoria}

Seuraava kappale kuvailee matriisin tekijöihinjakoa yleisellä tasolla sekä vuorottelevien pienimpien neliöiden (Alternating Least Squares, ALS) -algoritmia.
Työssä tarkasteltava Spark-sovelluskehys sisältää toteutuksen ALS-algoritmille, joka on matriisin tekijöihinjako-algoritmi. \cite{ryza15}

\section{Matriisin tekijöihinjako}

Matriisin tekijöihinjaossa matriisi hajoitetaan pienempien matriisien tuloksi.
Tekijöihinjako voidaan toteuttaa usealla eri tavalla.
Matriisin tekijöihinjako kuuluu suureen algoritmien luokkaan nimeltä latenttien tekijöiden mallit (Latent-factor models).
Latenttien tekijöiden mallit yrittävät selittää usean käyttäjän ja tuotteen välillä havaittuja vuorovaikutuksia käyttämällä suhteellisen pientä määrää piileviä, latentteja tekijöitä.
Voidaan esimerkiksi yrittää selittää miksi ihminen ostaisi tietyn albumin lukemattomien mahdolisuuksien joukosta kuvailemalla käyttäjiä ja tuotteita mieltymysten perusteella, joista ei ole mahdollista saada tietoa. \cite{ryza15}
Latenttia tekijää ei ole mahdollista tarkastella sellaisenaan.
Esimerkiksi ihmisen terveys on latentti tekijä, sillä sitä ei ole mahdollista mitata kuten esimerkiksi verenpainetta.

\begin{figure}[h]
	\label{matrix-factorization}
	\caption{Matrisin tekijöihinjako \cite{ryza15}}
	\centering
	\includegraphics[scale=0.8]{matrix_factorization}
\end{figure}

Matriisin tekijöihinjako-algoritmit käsittelevät käyttäjä- ja tuotetietoja suurena matriisina $A$.
Jokainen rivissä $i$ sekä sarakkeessa $j$ sijaitseva alkio esittää arvostelua, jonka käyttäjä on antanut tietylle tuotteelle. \cite{ryza15}

Yleensä $A$ on harva (sparse), jolla tarkoitetaan että useimmat $A$:n alkiot sisältävät arvon nolla.
Tämä johtuu siitä, että kaikista mahdollisuuksista usein vain muutama käyttäjä-tuote-kombinaatio on olemassa. \cite{ryza15}

Matriisin tekijöihinjako mallintaa $A$:n kahden pienemmän matriisin $X$ ja $Y$ tulona, jotka ovat varsin pieniä.
Koska $A$:ssa on monta riviä ja saraketta, $X$ ja $Y$ sisältävät paljon rivejä mutta vain muutaman $(k)$ sarakkeen.
Nämä $k$ saraketta vastaavat latentteja tekijöitä, joita käytetään kuvailemaan datassa sijaitsevia vuorovaikutuksia.
Hajotelma (factorization) on ainoastaan arvio, sillä $k$ on pieni. \cite{ryza15}

Tavanomainen lähestymistapa matriisin tekijöihinjakoon perustuvassa yhteisöllisessä suodatuksessa on kohdella käyttäjä-tuote matriisin alkioita käyttäjien antamina eksplisiittisinä arvosteluina.
Eksplisiittistä tietoa on esimerkiksi käyttäjän antama arvio tuotteelle.
Spark ALS kykenee käsittelemään sekä implisiittistä että eksplisiittistä tietoa.
Implisiittistä tietoa on esimerkiksi sivujen katselukerrat tai tieto siitä, onko käyttäjä kuunnellut tiettyä artistia.
\cite{spark14} \cite{ryza15}

Usein monissa tosielämän käyttötapauksissa on käytettävissä ainoastaan implisiittistä tietoa, kuten katselukerrat, klikkaukset, ostokset, tykkäykset tai jakamiset.
Spark MLlib kohtelee tietoa lukuina, jotka esittävät havaintojen vahvuutta, kuten klikkausten määrä tai kumulatiivinen aika, joka käytetään elokuvan katseluun.
MLlib ei siis yritä mallintaa arviomatriisia suoraan.
Ekplisiittisten arvioiden sijaan, nämä numerot liittyvät havaittujen käyttäjämieltymysten varmuuteen.
Tämän tiedon perusteella malli koettaa etsiä latentteja tekijöitä, joiden avulla voidaan ennustaa käyttäjän arvio tuotteelle. \cite{spark14}

Näihin algoritmeihin viitataan joskus matriisin täyttö (matrix completion) -algoritmeina.
Tämä johtuu siitä, että alkuperäinen matriisi $A$ saattaa olla harva vaikka matriisitulo $XY^T$ on tiheä.
Vaikka tulosmatriisi sisältää arvon kaikille alkioille, se on kuitenkin vain likiarvo $A$:sta. \cite{ryza15}

\subsection{Alternating Least Squares (ALS)}

Yhteisöllistä suodatusta käytetään usein suosittelijajärjestelmissä.
Nämä tekniikat pyrkivät täyttämään käyttäjä-tuote-assosiaatiomatriisin puuttuvat kohdat.
Spark MLlib tukee mallipohjaista yhteisösuodatusta, jossa käyttäjiä ja tuotteita kuvaillaan pienellä määrällä latentteja tekijöitä, joita voidaan käyttää puuttuvien kohtien ennustamiseen.
Spark MLlib käyttää \textit{vuorottelevien pienimpien neliöiden} (Alternating Least Squares, ALS) -algoritmia näiden latenttien tekijöiden oppimiseen. \cite{spark14}

Spark ALS yrittää arvata arvostelumatriisin $A$ kahden alemman dimension matriisin, $X$ ja $Y$, tulona. \cite{als14}

\begin{equation}
A = XY^T
\end{equation}

Tyypillisesti näihin arvioihin viitataan tekijämatriiseina.
Perinteinen lähestymistapa on iteratiivinen.
Jokaisen iteraation aikana, toista tekijämatriisia pidetään vakiona ja toinen ratkaistaan käyttäen \textit{pienimpien summien} -algoritmia.
Pienimpien summien algoritmeja käsitellään aliluvussa \ref{rmse}.
Juuri ratkaistua tekijämatriisia pidetään vuorostaan vakiona kun ratkaistaan toista tekijämatriisia. \cite{als14}
Spark ALS mahdollistaa massiivisen rinnakkaistamisen, sillä algoritmia voidaan suorittaa rinnakkain, toisistaan erillään.
Tämä on erinomainen ominaisuus suuren mittakaavan (large-scale) laskenta-algoritmille. \cite{ryza15}

Spark ALS on lohkotettu versio ALS tekijöihinjako-algoritmista.
Ajatuksena on ryhmittää kaksi tekijäryhmää, $käyttäjät$ ja $tuotteet$, lohkoihin.
Ryhmittämistä seuraa kommunikaation vähentäminen lähettämällä jokaiseen tuotelohkoon vain yksi kopio jokaisesta käyttäjävektorista iteraation aikana.
Vain ne käyttäjä-vektorit lähetetään, joita tarvitaan tuotelohkoissa.
Vähennetty kommunikaatio saavutetaan laskemalla valmiiksi joitain tietoja suositusmatriisista, jotta voidaan päätellä jokaisen käyttäjän ulostulot ja jokaisen tuotteen sisääntulot.
Ulostulolla tarkoitetaan niitä tuotelohkoja, joihin käyttäjä tulee myötävaikuttamaan.
Sisääntulolla tarkoitetaan niitä ominaisuusvektoreita jotka jokainen tuote ottaa vastaan niiltä käyttäjälohkoilta joista ne ovat riippuvaisia.
Tämä mahdollistaa sen, että voidaan lähettää vain taulukollinen ominaisuusvektoreita jokaisen käyttäjä- ja tuotelohkon välillä.
Vastaavasti tuotelohko löytää käyttäjän arviot ja päivittää tuotteita näiden viestien perusteella. \cite{als14}

Sen sijaan, että etsittäisiin alemman dimension arviot suositusmatriisille $A$, etsitäänkin arviot mieltymysmatriisi $P$:lle, jossa $P$:n alkiot saavat arvon 1 kun $r > 0$ ja arvon 0 kun $r< = 0$.
Eksplisiittisen tuotearvion sijaan arvostelut kuvaavat käyttäjän mieltymyksen ($r$, rating) vahvuuden luottamusarvoa. \cite{als14}

\begin{equation}
A_iY(Y^T Y)^{-1} = X_i
\end{equation}

ALS operoi kiinnittämällä yhden tuntemattomista $u_i$ ja $v_j$ ja vaihtelemalla tätä kiinnittämistä.
Kun toinen on kiinnitetty, toinen voidaan laskea ratkaisemalla pienimpien neliöiden ongelma.
Tämä lähestymistapa on hyödyllinen, koska se muuttaa aiemman, ei-konveksin, ongelman neliömäiseksi, jolloin se voidaan ratkaista optimaalisesti. \cite{aberger14} Ei-konveksilla tarkoitetaan sellaista ongelmaa, jolla saattaa olla olemassa useita paikallisia ratkaisuja ja saattaa kestää kauan tunnistaa, onko ongelmalla ratkaisua lainkaan, tai että löydetty ratkaisu on myös globaali ratkaisu. \cite{non_convex}
Alla on \cite{aberger14} mukainen yleinen kuvaus ALS-algoritmista.
Esimerkissä mainittu RMSE-funktio esitellään tarkemmin aliluvussa \ref{rmse}.

\begin{lstlisting}[caption=ALS-algoritmin kuvaus \cite{aberger14}]

1. Alusta matriisi V.

2. Kiinnitä V, ratkaise U minimoimalla RMSE-funktio.

3. Kiinnitä U, ratkaise V minimoimalla RMSE-funktio.

4. Toista askeleita 2 ja 3 konvergenssiin asti.

\end{lstlisting}

Matriisi V alustetaan asettamalla ensimmäiseksi riviksi arvioitavan kohteen keskimääräinen arvio ja pieni satunnaisluku jäljelläoleviin alkioihin.
Konvergenssilla tarkoitetaan jonkin ilmiön lähestymistä ajan kuluessa jotain tiettyä arvoa, tässä tapauksessa sitä, että RMSE ei enää pienene tarpeeksi.

\subsection{RMSE}
\label{rmse}
RMSE (Root Mean Square Error) on kenties suosituin ennustettujen arvosteluiden tarkkuuden evaluointiin käytetty metriikka.
Sitä käytetään yleisesti regressioalgoritmien avulla luotujen mallien evaluointiin.
Regressioalgoritmien yhteydessä virheellä tarkoitetaan havainnon todellisen sekä ennustetun numeroarvon välistä eroa.
RMSE:n tuntemiseksi tulee tuntea ensin MSE (Mean Square Error).
Kuten nimi viittaa, MSE on virheiden neliöiden keskiarvo ja se voidaan laskea neliöimällä jokaisen havainnon virhe ja laskemalla virheiden neliöiden keskiarvo.
RMSE voidaan puolestaan laskea ottamalla neliöjuuri MSE:stä.
Sekä RMSE että MSE edustavat opetusvirhettä ja ne ilmoittavat kuinka hyvin malli sovittuu opetusdataan.
Niiden avulla saadaan selville havaintojen sekä ennustettujen arvojen välinen poikkeavuus.
Alhaisemman MSE:n tai RMSE:n omaavan mallin sanotaan sovittuvan paremmin opetusdataan kuin korkeammat virhearvot omaavan mallin. \cite{guller15}

Suosittelujärjestelmä luo ennustettuja arvosteluita $\hat{r}_{ui}$ testiaineistolle $\tau$ käyttäjä-tuote pareja $(u,i)$ joille todelliset arviot $r$ tunnetaan. \cite{guller15}
Ennustettujen ja todellisten arvioiden välinen RMSE saadaan laskettua seuraavasti:

\begin{equation}
RMSE=\sqrt{\frac{1}{|\tau|} \sum_{(u,i)\in\tau}(\hat{r}_{ui}-r_{ui})^2}
\end{equation}

\section{Amazon Web Services (AWS)}

\textit{AWS (Amazon Web Services)} on Amazonin tarjoama kokoelma pilvilaskentaan (cloud computing) tarkoitettuja tai sitä avustavia palveluita.
Tässä työssä käytetään pääasiassa hyödyksi kahta AWS:n palvelua, EMR:ää (Elastic Map Reduce) sekä S3:a (Simple Storage Service).
EMR:n avulla on mahdollista käyttää Big Data sovelluskehyksiä, kuten Apache Sparkia.
S3 on skaalautuva tietovarasto internetin tarpeisiin, jonka tarkoitus on helpottaa ohjelmistokehittäjien elämää.
\cite{aws} \cite{emr} \cite{s3}

\subsection{Elastic Map Reduce (EMR)}
\label{emr}

Amazon EMR tarjoaa hallitun klusterialustan (managed cluster platform), joka mahdollistaa suurten datamäärien prosessoinnin.
EMR:ssä on mahdollista ajaa Apache Spark:ia ja se on kyvykäs liikuttelemaan suuria datamääriä AWS:n tietovarastoista, kuten S3:sta, ulos ja sisään.
EMR käyttää hyväkseen dynaamisesti skaalautuvia Amazon EC2 (Elastic Compute Cloud) instansseja.
Niiden tarkoituksena on tehdä prosessoinnista helppoa, nopeaa ja kustannustehokasta. \cite{emr}

Klusteri on Amazon EMR:n keskeisin komponentti.
Klusteri on kokoelma Amazon Elastic Compute Cloud (Amazon EC2) instansseja.
Jokaista instanssia klusterissa kutsutaan solmuksi.
Jokaisella solmulla on rooli eli tyyppi klusterin sisällä.
Amazon EMR asentaa erilaisia sovelluskomponentteja jokaiseen solmuun, antaen jokaiselle solmulle roolin hajautetussa sovelluksessa kuten Apache Spark. \cite{emr}

Amazon EMR:ssä on seuraavanlaisia solmuja: Master, Core ja Task.
Master on solmu, joka tarkkailee tehtävien ja klusterin tilaa ajamalla sovelluskomponentteja, jotka ovat vastuussa datan ja tehtävien jakamisesta Slave-solmuille.
Core on Slave-solmu, jossa sijaitsee sovelluskompontteja, jotka ajavat tehtäviä ja tallentavat dataa HDFS:ään klusterissa.
Task on Slave-solmu, jossa sijaitsee sovelluskomponentteja, jotka ajavat vain tehtäviä, Task-solmujen määrittäminen on vapaaehtoista. \cite{emr}

%\begin{figure}[h]
%	\caption{EMR-klusteri \cite{emr}}
%	\centering
%	\includegraphics[scale=0.8]{cluster-node-types}
%\end{figure}

%Yllä oleva kuva esittää klusterin, jossa on master-solmu sekä neljä slave-solmua.

\subsection{Simple Storage Service (S3)}

Amazon S3 (Simple Storage Service) on tietovarasto, joka tarjoaa yksinkertaisen rajapinnan, jonka avulla voidaan tallettaa tai noutaa minkä verran dataa tahansa, milloin vain ja mistä tahansa webissä.
Se on suunniteltu tekemään web-mittakaavan (web-scale) laskennasta yksinkertaisempaa kehittäjille.
Se antaa kehittäjille pääsyn samaan hyvin skaalautuvaan, luotettavaan, nopeaan ja edulliseen tietovarasto-infrastruktuuriin, jota Amazon käyttää itse globaalin verkkosivustokatraansa ajamiseen. \cite{s3}

Tässä työssä S3:a käytetään varastoimaan opetusdataa.
Tarvittaessa Apache Sparkin laskennan välivaiheita voidaan tallentaa muistiin jolloin niiden tulokset voitaisiin lukea suoraan ilman että niitä tarvitsee laskea uudelleen.
Myös koko opetettu malli voitaisiin tallentaa S3:een ja esimerkiksi ladata vaikka omalle koneelle, jolloin AWS:ää tulisi myös käytettyä optimaalisesti, sillä EC2 instanssien varaaminen aiheuttaa maksuja koko ajan.

\section{Scala}

\lstset{
	columns=flexible,
	breaklines=true,
	tabsize=2,
	language=Scala,
	commentstyle=\color{scalacomment},
	keywordstyle=\color{blue},
	numbers=left,
	numbersep=15pt,
}

Scala on moniparadigmainen ohjelmointikieli, joka tukee sekä olio- että funktionaalista ohjelmointia.
Funktionaalista ohjelmointia varten Scalasta löytyy tuki funktionaalisen ohjelmoinnin käsitteille, kuten muuttumattomat tietorakenteet ja funktiot ensimmäisen luokan kansalaisina.
Olio-ohjelmointia varten Scalasta löytyy tuki käsitteille kuten luokat, oliot ja piirre (trait).
Scala tukee myös kapselointia, perintää, moniperintää ja muita tärkeitä olio-ohjelmoinnin konsepteja.
Scala on staattisesti tyypitetty kieli ja sillä kirjoitetut ohjelmat käännetään Scala-kääntäjää käyttäen.
Scala on JVM-perustainen (Java Virtual Machine, Java-virtuaalikone) kieli, joten Scala kääntäjä kääntää sovelluksen Java-tavukoodiksi, joka voidaan ajaa missä tahansa Java-virtuaalikoneessa.
Tavukooditasolla Scala ohjelmaa ei voida erottaa Java sovelluksesta.
Scalan ollessa JVM-perustainen, Scala on täysin yhteensopiva Javan kanssa ja näin ollen Java-kirjastoja voidaan käyttää suoraan Scala-koodissa.
Tästä syystä Scala-sovellukset hyötyvät suuresta Java-koodin määrästä.
Vaikka Scala tukee sekä olio- että funktionaalista ohjelmointia, funktionaalista ohjelmointia suositaan. \cite{guller15}

\subsection{Perustyypit}

Scalan perustyypit numeroiden esittämiseen ovat Byte, Short, Int, Long, Float ja Double.
Lisäksi Scalassa on perustyypit Char, String ja Boolean.
Char on 16 bittinen etumerkitön Unicode-merkki.
String on jono Char:eja.
Boolean esittää totuusarvoa tosi (true) tai epätosi (false). \cite{guller15}

Javasta poiketen Scalassa ei ole ollenkaan primitiivisiä tyyppejä vaan jokainen tyyppi on toteutettu luokkana.
Käännöksen aikana kääntäjä tarvittaessa automaattisesti muuntaa Scala tyypit Javan primitiivisiksi tyypeiksi. \cite{guller15}

\subsection{Muuttujat}

Scalassa on kahdentyyppisiä muuttujia: muuttuvia ja vakioita.
Muuttuva muuttuja määritellään avainsanan $var$ avulla.
Muuttuvaa muuttujaa ei voida asettaa uudelleen luomisen jälkeen.
Var:ien käyttöä ei suositella, mutta joskus niiden käyttämisellä saadaan aikaan yksinkertaisempaa ohjelmakoodia ja tästä syystä Scala tukee myös muuttuvia muuttujia.
Vakiota, $val$, ei sen sijaan voida antaa uudelleen luomisen jälkeen. \cite{guller15}

Syntaksi $val$:in ja $var$:in luomiseksi on

\begin{lstlisting}[caption=Muuttujien luominen ja uudelleen asettaminen]
var x = 10
x = 20
val y = 10
\end{lstlisting}

Mikäli vakiota yritettään uudelleenmäärittää myöhemmin ohjelmassa, kääntäjä antaa virheen.
Huomionarvoista ylläolevassa syntaksissa on se, että Scala kääntäjä ei pakota määrittelemään muuttujan tyyppiä sillon kuin kääntäjä pystyy päättelemään (type deduction) sen.

\begin{lstlisting}[caption=Muuttujan luominen tyyppimäärittelyn avulla]
var x: Int = 10
val y: Int = 10
\end{lstlisting}

\subsection{Funktiot}

Funktio on lohko suoritettavaa koodia joka palauttaa arvon.
Se on konseptuaalisesti samankaltainen kuin matematiikassa: funktio ottaa sisääntulon ja palauttaa ulostulon. \cite{guller15}

Scalan funktiot ovat ensimmäisen luokan kansalaisia, jolla tarkoitetaan, että funktiota voidaan:

\begin{itemize}
	\item käyttää kuten muuttujaa
	\item antaa syötteenä toiselle funktiolle
	\item määritellä nimettömänä funktioliteraalina
	\item asettaa muuttujaan
	\item määritellä toisen funktion sisällä
	\item palauttaa toisen funktion ulostulona
\end{itemize}

\cite{guller15}

Scalassa funktio määritellään avainsanalla $def$.
Funktion määrittely aloitetaan funktion nimellä, jota seuraa sulkeissa olevat, pilkulla erotetut, parametrit tyyppimäärittelyineen.
Parametrien jälkeen funktiomäärittelyyn tulee kaksoispiste, funktion ulostulon tyyppi, yhtäsuuruusmerkki sekä funktion runko joko aaltosulkeissa tai ilman. \cite{guller15}

\lstset{language=Scala,tabsize=2}

\begin{lstlisting}[caption=Funktio]
def add(first: Int, second: Int): Int = {
	val sum = first + second
	return sum
}
\end{lstlisting}

Ylläolevassa esimerkissä funktion nimi on $add$ ja se ottaa kaksi $Int$ tyyppistä sisääntuloa.
Funktio palauttaa $Int$ tyyppisen arvon jonka se muodostaa lisäämällä annetut sisääntulot yhteen ja palauttamalla tuloksen.

Scala sallii myös lyhyemmän version samasta funktiosta:

\lstset{language=Scala,tabsize=2}

\begin{lstlisting}[caption=Funktio]
def add(first: Int, second: Int) = first + second
\end{lstlisting}

Toinen versio tekee täsmälleen saman asian kuin ensimmäinenkin, mutta se on vain kirjoitettu lyhyemmin.
Paluuarvon tyyppi on jätetty antamatta, sillä kääntäjä pystyy päättelemään sen koodista.
Paluuarvon tyyppi suositellaan kuitenkin annettavan aina.
Aaltosulkeet on myöskin jätetty pois, sillä ne ovat pakolliset vain kun funktion runko sisältää useamman kuin yhden lausekkeen.
Lisäksi, $return$ avainsana on ohitettu, sillä se on vapaaehtoinen.
Scalassa kaikki lausekkeet ovat arvon palauttavia lausekkeita, joten funktion rungon viimeisen lausekkeen arvosta tulee funktion paluuarvo. \cite{guller15}

\end{document}