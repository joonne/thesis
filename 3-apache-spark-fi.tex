\documentclass[main.tex]{thesis.tex}
\begin{document}

\chapter{Apache Spark}

Apache Spark on avoimen l‰hdekoodin sovelluskehys, joka yhdist‰‰ hajautettujen ohjelmien kirjoittamiseen tarkoitetun j‰rjestelm‰n sek‰ elegantin mallin ohjelmien kirjoittamiseen. \cite{ryza15}
Spark tarjoaa korkean tason rajapinnat Java, Scala, Python sek‰ R ohjelmointikielille.

Korkealla tasolla, jokainen Spark sovelus koostuu ajaja (driver) ohjelmasta sek‰ yhdest‰ tai useammasta t‰yt‰ntˆˆnpanijasta (executor).
Ajaja on ohjelma, joka ajaa k‰ytt‰j‰n p‰‰ohjelmaa ja suorittaa erilaisia rinnakkaisia operaatioita klusterissa.
T‰yt‰ntˆˆnpanija on yksi kone klusterissa.

Spark voidaan esitell‰ kuvailemalla sen edelt‰j‰‰, MapReduce:a, ja sen tarjoamia etuja.
MapReduce tarjosi yksinkertaisen mallin ohjelmien kirjoittamiseen ja pystyi suorittamaan kirjoitettua ohjelmaa rinnakkain sadoilla tietokoneilla.
MapReduce skaalautuu l‰hes lineaarisesti datan koon kasvaessa.
Suoritusaikaa hallitaan lis‰‰m‰ll‰ lis‰‰ tietokoneita suorittamaan teht‰v‰‰. 

Apache Spark s‰ilytt‰‰ MapReduce:n lineaarisen skaalautuvuuden ja vikasietokyvyn mutta laajentaa sit‰ kolmella merkitt‰v‰ll‰ tavalla.
Ensiksi, MapReducessa map- ja reduce-teht‰vien v‰liset tulokset t‰ytyy kirjoittaa levylle kun taas Spark kykenee v‰litt‰m‰‰n tulokset suoraan putkiston seuraavalle vaiheelle.
Toiseksi, Apache Spark kohtelee kehitt‰ji‰ paremmin tarjoamalla rikkaan joukon muunnoksia (transformations) joiden avulla voidaan muutamalla koodirivill‰ ilmaista monimutkaisia putkistoja. (ESIMERKKI?)
Kolmanneksi, Spark esittelee muistissa tapahtuvan prosessoinnin tarjoamalla abstraktion nimelt‰ Resilient Distributed Dataset (RDD). RDD tarjoaa kehitt‰j‰lle mahdollisuuden materialisoida mink‰ tahansa askeleen liukuhihnassa ja tallentaa sen muistiin.
T‰m‰ tarkoittaa sit‰, ett‰ tulevien askelien ei tarvitse laskea aiempia tuloksia uudelleen ja t‰llˆin on mahdollista jatkaa juuri k‰ytt‰j‰n haluamasta askeleesta.
Aiemmin t‰m‰nkaltaista ominaisuutta ei ole ollut saatavilla hajautetun laskennan j‰rjestelmiss‰. \cite{ryza15}

Spark ohjelmia voidaan kirjottaa Java, Scala, Python tai R ohjelmointikielell‰.
Scalan k‰ytt‰misell‰ saavutetaan kuitenkin muutamia etuja, joita muut kielet eiv‰t tarjoa.
Tehokkuus paranee, sill‰ teht‰v‰t kuten datan siirt‰minen eri kerrosten v‰lill‰ tai muunnosten suorittaminen datalle saattaa johtaa heikompaan tehokkuuteen.
Spark on kirjoitettu Scala-ohjelmointikielell‰, joten viimeisimm‰t ja parhaimmat ominaisuudet ovat aina k‰ytˆss‰.
Spark ohjelmoinnin filosofia on helpompi ymm‰rt‰‰ kun Sparkia k‰ytet‰‰n kielell‰, jolla se on rakennettu.
Suurin hyˆty jonka Scalan k‰ytt‰minen tarjoaa, on kuitenkin kehitt‰j‰kokemus joka tulee saman ohjelmointikielen k‰ytt‰misest‰ kaikkeen.
Datan tuonti, manipulointi ja koodin l‰hett‰minen klustereihin hoituvat samalla kielell‰. \cite{ryza15}

Spark-jakelun mukana toimitetaan luku-evaluointi-tulostus-silmukka, komentorivityˆkalu, (Read eval print loop, REPL), joka mahdollistaa uusien asioiden nopean testailun konsolissa, eik‰ sovelluksista tarvitse rakentaa itsen‰isi‰ (self-contained) alusta asti.
Usein kun REPLiss‰ kehitetty sovelluksen tai sovelluksen osan voidaan katsoa olevan tarpeeksi valmis, on j‰rkev‰‰ tehd‰ siit‰ koottu kirjasto (JAR).
N‰in varmistutaan etteiv‰t koodi tai tulokset p‰‰se katoamaan, vaikkakin REPL tarjoaa samantapaisen muistin komentohistoriasta kuin perinteinen komentorivikin.

SELITYS JAR:ISTA JA EHKƒ JVM:STƒ?

\section{Resilient Distributed Dataset (RDD)}

Resilient Distributed Dataset (RDD) on Sparkin tarjoama p‰‰abstraktio.
RDD on muuttumaton, osioitu elementtikokoelma joka voidaan hajauttaa klusterin useiden koneiden v‰lill‰. \cite{spark-rdd}

T‰rke‰ yksityiskohta ymm‰rt‰‰ RDD:st‰ on ett‰ ne ovat laiskasti evaluoituvia.
Laiska evaluaatio (lazy evaluation) on evaluointi taktiikka, jossa lausekkeen evaluointia viivytet‰‰n siihen asti kun sen arvoa tarvitaan.
Kun uusi RDD luodaan, mit‰‰n ei oikeasti viel‰ tapahdu.
Spark tiet‰‰ miss‰ data sijaitsee tai miten data saadaan laskettua kun tulee aika tehd‰ sille jotain.

RDD voidaan luoda kahdella tavalla.
Olemassaoleva Scala kokoelma voidaan rinnakkaistaa (parallelize).
Toinen keino on viitata ulkoiseen aineistoon ulkoisessa varastointij‰rjestelm‰ss‰ kuten HDFS:s‰, HBase:ssa tai miss‰ tahansa Hadoopin tuntemassa tiedostoj‰rjestelm‰ss‰. \cite{spark-programming-guide}

RDD:t voidaan tallentaa muistiin, jolloin ohjelmistokehitt‰j‰ voi uudelleenk‰ytt‰‰ niit‰ tehokkaasti rinnakkaisissa operaatioissa.
RDD:t voivat palautua solmuvirheist‰ automaattisesti k‰ytt‰en Directed Acyclic Graph (DAD) moottoria.
DAG tukee syklist‰ datavirtaa.
Jokaista Spark tyˆt‰ kohti luodaan DAG klusterissa suoritettavan teht‰v‰n tasoista.
Verrattuna MapReduceen, joka luo DAGin kahdesta ennaltam‰‰r‰tyst‰ tilasta (Map ja Reduce), Sparkin luomat DAGit voivat sis‰lt‰‰ mink‰ tahansa m‰‰r‰n tasoja.
T‰st‰ syyst‰ jotkin tyˆt voivat valmistua nopeammin kuin ne valmistuisivat MapReducessa.
Yksinkertaisimmat tyˆt voivat valmistua vain yhden tason j‰lkeen ja monimutkaisemmat teht‰v‰t valmistuvat yhden monitasoisen ajon j‰lkeen, ilman ett‰ niit‰ t‰ytyy pilkkoa useampiin tˆihin. \cite{mapRSpark}

\begin{figure}[h]
	\caption{Directed Acyclic Graph \cite{dag-image}}
	\centering
	\includegraphics[scale=1.0]{directed_acyclic_graph}
\end{figure}

\section{Dataset API}

Dataset (DS) on RDD:n korvaaja Sparkissa.
DS on vahvasti tyypitetty kokoelma aluespesifisi‰ objekteja jotka voidaan muuntaa rinnakkain k‰ytt‰en funktionaalisia tai relaatio-operaatioita.
Dataset:ille olemassa olevat operaatiot on jaettu muunnoksiin ja toimiin.
Muunnokset ovat operaatioita, jotka luovat uusia Datasettej‰, kuten map, filter, select, aggregate.
Toimet ovat operaatioita jotka laukaisevat laskentaa ja palauttavat tuloksia.
Toimia ovat esimerkiksi count, show tai datan kirjoittaminen tiedostoj‰rjestelm‰‰n. \cite{spark-dataset}

Dataset-instanssit ovat laiskoja luonteeltaan, jolla tarkoitetaan sit‰, ett‰ laskenta aloitetaan vasta kun toimintoa kutsutaan.
Dataset on pohjimmiltaan looginen suunnitelma, jolla kuvataan datan tuottamiseen tarvittava laskenta.
Toimea kutsuttaessa, Sparkin kyselyoptimoija (query optimizer) optimoi loogisen suunnitelman ja generoi fyysisen suunnitelman.
Fyysinen suunnitelma takaa rinnakkaisesti ja hajautetusti tapahtuvan tehokkaan suorituksen.
Loogista suunnitelmaa, kuten myˆs optimoitu fyysist‰ suunnitelmaa, voidaan tutkia k‰ytt‰m‰ll‰ DS:n $explain$ funktiota. \cite{spark-dataset}

Domain-spesifisten olioiden tehokkaaseen tukemiseen tarvitaan enkooderia.
Enkooderilla tarkoitetaan ohjelmaa, joka muuntaa tietoa jonkin algoritmin mukaisesti ja t‰ss‰ tapauksessa sit‰ k‰ytet‰‰n yhdist‰m‰‰n domain-spesifinen tyyppi $T$ Sparkin sis‰iseen tyyppij‰rjestelm‰‰n.
Enkooderia voidaan k‰ytt‰‰ esimerkiksi luokan $Person$, joka sis‰lt‰‰ kent‰t nimi (merkkijono) ja ik‰ (kokonaisluku), kertomaan Sparkille generoi koodia ajon aikana joka serialisoi $Person$ olion bin‰‰rirakenteeksi.
Generoidulla bin‰‰rirakenteella on usein pienempi muistijalanj‰lki ja se on myˆs optimoitu tehokkaaseen dataprosessointiin.
Datan bin‰‰riesitys voidaan tarkistaa k‰ytt‰m‰ll‰ DS:n tarjoamaa $schema$ funktiota. \cite{spark-dataset}

Two ways typically exist to create a Dataset.
The most common way is to make use of the read function provided by SparkSession and point Spark to some files on the storage system.
Such as the following $json$ file.

Dataset voidaan luoda tyypillisesti kahdella eri tavalla.
Yleisin tapa on k‰ytt‰‰ $SparkSession$:in tarjoamaa $read$ funktiota ja osoittaa Spark joihinkin tiedostoihin tiedostoj‰rjestelm‰ss‰, kuten seuraavaan $json$ tiedostoon.

\lstset{
	string=[s]{"}{"},
	stringstyle=\color{blue},
	comment=[l]{:},
	commentstyle=\color{black},
}

\begin{lstlisting}[caption=Esimerkki JSON tiedosto]

[{
  "name": "Matt",
  "salary": 5400
}, {
  "name": "George",
  "salary": 6000
}]

\end{lstlisting}


\lstset{
	frame=0,
	language=Scala,
	breaklines=true,
}

Dataset voidaan luoda myˆs tekem‰ll‰ muutoksia olemassaoleville Dataset olioille:

\begin{lstlisting}[caption=Creating a new Dataset through a transformation]

val names = people.map(_.name)

\end{lstlisting}

\begin{lstlisting}[caption=Uuden Dataset olion luominen k‰ytt‰en read funktiota]

val people = spark.read.json("./people.json").as[Person]

\end{lstlisting}

jossa $Person$ olisi Scala case-luokka, esimerkiksi:

\begin{lstlisting}[caption=case class Person]

case class Person(id: BigInt, firstName: String, lastName: String)

\end{lstlisting}

Case-luokat ovat tavallisia Scala-luokkia jotka ovat:

\begin{itemize}
	\item Oletustarvoisesti muuttumattomia (immutable)
	\item Hajoitettavia (decomposable) hahmonsovitusta hyv‰ksik‰ytt‰en
	\item Vertailtavissa viitteiden sijasta rakenteellisen samankaltaisuuden mukaan
	\item Lyhyit‰ luoda (instantiate) ja k‰ytt‰‰
\end{itemize}

Mik‰li tyyppimuunnos (casting) j‰tett‰isiin tekem‰tt‰, p‰‰dytt‰isiin luomaan DataFrame olio, jonka sis‰inen mallin (schema) Spark pyrkisi arvaamaan.
Tyyppimuunnos tehd‰‰n k‰ytt‰m‰ll‰ $as$ avainsanaa.

\begin{lstlisting}[caption=SparkSession kontekstin luominen]
val spark = SparkSession
.builder
.appName("MovieLensALS")
.config("spark.executor.memory", "2g")
.getOrCreate()
\end{lstlisting}

SparkSession on Spark ohjelmoinnin aloituspiste, kun halutaan k‰ytt‰‰ Dataset ja Dataframe rajapintoja.
Yll‰olevassa koodinp‰tk‰ss‰ luodaan $SparkSession$ ketjuttamalla rakentaja metodin kutsuja.

\cite{spark-dataset}

Dataset oliot ovat samankaltaisia kuin RDD:t, sill‰ nekin tarjoavat vahvan tyypityksen ja mahdollisuuden k‰ytt‰‰ voimakkaita lambda-funktioita \cite{spark-sql-programming-guide}.
Lambda-funktioita avustaa Spark SQL:n optimoitu suoritusmoottori \cite{spark-sql-programming-guide}.
Perinteisen serialisoinnin, kuten Java serialisoinnin, sijaan, k‰ytet‰‰n erikoistunutta enkooderia olioiden serialisointiin.
Serialisaatiolla tarkoitetaan olion muuntamista tavuiksi, jolloin olion muistijalanj‰lki pienenee.
Yleisesti serialisointia tarvitaan datan prosessointiin tai verkon yli l‰hett‰miseen.
Molempia, sek‰ enkoodereita ja serialisointia k‰ytet‰‰n olioiden muuntamiseen tavuiksi, mutta koodi luo enkooderit dynaamisesti.
Enkooderit k‰ytt‰v‰t sellaista muotoa, ett‰ Spark kykenee suorittamaan monenlaisia operaatioita, kuten suodattamista, j‰rjest‰mist‰ ja hajautusta (hashing), ilman ett‰ tavuja tarvitsee deserialisoida takaisin objektiksi. \cite{spark-programming-guide}

Seuraavassa koodilistauksessa luodaan uusi Datase lukemalla $json$ tiedosto tiedostoj‰rjestelm‰st‰.
Seuraavaksi luodaan uusi Dataset muunnoksen kautta.
Objektin kloonaamiseksi k‰ytet‰‰n case luokan $copy$ metodia, koska $people$ Dataset oli m‰‰ritelty muuttumattomaksi.
Lopuksi fyysinen suunnitelma tulostetaan konsoliin k‰ytt‰m‰ll‰ $explain$ funktiota uudelle Dataset objektille.

\begin{lstlisting}[caption=Dataset olion loogisen ja fyysisen suunnitelman n‰ytt‰minen]

val people = spark.read.json("./people.json").as[Person]
val peopleWithDoubleSalary = people.map { person => 
	person.copy(salary = person.salary * 2)
}
peopleWithDoubleSalary.explain

== Physical Plan ==
*SerializeFromObject [staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, assertnotnull(input[0, $line62.$read$$iw$$iw$Person, true], top level Product input object).name, true) AS name#212, staticinvoke(class org.apache.spark.sql.types.Decimal$, DecimalType(38,0), apply, assertnotnull(input[0, $line62.$read$$iw$$iw$Person, true], top level Product input object).salary, true) AS salary#213]
+- *MapElements <function1>, obj#211: $line62.$read$$iw$$iw$Person
+- *DeserializeToObject newInstance(class $line62.$read$$iw$$iw$Person), obj#210: $line62.$read$$iw$$iw$Person
+- *FileScan json [name#200,salary#201L] Batched: false, Format: JSON, Location: InMemoryFileIndex[file:/home/joonne/Documents/GitHub/thesis-code/people.json], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<name:string,salary:bigint>

\end{lstlisting}

\section{DataFrame API}

A DataFrame is essentially a Dataset that is organized into named columns.
It is conceptually equivalent to a table in a relational database or a data frame in R or Python, but it has richer optimizations under the hood.
DataFrames can be constructed from a range of sources such as structured data files, tables in Hive, external databases, or existing RDDs.
The DataFrame API is available in Scala, Java, Python, and R.
In the Scala API a DataFrame is represented by a Dataset of Rows, it essentially is simply a type alias of Dataset[Row].
\cite{spark-sql-programming-guide}

DataFrame on pohjimmiltaan nimettyihin sarakkeisiin j‰rjestetty Dataset.
Se on k‰sitteellisesti yhtenev‰ relaatiotietokannan taulun tai R/Python kielten tietokehyksen (data frame) kanssa, mutta DataFrame omaa rikkaammat optimoinnit konepellin alla.
DataFrame voidaan rakentaa

\begin{lstlisting}[caption=Creating a new DataFrame by using read function]

val people = spark.read.json("./people.json")

\end{lstlisting}

When creating a DataFrame, the schema of the 

\begin{figure}[h]
	\caption{DataFrame}
	\centering
	\includegraphics[scale=1.0]{dataframe}
\end{figure}

\section{Matrix Factorization}

Matrix factorization denotes a task in which a matrix is decomposed into a product of matrices.
There are many different matrix decompositions.
The following chapter will describe matrix factorization in general and the Alternating Least Squares algorithm which is the matrix factorization algorithm that is implemented in Spark.
It is based on same idea as Netflix prize winner, matrix factorization models.

Matrix factorization belongs to a vast class of algorithms called latent-factor models.
Latent-factor models try to explain observed interactions between a large number of users and products through a relatively small number of unobserved, underlying reasons.
For example, they can try to explain why people would buy a particular album out of endless possibilities by describing users and albums in terms of tastes which are not directly available as data. \cite{ryza15}
A latent factor is not available for direct observation. For example health of a human being is a latent factor.
Health can not be observed as a variable such as blood pressure.

\begin{figure}[h]
	\caption{Matrix factorization \cite{ryza15}}
	\centering
	\includegraphics[scale=0.8]{matrix_factorization}
\end{figure}

Matrix factorization algorithms treat the user and product data as if it was a large matrix A.
Each entry in row $i$ and column $j$ represents a rating the user has given to a specific product. \cite{ryza15}

Usually $A$ is sparse, which denotes that most of the entries of $A$ are 0.
This is due to the fact that usually only a few of all the possible user-product combinations exist.

Matrix factorization models factor $A$ as the matrix product of two smaller matrices, $X$ and $Y$, which are quite tiny.
Since $A$ has many rows and columns, both of them have many rows, but both have just a few columns $(k)$. The $k$ columns match to the latent factors that are being used to explain the interactions of the data.
The factorization can only be approximate because $k$ is small. \cite{ryza15}

The standard approach to matrix factorization based collaborative filtering treats the entries in the user-product matrix as explicit preferences given by the user to the product, for example users giving ratings to movies.
Implicit data denotes for example page views or a value representing if a user has listened to a artist. Explicit data means actual ratings that a user has given to a product.
Spark ALS can handle both implicit and explicit data. \cite{spark14} \cite{ryza15}

Usually many real-world use cases have access only to implicit feedback data such as views, clicks, purchases, likes or shares.
However, instead of trying to model the matrix of ratings directly, the approach in Spark MLlib treats the data as numbers representing the strength of the observations such as the number of clicks, or the cumulative duration someone spent viewing a movie.
Instead of explicit ratings, these numbers are related to the level of confidence in observed user preferences.
Based on this data, the model tries to find latent factors that can be used to predict the expected preference of a user for an item. \cite{spark14}

Sometimes these algorithms are referred to as matrix completion algorithms.
This is because the original matrix $A$ may be sparse while the product $XY^T$ is dense.
Hence, the product is only an approximation of $A$. \cite{ryza15}

\subsection{Alternating Least Squares (ALS)}

Collaborative filtering is commonly used for recommender systems.
These techniques aim to fill in the missing entries of a user-item association matrix.
Spark MLlib currently supports model-based collaborative filtering, in which users and products are described by a small set of latent factors that can be used to predict missing entries.
Spark MLlib uses the Alternating Least Squares (ALS) algorithm to learn these latent factors. \cite{spark14}

Spark ALS attempts to estimate the ratings matrix A as the product of two lower-rank matrices, $X$ and $Y$. \cite{als14}

\begin{equation}
A = XY^T
\end{equation}

Typically these approximations are referred to as factor matrices.
The general approach is iterative.
During each iteration, one of the factor matrices is held constant, while the other is solved for using least squares. The newly-solved factor matrix is then held constant while solving for the other factor matrix. \cite{als14}
Spark ALS enables massive parallelization since it can be done separately, it can be done in parallel which is an excellent feature for a large-scale computation algorithm. \cite{ryza15}

Spark ALS is a blocked implementation of the ALS factorization algorithm.
Idea is to group the two sets of factors, referred to as $users$ and $products$, into blocks.
Grouping is followed by reducing communication by only sending one copy of each user vector to each product block on each iteration.
Only those user feature vectors are sent that are needed by the the product blocks.
Reduced communication is achieved by precomputing some information about the ratings matrix to determine the out-links of each user and in-links of each product.
Out-link denotes those blocks of products that the user will contribute to.
In-link refers to the feature vectors that each product receives from each user block they depend on.
This allows to send only an array of feature vectors between each user block and product block.
Consequently the product block will find the users' ratings and update the products based on these messages. \cite{als14}

Essentially, instead of finding the low-rank approximations to the rating matrix $A$, it finds the approximations for a preference matrix $P$ where the elements of $P$ are $1$ when $r > 0$ and $0$ when $r <= 0$.
The ratings then act as confidence values related to strength of indicated user preferences rather than explicit ratings given to items. \cite{als14}

\begin{equation}
A_iY(Y^T Y)^{-1} = X_i
\end{equation}

Alternating Least Squares operates by rotating between fixing one of the unknowns $u_i$ or $v_j$.
While the other is fixed the other can be computed by solving the least-squares problem.
This approach is useful because it turns the previous non-convex problem into a quadratic that can be solved optimally \cite{aberger14}.
A general description of the algorithm for ALS for collaborative filtering taken from \cite{aberger14} is as follows:

\begin{lstlisting}[caption=Alternating Least Squares algorithm \cite{aberger14}]

1. Initialize matrix V by assigning the average rating for that movie as the first row, and small random numbers for the remaining entries.

2. Fix V, solve U by minimizing the RMSE function.

3. Fix U, solve V by minimizing the RMSE function.

4. Repeat Steps 2 and 3 until convergence.

\end{lstlisting}

Minimizing the Root Mean Square Error RMSE function denotes a task in which line is plotted. EXPLAIN RMSE.

\end{document}