\documentclass[main.tex]{thesis.tex}
\begin{document}

\chapter{Apache Spark}

Apache Spark on avoimen lähdekoodin sovelluskehys, joka yhdistää hajautettujen ohjelmien kirjoittamiseen tarkoitetun järjestelmän sekä elegantin mallin ohjelmien kirjoittamiseen. \cite{ryza15}
Spark tarjoaa korkean tason rajapinnat Java, Scala, Python sekä R ohjelmointikielille.

Korkealla tasolla, jokainen Spark sovelus koostuu ajaja (driver) ohjelmasta sekä yhdestä tai useammasta täytäntöönpanijasta (executor).
Ajaja on ohjelma, joka ajaa käyttäjän pääohjelmaa ja suorittaa erilaisia rinnakkaisia operaatioita klusterissa.
Täytäntöönpanija on yksi kone klusterissa.

Spark voidaan esitellä kuvailemalla sen edeltäjää, MapReduce:a, ja sen tarjoamia etuja.
MapReduce tarjosi yksinkertaisen mallin ohjelmien kirjoittamiseen ja pystyi suorittamaan kirjoitettua ohjelmaa rinnakkain sadoilla tietokoneilla.
MapReduce skaalautuu lähes lineaarisesti datan koon kasvaessa.
Suoritusaikaa hallitaan lisäämällä lisää tietokoneita suorittamaan tehtävää. 

Apache Spark säilyttää MapReduce:n lineaarisen skaalautuvuuden ja vikasietokyvyn mutta laajentaa sitä kolmella merkittävällä tavalla.
Ensiksi, MapReducessa map- ja reduce-tehtävien väliset tulokset täytyy kirjoittaa levylle kun taas Spark kykenee välittämään tulokset suoraan putkiston seuraavalle vaiheelle.
Toiseksi, Apache Spark kohtelee kehittäjiä paremmin tarjoamalla rikkaan joukon muunnoksia (transformations) joiden avulla voidaan muutamalla koodirivillä ilmaista monimutkaisia putkistoja. (ESIMERKKI?)
Kolmanneksi, Spark esittelee muistissa tapahtuvan prosessoinnin tarjoamalla abstraktion nimeltä Resilient Distributed Dataset (RDD). RDD tarjoaa kehittäjälle mahdollisuuden materialisoida minkä tahansa askeleen liukuhihnassa ja tallentaa sen muistiin.
Tämä tarkoittaa sitä, että tulevien askelien ei tarvitse laskea aiempia tuloksia uudelleen ja tällöin on mahdollista jatkaa juuri käyttäjän haluamasta askeleesta.
Aiemmin tämänkaltaista ominaisuutta ei ole ollut saatavilla hajautetun laskennan järjestelmissä. \cite{ryza15}

Spark ohjelmia voidaan kirjottaa Java, Scala, Python tai R ohjelmointikielellä.
Scalan käyttämisellä saavutetaan kuitenkin muutamia etuja, joita muut kielet eivät tarjoa.
Tehokkuus paranee, sillä tehtävät kuten datan siirtäminen eri kerrosten välillä tai muunnosten suorittaminen datalle saattaa johtaa heikompaan tehokkuuteen.
Spark on kirjoitettu Scala-ohjelmointikielellä, joten viimeisimmät ja parhaimmat ominaisuudet ovat aina käytössä.
Spark ohjelmoinnin filosofia on helpompi ymmärtää kun Sparkia käytetään kielellä, jolla se on rakennettu.
Suurin hyöty, jonka Scalan käyttäminen tarjoaa, on kuitenkin kehittäjäkokemus joka tulee saman ohjelmointikielen käyttämisestä kaikkeen.
Datan tuonti, manipulointi ja koodin lähettäminen klustereihin hoituvat samalla kielellä. \cite{ryza15}

Spark-jakelun mukana toimitetaan luku-evaluointi-tulostus-silmukka, komentorivityökalu, (Read eval print loop, REPL), joka mahdollistaa uusien asioiden nopean testailun konsolissa, eikä sovelluksista tarvitse rakentaa itsenäisiä (self-contained) alusta asti.
Kun REPLissä kehitetyn sovelluksen tai sovelluksen osan voidaan katsoa olevan tarpeeksi valmis, on järkevää tehdä siitä koottu kirjasto (JAR).
Näin varmistutaan etteivät koodi tai tulokset pääse katoamaan, vaikkakin REPL tarjoaa samantapaisen muistin komentohistoriasta kuin perinteinen komentorivikin.

SELITYS JAR:ISTA JA EHKÄ JVM:STÄ?

\section{Scala}

Scala on hybridiohjelmointikieli, joka tukee sekä olio- että funktionaalista ohjelmointia.
Funktionaalista ohjelmointia varten Scalasta löytyy tuki funktionaalisen ohjelmoinnin konsepteille kuten muuttumattomat tietorakenteet ja funktiot ensimmäisen luokan kansalaisina.
Olio-ohjelmointia varten Scalasta löytyy tuki konsepteille kuten luokat, objekti ja piirre (trait).
Scala tukee myös kapselointia, perintää, moniperintää ja muita tärkeitä olio-ohjelmoinnin konsepteja.
Scala on staattisesti tyypitetty kieli ja Scala ohjelmat käännetään Scala kääntäjää käyttäen.
Scala on JVM (Java Virtual Machine, Java virtuaalikone) perustainen kieli, joten Scala kääntäjä kääntää sovelluksen Java tavukoodiksi, joka voidaan ajaa missä tahansa Java virtuaalikoneessa.
Tavukooditasolla Scala ohjelmaa ei voida erottaa Java sovelluksesta.
Scalan ollessa JVM-perustainen, Scala on täysin yhteensopiva Javan kanssa.
Java kirjastoja voidaan käyttää suoraan Scala koodissa.
Tästä syystä Scala sovellukset hyötyvät suuresta Java koodin määrästä.
Vaikka Scala tukee sekä olio- että funktionaalista ohjelmointia, funktionaalista ohjelmointia suositaan. \cite{guller15}

\subsection{Perustyypit}

Scalan perustyypit numeroiden esittämiseen ovat Byte, Short, Int, Long, Float ja Double. Lisäksi Scalassa on perustyypit Char, String ja Boolean.
Char on 16 bittinen etumerkitön Unicode merkki.
String on jono Char:eja.
Boolean esittää totuusarvoa true tai false.

Huomionarvoista on se, että Scalassa ei ole ollenkaan primitiivisiä tyyppejä kuten Javassa.
Jokainen tyyppi on toteutettu luokkana ja käännöksen aikana kääntäjä tarvittaessa automaattisesti muuntaa Scala tyypit Javan primitiivi tyypeiksi.

\subsection{Muuttujat}

Scalassa on kahdentyyppisiä muuttujia: muuttuvia ja muuttumattomia.
Muuttuva muuttuja määritellään avainsanan $var$ avulla.
Muuttuvan muuttuja voidaan antaa uudelleen luomisen jälkeen.
Var:ien käyttöä ei suositella, mutta joskus niiden käyttämisellä saadaan aikaan yksinkertaisempaa koodia ja tästä syystä Scala tukee myös muuttuvia muuttujia.
Syntaksi $var$:in luomiseksi on

\begin{lstlisting}[caption=Muuttuvan muuttujan luominen ja uudelleen antaminen]
var x = 10
x = 20
\end{lstlisting}

Muuttumatonta muuttujaa, $val$, ei sen sijaan voida antaa uudelleen luomisen jälkeen.
Syntaksi $val$:in luomiseksi on

\begin{lstlisting}[caption=Muuttumattoman muuttujan luominen]
val y = 10
\end{lstlisting}

Mikäli muuttumatonta muuttujaa koitetaan antaa uudelleen myöhemmin ohjelmassa, kääntäjä antaa virheen.
Huomionarvoista ylläolevassa syntaksissa on se, että Scala kääntäjä ei pakota määrittelemään muuttujan tyyppiä sillon kuin kääntäjä pystyy päättelemään sen.

\begin{lstlisting}[caption=Muuttujan luominen tyyppimäärittelyn avulla]
var x: Int = 10
val y: Int = 10
\end{lstlisting}

\subsection{Funktiot}

Funktio on lohko suoritettavaa koodia joka palauttaa arvon.
Se on konseptuaalisesti samankaltainen kuin matematiikassa: funktio ottaa sisääntulon ja palauttaa ulostulon.
Scalan funktiot ovat ensimmäisen luokan kansalaisia, jolla tarkoitetaan että funktiota voi käyttää kuten muuttujaa.
Funktion voi antaa sisääntulona toiselle funktiolle, se voidaan määritellä nimettömänä funktioliteraalina.
Funktio voidaan asettaa muuttujaan.
Funktio voidaan määritellä toisen funktion sisällä.
funktio voidaan palauttaa toisen funktion ulostulona.

Funktio määritellään avainsanalla $def$.
Funktion määrittely aloitetaan funktion nimellä, jota seuraa sulkeissa olevat, pilkulla erotetut, parametrit tyyppimäärittelyineen.
Parametrien jälkeen funktiomäärittelyyn tulee kaksoispiste, funktion ulostulon tyyppi, yhtäsuuruusmerkki sekä funktion runko joko aaltosulkeissa tai ilman.

\begin{lstlisting}[caption=Funktio]
def add(firstInput: Int, secondInput: Int): Int = {
	val sum = firstInput + secondInput
	return sum
}
\end{lstlisting}

Ylläolevassa esimerkissä funktion nimi on $add$ ja se ottaa kaksi Int tyyppistä sisääntuloa.
Funktio palauttaa Int tyyppisen arvon jonka se muodostaa lisäämällä annetut sisääntulot yhteen ja palauttamalla tuloksen.

Scala sallii myös lyhyemmän version samasta funktiosta:

\begin{lstlisting}[caption=Funktio]
def add(firstInput: Int, secondInput: Int): Int = firstInput + secondInput
\end{lstlisting}

Toinen versio tekee täsmällee saman asian kuin ensimmäinenkin, mutta vain lyhyemmin kirjoitettuna.
Paluuarvon tyyppi on jätetty antamatta, sillä kääntäjä pystyy päättelemään sen koodista.
Paluuarvo suositellaan kuitenkin annettavan aina.
Aaltosulkeet on myöskin jätetty pois, sillä ne ovat pakolliset vain kun funktion runko sisältää useamman kuin yhden käskyn.
Lisäksi, $return$ avainsana on ohitettu, sillä se on vapaaehtoinen.
Scalassa kaikki on arvon palauttavia lausekkeita, joten funktion rungon viimeisen lausekkeen arvosta tulee funktion paluuarvo.

\section{Resilient Distributed Dataset (RDD)}

Resilient Distributed Dataset (RDD) on Sparkin tarjoama pääabstraktio.
RDD on muuttumaton, osioitu elementtikokoelma, joka voidaan hajauttaa klusterin useiden koneiden välillä. \cite{spark-rdd}

Tärkeä yksityiskohta ymmärtää RDD:stä on että ne ovat laiskasti evaluoituvia.
Laiska evaluaatio (lazy evaluation) on evaluointi taktiikka, jossa lausekkeen evaluointia viivytetään siihen asti kun sen arvoa tarvitaan.
Kun uusi RDD luodaan, mitään ei oikeasti vielä tapahdu.
Spark tietää missä data sijaitsee tai miten data saadaan laskettua kun tulee aika tehdä sille jotain.

RDD voidaan luoda kahdella tavalla.
Olemassaoleva Scala kokoelma voidaan rinnakkaistaa (parallelize).
Toinen keino on viitata ulkoiseen aineistoon ulkoisessa varastointijärjestelmässä kuten HDFS:sä, HBase:ssa tai missä tahansa Hadoopin tuntemassa tiedostojärjestelmässä. \cite{spark-programming-guide}

RDD:t voidaan tallentaa muistiin, jolloin ohjelmistokehittäjä voi uudelleenkäyttää niitä tehokkaasti rinnakkaisissa operaatioissa.
RDD:t voivat palautua solmuvirheistä automaattisesti käyttäen Directed Acyclic Graph (DAD) moottoria.
DAG tukee syklistä datavirtaa.
Jokaista Spark työtä kohti luodaan DAG klusterissa suoritettavan tehtävän tasoista.
Verrattuna MapReduceen, joka luo DAGin kahdesta ennaltamäärätystä tilasta (Map ja Reduce), Sparkin luomat DAGit voivat sisältää minkä tahansa määrän tasoja.
Tästä syystä jotkin työt voivat valmistua nopeammin kuin ne valmistuisivat MapReducessa.
Yksinkertaisimmat työt voivat valmistua vain yhden tason jälkeen ja monimutkaisemmat tehtävät valmistuvat yhden monitasoisen ajon jälkeen, ilman että niitä täytyy pilkkoa useampiin töihin. \cite{mapRSpark}

\begin{figure}[h]
	\caption{Directed Acyclic Graph \cite{dag-image}}
	\centering
	\includegraphics[scale=1.0]{directed_acyclic_graph}
\end{figure}

\section{Dataset API}

Dataset (DS) on RDD:n korvaaja Sparkissa.
DS on vahvasti tyypitetty kokoelma aluespesifisiä objekteja jotka voidaan muuntaa rinnakkain käyttäen funktionaalisia tai relaatio-operaatioita.
Dataset:ille olemassa olevat operaatiot on jaettu muunnoksiin ja toimiin.
Muunnokset ovat operaatioita, jotka luovat uusia Datasettejä, kuten map, filter, select, aggregate.
Toimet ovat operaatioita jotka laukaisevat laskentaa ja palauttavat tuloksia.
Toimia ovat esimerkiksi count, show tai datan kirjoittaminen tiedostojärjestelmään. \cite{spark-dataset}

Dataset-instanssit ovat laiskoja luonteeltaan, jolla tarkoitetaan sitä, että laskenta aloitetaan vasta kun toimintoa kutsutaan.
Dataset on pohjimmiltaan looginen suunnitelma, jolla kuvataan datan tuottamiseen tarvittava laskenta.
Toimea kutsuttaessa, Sparkin kyselyoptimoija (query optimizer) optimoi loogisen suunnitelman ja generoi fyysisen suunnitelman.
Fyysinen suunnitelma takaa rinnakkaisesti ja hajautetusti tapahtuvan tehokkaan suorituksen.
Loogista suunnitelmaa, kuten myös optimoitu fyysistä suunnitelmaa, voidaan tutkia käyttämällä DS:n $explain$ funktiota. \cite{spark-dataset}

Domain-spesifisten olioiden tehokkaaseen tukemiseen tarvitaan enkooderia.
Enkooderilla tarkoitetaan ohjelmaa, joka muuntaa tietoa jonkin algoritmin mukaisesti ja tässä tapauksessa sitä käytetään yhdistämään domain-spesifinen tyyppi $T$ Sparkin sisäiseen tyyppijärjestelmään.
Enkooderia voidaan käyttää esimerkiksi luokan $Person$, joka sisältää kentät nimi (merkkijono) ja ikä (kokonaisluku), kertomaan Sparkille generoi koodia ajon aikana joka serialisoi $Person$ olion binäärirakenteeksi.
Generoidulla binäärirakenteella on usein pienempi muistijalanjälki ja se on myös optimoitu tehokkaaseen dataprosessointiin.
Datan binääriesitys voidaan tarkistaa käyttämällä DS:n tarjoamaa $schema$ funktiota. \cite{spark-dataset}

Dataset voidaan luoda tyypillisesti kahdella eri tavalla.
Yleisin tapa on käyttää $SparkSession$:in tarjoamaa $read$ funktiota ja osoittaa Spark joihinkin tiedostoihin tiedostojärjestelmässä, kuten seuraavaan $json$ tiedostoon.

\lstset{
	string=[s]{"}{"},
	stringstyle=\color{blue},
	comment=[l]{:},
	commentstyle=\color{black},
}

\begin{lstlisting}[caption=Esimerkki JSON tiedosto]

[{
  "name": "Matt",
  "salary": 5400
}, {
  "name": "George",
  "salary": 6000
}]

\end{lstlisting}


\lstset{
	frame=0,
	language=Scala,
	breaklines=true,
}

Dataset voidaan luoda myös tekemällä muutoksia olemassaoleville Dataset olioille:

\begin{lstlisting}[caption=Creating a new Dataset through a transformation]

val names = people.map(_.name)

\end{lstlisting}

\begin{lstlisting}[caption=Uuden Dataset olion luominen käyttäen read funktiota]

val people = spark.read.json("./people.json").as[Person]

\end{lstlisting}

jossa $Person$ olisi Scala case-luokka, esimerkiksi:

\begin{lstlisting}[caption=case class Person]

case class Person(id: BigInt, firstName: String, lastName: String)

\end{lstlisting}

Case-luokat ovat tavallisia Scala-luokkia jotka ovat:

\begin{itemize}
	\item Oletustarvoisesti muuttumattomia (immutable)
	\item Hajoitettavia (decomposable) hahmonsovitusta hyväksikäyttäen
	\item Vertailtavissa viitteiden sijasta rakenteellisen samankaltaisuuden mukaan
	\item Lyhyitä luoda (instantiate) ja käyttää
\end{itemize}

Mikäli tyyppimuunnos (casting) jätettäisiin tekemättä, päädyttäisiin luomaan DataFrame olio, jonka sisäinen mallin (schema) Spark pyrkisi arvaamaan.
Tyyppimuunnos tehdään käyttämällä $as$ avainsanaa.

\begin{lstlisting}[caption=SparkSession kontekstin luominen]
val spark = SparkSession
.builder
.appName("MovieLensALS")
.config("spark.executor.memory", "2g")
.getOrCreate()
\end{lstlisting}

SparkSession on Spark ohjelmoinnin aloituspiste, kun halutaan käyttää Dataset ja Dataframe rajapintoja.
Ylläolevassa koodinpätkässä luodaan $SparkSession$ ketjuttamalla rakentaja metodin kutsuja.

\cite{spark-dataset}

Dataset oliot ovat samankaltaisia kuin RDD:t, sillä nekin tarjoavat vahvan tyypityksen ja mahdollisuuden käyttää voimakkaita lambda-funktioita \cite{spark-sql-programming-guide}.
Lambda-funktioita avustaa Spark SQL:n optimoitu suoritusmoottori \cite{spark-sql-programming-guide}.
Perinteisen serialisoinnin, kuten Java serialisoinnin, sijaan, käytetään erikoistunutta enkooderia olioiden serialisointiin.
Serialisaatiolla tarkoitetaan olion muuntamista tavuiksi, jolloin olion muistijalanjälki pienenee.
Yleisesti serialisointia tarvitaan datan prosessointiin tai verkon yli lähettämiseen.
Molempia, sekä enkoodereita ja serialisointia käytetään olioiden muuntamiseen tavuiksi, mutta koodi luo enkooderit dynaamisesti.
Enkooderit käyttävät sellaista muotoa, että Spark kykenee suorittamaan monenlaisia operaatioita, kuten suodattamista, järjestämistä ja hajautusta (hashing), ilman että tavuja tarvitsee deserialisoida takaisin objektiksi. \cite{spark-programming-guide}

Seuraavassa koodilistauksessa luodaan uusi Datase lukemalla $json$ tiedosto tiedostojärjestelmästä.
Seuraavaksi luodaan uusi Dataset muunnoksen kautta.
Objektin kloonaamiseksi käytetään case luokan $copy$ metodia, koska $people$ Dataset oli määritelty muuttumattomaksi.
Lopuksi fyysinen suunnitelma tulostetaan konsoliin käyttämällä $explain$ funktiota uudelle Dataset objektille.

\begin{lstlisting}[caption=Dataset olion fyysisen suunnitelman näyttäminen]

val people = spark.read.json("./people.json").as[Person]
val peopleWithDoubleSalary = people.map { person => 
	person.copy(salary = person.salary * 2)
}
peopleWithDoubleSalary.explain

== Physical Plan ==
*SerializeFromObject [staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, assertnotnull(input[0, $line62.$read$$iw$$iw$Person, true], top level Product input object).name, true) AS name#212, staticinvoke(class org.apache.spark.sql.types.Decimal$, DecimalType(38,0), apply, assertnotnull(input[0, $line62.$read$$iw$$iw$Person, true], top level Product input object).salary, true) AS salary#213]
+- *MapElements <function1>, obj#211: $line62.$read$$iw$$iw$Person
+- *DeserializeToObject newInstance(class $line62.$read$$iw$$iw$Person), obj#210: $line62.$read$$iw$$iw$Person
+- *FileScan json [name#200,salary#201L] Batched: false, Format: JSON, Location: InMemoryFileIndex[file:/home/joonne/Documents/GitHub/thesis-code/people.json], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<name:string,salary:bigint>

\end{lstlisting}

\section{DataFrame API}

DataFrame on pohjimmiltaan nimettyihin sarakkeisiin järjestetty Dataset.
Se on käsitteellisesti yhtenevä relaatiotietokannan taulun tai R/Python kielten tietokehyksen (data frame) kanssa, mutta DataFrame omaa rikkaammat optimoinnit konepellin alla.
DataFrame voidaan rakentaa useammalla tavalla, kuten esimerkiksi jäsennellyistä tiedostoista, Hive tauluista, ulkoisista tietokannoista tai olemassaolevista RDD olioista.
DataFrame rajapinta on saatavilla Scala, Java, Python ja R -ohjelmointikielille.
Scala rajapinnassa DataFrame on riveistä rakentuva Dataset, se on siis yksinkertaisesti tyyppialias Dataset[Row]. \cite{spark-programming-guide}

\begin{lstlisting}[caption=DataFrame luominen käyttäen read funktiota]

val people = spark.read.json("./people.json")

\end{lstlisting}

DataFrame objektia luotaessa, Spark arvaa luodun objektin sisäisen mallin.

\begin{figure}[h]
	\caption{DataFrame}
	\centering
	\includegraphics[scale=1.0]{dataframe}
\end{figure}

\section{Matriisin tekijöihinjako}

Matriisin tekijöihinjako on toimi, jossa matriisi hajoitetaan matriisien tuloksi.
Matriisi voidaan hajoittaa tekijöihinsä usealla eri tavalla.
Seuraava kappale kuvailee matriisin tekijöihinjakoa yleisellä tasolla sekä vuorottelevien pienenmpien neliöiden (Alternating Least Squares, ALS) algoritmia. ALS on Sparkin toteuttama matriisin tekijöihinjako algoritmi ja se perustuu samalle ajatukselle Netflix prize kilpailun voittajan, matriisin tekijöihinjako mallin kanssa.

Matriisin tekijöihinjako kuuluu suureen algoritmien luokkaan nimeltä piilevien tekijöiden mallit (Latent-factor models).
Piilevien tekijöiden mallit yrittävät selittää usean käyttäjän ja tuotteen välillä havaittuja vuorovaikutuksia käyttämällä suhteellisen pientä määrää havaitsemattomia, piileviä syitä.
Voidaan esimerkiksi yrittää selittää miksi ihminen ostaisi tietyn albumin lukemattomien mahdolisuuksien joukosta kuvailemalla käyttäjiä ja tuotteita mieltymysten perusteella, joista ei ole mahdollista saada tietoa. \cite{ryza15}
Piilevää tekijää ei ole mahdollista tarkastella sellaisenaan.
Ihmisen terveys on esimerkki piilevästä tekijästä, sillä
sitä ei ole mahdollista mitata kuten esimerkiksi verenpainetta.

\begin{figure}[h]
	\caption{Matrix factorization \cite{ryza15}}
	\centering
	\includegraphics[scale=0.8]{matrix_factorization}
\end{figure}

Matriisin tekijöihinjako algoritmit käsittelevät käyttäjä- ja tuotetietoja suurena matriisina A.
Jokainen rivissä $i$ sekä sarakkeessa $j$ sijaitseva kohta esittää arvostelua jonka käyttäjä on antanut tietylle tuotteelle. \cite{ryza15}

Yleensä $A$ on harva (sparse), jolla tarkoitetaan että useimmat $A$:n alkiot sisältävät 0.
Tämä johtuu siitä, että usein vain muutama käyttäjä-tuote kombinaatio on olemassa kaikista mahdollisuuksista.

Matriisin tekijöihinjako mallintaa $A$:n kahden pienemmän matriisin $X$ ja $Y$ tulona, jotka ovat varsin pieniä.
Koska $A$:ssa on monta riviä ja saraketta, $X$ ja $Y$ sisältävät paljon rivejä mutta vain muutaman $(k)$ sarakkeen.
Nämä $k$ saraketta vastaavat piileviä tekijöitä joita käytetään kuvailemaan tiedossa sijaitsevia vuorovaikutuksia.
Hajotelma (factorization) on ainoastaan arvio, sillä $k$ on pieni. \cite{ryza15}

Tavanomainen lähestymistapa matriisin tekijöihinjakoon perustuvassa yhteisöllisessä suodatuksessa on kohdella käyttäjä-tuote matriisin alkioita käyttäjien antamina täsmällisinä arvosteluina.
Eksplisiittistä tietoa on esimerkiksi käyttäjän antama arvio tuotteelle.
Spark ALS kykenee käsittelemään sekä implisiittistä että eksplisiittistä tietoa.
Implisiittistä tietoa on esimerkiksi sivujen katselukerrat tai tieto siitä, onko käyttäjä kuunnellut tiettyä artistia.
\cite{spark14} \cite{ryza15}

Usein monissa tosielämän käyttötapauksissa on käytettävissä ainoastaan implisiittistä tieto kuten katselukerrat, klikkaukset, ostos, tykkäykset tai jakamiset.
Spark MLlib kohtelee tietoa numeroina jotka esittävät havaintojen vahvuutta kuten klikkausten määrä tai kumulatiivinen aika joka käytetään elokuvan katseluun, sen sijaan että mallinnettaisiin arviomatriisia suoraan.
Ekplisiittisten arvioioiden sijaan, nämä numerot liittyvät havaittujen käyttäjämieltymysten varmuuteen.
Tämän tiedon perusteella malli koettaa etsiä piileviä tekijöitä joiden avulla voidaan ennustaa käyttäjän odotettu arvio tuotteelle. \cite{spark14}

Näihin algoritmeihin viitataan joskus matriisin täyttö algoritmeina.
Tämä johtuu siitä että alkuperäinen matriisi $A$ saattaa olla harva vaikka matriisitulo $XY^T$ on tiheä.
Vaikka tulosmatriisi sisältää arvon kaikille alkioille, se on kuitenkin vain arvio $A$:sta. \cite{ryza15}

\subsection{Alternating Least Squares (ALS)}

Yhteisöllistä suodatusta käytetään usein suosittelijajärjestelmissä.
Nämä tekniikat pyrkivät täyttämään käyttäjä-tuote assosiaatiomatriisin puuttuvat kohdat.
Spark MLlib tukee mallipohjaista yhteisösuodatusta, jossa käyttäjiä ja tuotteita kuvaillaan pienellä määrällä piileviä tekijöitä, joita voidaan käyttää puuttuvien kohtien ennustamiseen.
Spark MLlib käyttää vaihtelevien pienimpien neliöiden (Alternating Least Squares, ALS) algoritmia näiden piilevien tekijöiden oppimiseen. \cite{spark14}

Spark ALS yrittää arvioida arvostelumatriisin $A$ kahden alemman arvon matriisin, $X$ ja $Y$, tulona. \cite{als14}

\begin{equation}
A = XY^T
\end{equation}

Tyypillisesti näihin arvioihin viitataan tekijämatriiseina.
Perinteinen lähestymistapa on iteratiivinen.
Jokaisen iteraation aikana, toista tekijämatriisia pidetään vakiona ja toinen ratkaistaan käyttäen pienimpien summien algoritmia.
Juuri ratkaistua tekijämatriisia pidetään vuorostaan vakiona kun ratkaistaan toista tekijämatriisia. \cite{als14}
Spark ALS mahdollistaa massiivisen rinnakkaistamisen sillä algoritmia voidaan suorittaa erikseen.
Tämä on erinomainen ominaisuus laajamittaiselle (large-scale) laskenta-algoritmille. \cite{ryza15}

Spark ALS on lohkotettu versio ALS tekijöihinjako algoritmista.
Ajatuksena on ryhmittää kaksi tekijäryhmää, $käyttäjät$ ja $tuotteet$, lohkoihin.
Ryhmittämistä seuraa kommunikaation vähentäminen lähettämällä jokaiseen tuotelohkoon vain yksi kopio jokaisesta käyttäjävektorista iteraation aikana.
Vain ne käyttäjä vektorit lähetetään, joita tarvitaan tuotelohkoissa.
Vähennetty kommunikaatio saavutetaan valmiiksi laskemalla joitain tietoja suositusmatriisista jotta voidaan päätellä jokaisen käyttäjän ulostulot ja jokaisen tuotteen sisääntulot.
Ulostulolla tarkoitetaan niitä tuotelohkoja, joihin käyttäjä tulee myötävaikuttamaan.
Sisääntulolla tarkoitetaan niitä ominaisuusvektoreita jotka jokainen tuote ottaa vastaan niiltä käyttäjälohkoilta joista ne ovat riippuvaisia.
Tämä mahdollistaa sen, että voidaan lähettää vain taulukollisen ominaisuusvektoreita jokaisen käyttäjä- ja tuotelohkon välillä.
Vastaavasti tuotelohko löytää käyttäjän arviot ja päivittää tuotteita näiden viestien perusteella. \cite{als14}

Sen sijaan että etsittäisiin, alemman tason arviot suositusmatriisille $A$, etsitäänkin arviot mieltymysmatriisi $P$:lle, jossa $P$:n alkiot saavat arvon 1 kun $r > 0$ ja arvon 0 kun $r< = 0$.
Eksplisiittisen tuotearvion sijaan arvostelut kuvaavat käyttäjän mieltymyksen vahvuuden luottamusarvoa. \cite{als14}

\begin{equation}
A_iY(Y^T Y)^{-1} = X_i
\end{equation}

ALS operoi kiinnittämällä yhden tuntemattomista $u_i$ ja $v_j$ ja vaihtelemalla tätä kiinnittämistä.
Kun toinen on kiinnitetty, toinen voidaan laskea ratkaisemalla pienimpien neliöiden ongelma.
Tämä lähestymistapa on hyödyllinen, koska se muuttaa aiemman, ei-konveksin, ongelman kvadraattiseksi, eli neliömäiseksi, jolloin se voidaan ratkaista optimaalisesti. \cite{aberger14}
Alla on \cite{aberger14} mukainen yleinen kuvaus ALS algoritmista:

\begin{lstlisting}[caption=Vaihtelevien pienimpien neliöiden algoritmi (ALS) \cite{aberger14}]

1. Alusta matriisi V asettamalla ensimmäiseksi riviksi elokuvan keskimääräinen arvio ja pieni satunnaisluku jäljelläoleviin alkioihin.

2. Kiinnitä V, ratkaise U minimoimalla RMSE funktio.

3. Kiinnitä U, ratkaise V minimoimalla RMSE funktio.

4. Toista askeleita 2 ja 3 konvergenssiin asti.

\end{lstlisting}

RMSE (Root Mean Square Error) on kenties suosituin ennustettujen arvosteluiden tarkkuuden evaluiointiin käytetty metriikka.
Sitä käytetään yleisesti regressioalgoritmien avulla luotujen mallien evaluointiin.
Läheinen metriikka on MSE (Mean Square Error).
Regressioalgoritmien yhteydessä virheellä tarkoitetaan havainnon todellisen sekä ennustetun numeroarvon välistä eroa.
Kuten nimi viittaa, MSE on virheiden neliöiden keskiarvo.
Se voidaan laskea neliöimällä jokaisen havainnon virhe ja laskemalla virheiden neliöiden keskiarvo.
RMSE voidaan puolestaan laskea ottamalla neliöjuuri MSE:stä.
RMSE sekä MSE edustavat opetusvirhettä.
Ne ilmoittavat kuinka hyvin malli sovittuu opetusdataan.
Niiden avulla saadaan selville havaintojen sekä ennustettujen arvojen välinen poikkeavuus.
Alhaisemman MSE:n tai RMSE:n omaavan mallin sanotaan sovittuvan paremmin opetusdataan kuin korkeammat virhearvot omaavan mallin. \cite{guller15}

Suosittelujärjestelmä luo ennustettuja arvosteluita $\hat{r}_{ui}$ testiaineistolle $\tau$ käyttäjä-tuote pareja $(u,i)$ joille todelliset arviot $r$ tunnetaan.
Ennustettujen ja todellisten arvioiden välinen RMSE saadaan laskettua seuraavasti:

\begin{equation}
RMSE=\sqrt{\frac{1}{|\tau|} \sum_{(u,i)\in\tau}(\hat{r}_{ui}-r_{ui})^2}
\end{equation}

Konvergenssilla tarkoitetaan jonkin ilmiön lähestymistä ajan kuluessa jotain tiettyä arvoa, tässä tapauksessa sitä, että RMSE ei enään pienene tarpeeksi.

\end{document}