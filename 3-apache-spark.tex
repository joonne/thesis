\documentclass[main.tex]{thesis.tex}
\begin{document}

\chapter{Apache Spark}

Apache Spark on avoimen l‰hdekoodin sovelluskehys, joka yhdist‰‰ hajautettujen ohjelmien kirjoittamiseen tarkoitetun j‰rjestelm‰n sek‰ elegantin mallin ohjelmien kirjoittamiseen. \cite{ryza15}
Spark tarjoaa korkean tason rajapinnat Java, Scala, Python sek‰ R ohjelmointikielille.

Korkealla tasolla, jokainen Spark sovelus koostuu ajaja (driver) ohjelmasta sek‰ yhdest‰ tai useammasta t‰yt‰ntˆˆnpanijasta (executor).
Ajaja on ohjelma, joka ajaa k‰ytt‰j‰n p‰‰ohjelmaa ja suorittaa erilaisia rinnakkaisia operaatioita klusterissa.
T‰yt‰ntˆˆnpanija on yksi kone klusterissa.

Spark voidaan esitell‰ kuvailemalla sen edelt‰j‰‰, MapReduce:a, ja sen tarjoamia etuja.
MapReduce tarjosi yksinkertaisen mallin ohjelmien kirjoittamiseen ja pystyi suorittamaan kirjoitettua ohjelmaa rinnakkain sadoilla tietokoneilla.
MapReduce skaalautuu l‰hes lineaarisesti datan koon kasvaessa.
Suoritusaikaa hallitaan lis‰‰m‰ll‰ lis‰‰ tietokoneita suorittamaan teht‰v‰‰. 

Apache Spark s‰ilytt‰‰ MapReduce:n lineaarisen skaalautuvuuden ja vikasietokyvyn mutta laajentaa sit‰ kolmella merkitt‰v‰ll‰ tavalla.
Ensiksi, MapReducessa map- ja reduce-teht‰vien v‰liset tulokset t‰ytyy kirjoittaa levylle kun taas Spark kykenee v‰litt‰m‰‰n tulokset suoraan putkiston seuraavalle vaiheelle.
Toiseksi, Apache Spark kohtelee kehitt‰ji‰ paremmin tarjoamalla rikkaan joukon muunnoksia (transformations) joiden avulla voidaan muutamalla koodirivill‰ ilmaista monimutkaisia putkistoja. (ESIMERKKI?)
Kolmanneksi, Spark esittelee muistissa tapahtuvan prosessoinnin tarjoamalla abstraktion nimelt‰ Resilient Distributed Dataset (RDD). RDD tarjoaa kehitt‰j‰lle mahdollisuuden materialisoida mink‰ tahansa askeleen liukuhihnassa ja tallentaa sen muistiin.
T‰m‰ tarkoittaa sit‰, ett‰ tulevien askelien ei tarvitse laskea aiempia tuloksia uudelleen ja t‰llˆin on mahdollista jatkaa juuri k‰ytt‰j‰n haluamasta askeleesta.
Aiemmin t‰m‰nkaltaista ominaisuutta ei ole ollut saatavilla hajautetun laskennan j‰rjestelmiss‰. \cite{ryza15}

Spark ohjelmia voidaan kirjottaa Java, Scala, Python tai R ohjelmointikielell‰.
Scalan k‰ytt‰misell‰ saavutetaan kuitenkin muutamia etuja, joita muut kielet eiv‰t tarjoa.
Tehokkuus paranee, sill‰ teht‰v‰t kuten datan siirt‰minen eri kerrosten v‰lill‰ tai muunnosten suorittaminen datalle saattaa johtaa heikompaan tehokkuuteen.
Spark on kirjoitettu Scala-ohjelmointikielell‰, joten viimeisimm‰t ja parhaimmat ominaisuudet ovat aina k‰ytˆss‰.
Spark ohjelmoinnin filosofia on helpompi ymm‰rt‰‰ kun Sparkia k‰ytet‰‰n kielell‰, jolla se on rakennettu.
Suurin hyˆty, jonka Scalan k‰ytt‰minen tarjoaa, on kuitenkin kehitt‰j‰kokemus joka tulee saman ohjelmointikielen k‰ytt‰misest‰ kaikkeen.
Datan tuonti, manipulointi ja koodin l‰hett‰minen klustereihin hoituvat samalla kielell‰. \cite{ryza15}

Spark-jakelun mukana toimitetaan luku-evaluointi-tulostus-silmukka, komentorivityˆkalu, (Read eval print loop, REPL), joka mahdollistaa uusien asioiden nopean testailun konsolissa, eik‰ sovelluksista tarvitse rakentaa itsen‰isi‰ (self-contained) alusta asti.
Kun REPLiss‰ kehitetyn sovelluksen tai sovelluksen osan voidaan katsoa olevan tarpeeksi valmis, on j‰rkev‰‰ tehd‰ siit‰ koottu kirjasto (JAR).
N‰in varmistutaan etteiv‰t koodi tai tulokset p‰‰se katoamaan, vaikkakin REPL tarjoaa samantapaisen muistin komentohistoriasta kuin perinteinen komentorivikin.

SELITYS JAR:ISTA JA EHKƒ JVM:STƒ?

\section{Scala}

Scala on hybridiohjelmointikieli, joka tukee sek‰ olio- ett‰ funktionaalista ohjelmointia.
Funktionaalista ohjelmointia varten Scalasta lˆytyy tuki funktionaalisen ohjelmoinnin konsepteille kuten muuttumattomat tietorakenteet ja funktiot ensimm‰isen luokan kansalaisina.
Olio-ohjelmointia varten Scalasta lˆytyy tuki konsepteille kuten luokat, objekti ja piirre (trait).
Scala tukee myˆs kapselointia, perint‰‰, moniperint‰‰ ja muita t‰rkeit‰ olio-ohjelmoinnin konsepteja.
Scala on staattisesti tyypitetty kieli ja Scala ohjelmat k‰‰nnet‰‰n Scala k‰‰nt‰j‰‰ k‰ytt‰en.
Scala on JVM (Java Virtual Machine, Java virtuaalikone) perustainen kieli, joten Scala k‰‰nt‰j‰ k‰‰nt‰‰ sovelluksen Java tavukoodiksi, joka voidaan ajaa miss‰ tahansa Java virtuaalikoneessa.
Tavukooditasolla Scala ohjelmaa ei voida erottaa Java sovelluksesta.
Scalan ollessa JVM-perustainen, Scala on t‰ysin yhteensopiva Javan kanssa.
Java kirjastoja voidaan k‰ytt‰‰ suoraan Scala koodissa.
T‰st‰ syyst‰ Scala sovellukset hyˆtyv‰t suuresta Java koodin m‰‰r‰st‰.
Vaikka Scala tukee sek‰ olio- ett‰ funktionaalista ohjelmointia, funktionaalista ohjelmointia suositaan. \cite{guller15}

\subsection{Perustyypit}

Scalan perustyypit numeroiden esitt‰miseen ovat Byte, Short, Int, Long, Float ja Double. Lis‰ksi Scalassa on perustyypit Char, String ja Boolean.
Char on 16 bittinen etumerkitˆn Unicode merkki.
String on jono Char:eja.
Boolean esitt‰‰ totuusarvoa true tai false.

Huomionarvoista on se, ett‰ Scalassa ei ole ollenkaan primitiivisi‰ tyyppej‰ kuten Javassa.
Jokainen tyyppi on toteutettu luokkana ja k‰‰nnˆksen aikana k‰‰nt‰j‰ tarvittaessa automaattisesti muuntaa Scala tyypit Javan primitiivi tyypeiksi.

\subsection{Muuttujat}

Scalassa on kahdentyyppisi‰ muuttujia: muuttuvia ja muuttumattomia.
Muuttuva muuttuja m‰‰ritell‰‰n avainsanan $var$ avulla.
Muuttuvan muuttuja voidaan antaa uudelleen luomisen j‰lkeen.
Var:ien k‰yttˆ‰ ei suositella, mutta joskus niiden k‰ytt‰misell‰ saadaan aikaan yksinkertaisempaa koodia ja t‰st‰ syyst‰ Scala tukee myˆs muuttuvia muuttujia.
Syntaksi $var$:in luomiseksi on

\begin{lstlisting}[caption=Muuttuvan muuttujan luominen ja uudelleen antaminen]
var x = 10
x = 20
\end{lstlisting}

Muuttumatonta muuttujaa, $val$, ei sen sijaan voida antaa uudelleen luomisen j‰lkeen.
Syntaksi $val$:in luomiseksi on

\begin{lstlisting}[caption=Muuttumattoman muuttujan luominen]
val y = 10
\end{lstlisting}

Mik‰li muuttumatonta muuttujaa koitetaan antaa uudelleen myˆhemmin ohjelmassa, k‰‰nt‰j‰ antaa virheen.
Huomionarvoista yll‰olevassa syntaksissa on se, ett‰ Scala k‰‰nt‰j‰ ei pakota m‰‰rittelem‰‰n muuttujan tyyppi‰ sillon kuin k‰‰nt‰j‰ pystyy p‰‰ttelem‰‰n sen.

\begin{lstlisting}[caption=Muuttujan luominen tyyppim‰‰rittelyn avulla]
var x: Int = 10
val y: Int = 10
\end{lstlisting}

\subsection{Funktiot}

Funktio on lohko suoritettavaa koodia joka palauttaa arvon.
Se on konseptuaalisesti samankaltainen kuin matematiikassa: funktio ottaa sis‰‰ntulon ja palauttaa ulostulon.
Scalan funktiot ovat ensimm‰isen luokan kansalaisia, jolla tarkoitetaan ett‰ funktiota voi k‰ytt‰‰ kuten muuttujaa.
Funktion voi antaa sis‰‰ntulona toiselle funktiolle, se voidaan m‰‰ritell‰ nimettˆm‰n‰ funktioliteraalina.
Funktio voidaan asettaa muuttujaan.
Funktio voidaan m‰‰ritell‰ toisen funktion sis‰ll‰.
funktio voidaan palauttaa toisen funktion ulostulona.

Funktio m‰‰ritell‰‰n avainsanalla $def$.
Funktion m‰‰rittely aloitetaan funktion nimell‰, jota seuraa sulkeissa olevat, pilkulla erotetut, parametrit tyyppim‰‰rittelyineen.
Parametrien j‰lkeen funktiom‰‰rittelyyn tulee kaksoispiste, funktion ulostulon tyyppi, yht‰suuruusmerkki sek‰ funktion runko joko aaltosulkeissa tai ilman.

\begin{lstlisting}[caption=Funktio]
def add(firstInput: Int, secondInput: Int): Int = {
	val sum = firstInput + secondInput
	return sum
}
\end{lstlisting}

Yll‰olevassa esimerkiss‰ funktion nimi on $add$ ja se ottaa kaksi Int tyyppist‰ sis‰‰ntuloa.
Funktio palauttaa Int tyyppisen arvon jonka se muodostaa lis‰‰m‰ll‰ annetut sis‰‰ntulot yhteen ja palauttamalla tuloksen.

Scala sallii myˆs lyhyemm‰n version samasta funktiosta:

\begin{lstlisting}[caption=Funktio]
def add(firstInput: Int, secondInput: Int): Int = firstInput + secondInput
\end{lstlisting}

Toinen versio tekee t‰sm‰llee saman asian kuin ensimm‰inenkin, mutta vain lyhyemmin kirjoitettuna.
Paluuarvon tyyppi on j‰tetty antamatta, sill‰ k‰‰nt‰j‰ pystyy p‰‰ttelem‰‰n sen koodista.
Paluuarvo suositellaan kuitenkin annettavan aina.
Aaltosulkeet on myˆskin j‰tetty pois, sill‰ ne ovat pakolliset vain kun funktion runko sis‰lt‰‰ useamman kuin yhden k‰skyn.
Lis‰ksi, $return$ avainsana on ohitettu, sill‰ se on vapaaehtoinen.
Scalassa kaikki on arvon palauttavia lausekkeita, joten funktion rungon viimeisen lausekkeen arvosta tulee funktion paluuarvo.

\section{Resilient Distributed Dataset (RDD)}

Resilient Distributed Dataset (RDD) on Sparkin tarjoama p‰‰abstraktio.
RDD on muuttumaton, osioitu elementtikokoelma, joka voidaan hajauttaa klusterin useiden koneiden v‰lill‰. \cite{spark-rdd}

T‰rke‰ yksityiskohta ymm‰rt‰‰ RDD:st‰ on ett‰ ne ovat laiskasti evaluoituvia.
Laiska evaluaatio (lazy evaluation) on evaluointi taktiikka, jossa lausekkeen evaluointia viivytet‰‰n siihen asti kun sen arvoa tarvitaan.
Kun uusi RDD luodaan, mit‰‰n ei oikeasti viel‰ tapahdu.
Spark tiet‰‰ miss‰ data sijaitsee tai miten data saadaan laskettua kun tulee aika tehd‰ sille jotain.

RDD voidaan luoda kahdella tavalla.
Olemassaoleva Scala kokoelma voidaan rinnakkaistaa (parallelize).
Toinen keino on viitata ulkoiseen aineistoon ulkoisessa varastointij‰rjestelm‰ss‰ kuten HDFS:s‰, HBase:ssa tai miss‰ tahansa Hadoopin tuntemassa tiedostoj‰rjestelm‰ss‰. \cite{spark-programming-guide}

RDD:t voidaan tallentaa muistiin, jolloin ohjelmistokehitt‰j‰ voi uudelleenk‰ytt‰‰ niit‰ tehokkaasti rinnakkaisissa operaatioissa.
RDD:t voivat palautua solmuvirheist‰ automaattisesti k‰ytt‰en Directed Acyclic Graph (DAD) moottoria.
DAG tukee syklist‰ datavirtaa.
Jokaista Spark tyˆt‰ kohti luodaan DAG klusterissa suoritettavan teht‰v‰n tasoista.
Verrattuna MapReduceen, joka luo DAGin kahdesta ennaltam‰‰r‰tyst‰ tilasta (Map ja Reduce), Sparkin luomat DAGit voivat sis‰lt‰‰ mink‰ tahansa m‰‰r‰n tasoja.
T‰st‰ syyst‰ jotkin tyˆt voivat valmistua nopeammin kuin ne valmistuisivat MapReducessa.
Yksinkertaisimmat tyˆt voivat valmistua vain yhden tason j‰lkeen ja monimutkaisemmat teht‰v‰t valmistuvat yhden monitasoisen ajon j‰lkeen, ilman ett‰ niit‰ t‰ytyy pilkkoa useampiin tˆihin. \cite{mapRSpark}

\begin{figure}[h]
	\caption{Directed Acyclic Graph \cite{dag-image}}
	\centering
	\includegraphics[scale=1.0]{directed_acyclic_graph}
\end{figure}

\section{Dataset API}

Dataset (DS) on RDD:n korvaaja Sparkissa.
DS on vahvasti tyypitetty kokoelma aluespesifisi‰ objekteja jotka voidaan muuntaa rinnakkain k‰ytt‰en funktionaalisia tai relaatio-operaatioita.
Dataset:ille olemassa olevat operaatiot on jaettu muunnoksiin ja toimiin.
Muunnokset ovat operaatioita, jotka luovat uusia Datasettej‰, kuten map, filter, select, aggregate.
Toimet ovat operaatioita jotka laukaisevat laskentaa ja palauttavat tuloksia.
Toimia ovat esimerkiksi count, show tai datan kirjoittaminen tiedostoj‰rjestelm‰‰n. \cite{spark-dataset}

Dataset-instanssit ovat laiskoja luonteeltaan, jolla tarkoitetaan sit‰, ett‰ laskenta aloitetaan vasta kun toimintoa kutsutaan.
Dataset on pohjimmiltaan looginen suunnitelma, jolla kuvataan datan tuottamiseen tarvittava laskenta.
Toimea kutsuttaessa, Sparkin kyselyoptimoija (query optimizer) optimoi loogisen suunnitelman ja generoi fyysisen suunnitelman.
Fyysinen suunnitelma takaa rinnakkaisesti ja hajautetusti tapahtuvan tehokkaan suorituksen.
Loogista suunnitelmaa, kuten myˆs optimoitu fyysist‰ suunnitelmaa, voidaan tutkia k‰ytt‰m‰ll‰ DS:n $explain$ funktiota. \cite{spark-dataset}

Domain-spesifisten olioiden tehokkaaseen tukemiseen tarvitaan enkooderia.
Enkooderilla tarkoitetaan ohjelmaa, joka muuntaa tietoa jonkin algoritmin mukaisesti ja t‰ss‰ tapauksessa sit‰ k‰ytet‰‰n yhdist‰m‰‰n domain-spesifinen tyyppi $T$ Sparkin sis‰iseen tyyppij‰rjestelm‰‰n.
Enkooderia voidaan k‰ytt‰‰ esimerkiksi luokan $Person$, joka sis‰lt‰‰ kent‰t nimi (merkkijono) ja ik‰ (kokonaisluku), kertomaan Sparkille generoi koodia ajon aikana joka serialisoi $Person$ olion bin‰‰rirakenteeksi.
Generoidulla bin‰‰rirakenteella on usein pienempi muistijalanj‰lki ja se on myˆs optimoitu tehokkaaseen dataprosessointiin.
Datan bin‰‰riesitys voidaan tarkistaa k‰ytt‰m‰ll‰ DS:n tarjoamaa $schema$ funktiota. \cite{spark-dataset}

Dataset voidaan luoda tyypillisesti kahdella eri tavalla.
Yleisin tapa on k‰ytt‰‰ $SparkSession$:in tarjoamaa $read$ funktiota ja osoittaa Spark joihinkin tiedostoihin tiedostoj‰rjestelm‰ss‰, kuten seuraavaan $json$ tiedostoon.

\lstset{
	string=[s]{"}{"},
	stringstyle=\color{blue},
	comment=[l]{:},
	commentstyle=\color{black},
}

\begin{lstlisting}[caption=Esimerkki JSON tiedosto]

[{
  "name": "Matt",
  "salary": 5400
}, {
  "name": "George",
  "salary": 6000
}]

\end{lstlisting}


\lstset{
	frame=0,
	language=Scala,
	breaklines=true,
}

Dataset voidaan luoda myˆs tekem‰ll‰ muutoksia olemassaoleville Dataset olioille:

\begin{lstlisting}[caption=Creating a new Dataset through a transformation]

val names = people.map(_.name)

\end{lstlisting}

\begin{lstlisting}[caption=Uuden Dataset olion luominen k‰ytt‰en read funktiota]

val people = spark.read.json("./people.json").as[Person]

\end{lstlisting}

jossa $Person$ olisi Scala case-luokka, esimerkiksi:

\begin{lstlisting}[caption=case class Person]

case class Person(id: BigInt, firstName: String, lastName: String)

\end{lstlisting}

Case-luokat ovat tavallisia Scala-luokkia jotka ovat:

\begin{itemize}
	\item Oletustarvoisesti muuttumattomia (immutable)
	\item Hajoitettavia (decomposable) hahmonsovitusta hyv‰ksik‰ytt‰en
	\item Vertailtavissa viitteiden sijasta rakenteellisen samankaltaisuuden mukaan
	\item Lyhyit‰ luoda (instantiate) ja k‰ytt‰‰
\end{itemize}

Mik‰li tyyppimuunnos (casting) j‰tett‰isiin tekem‰tt‰, p‰‰dytt‰isiin luomaan DataFrame olio, jonka sis‰inen mallin (schema) Spark pyrkisi arvaamaan.
Tyyppimuunnos tehd‰‰n k‰ytt‰m‰ll‰ $as$ avainsanaa.

\begin{lstlisting}[caption=SparkSession kontekstin luominen]
val spark = SparkSession
.builder
.appName("MovieLensALS")
.config("spark.executor.memory", "2g")
.getOrCreate()
\end{lstlisting}

SparkSession on Spark ohjelmoinnin aloituspiste, kun halutaan k‰ytt‰‰ Dataset ja Dataframe rajapintoja.
Yll‰olevassa koodinp‰tk‰ss‰ luodaan $SparkSession$ ketjuttamalla rakentaja metodin kutsuja.

\cite{spark-dataset}

Dataset oliot ovat samankaltaisia kuin RDD:t, sill‰ nekin tarjoavat vahvan tyypityksen ja mahdollisuuden k‰ytt‰‰ voimakkaita lambda-funktioita \cite{spark-sql-programming-guide}.
Lambda-funktioita avustaa Spark SQL:n optimoitu suoritusmoottori \cite{spark-sql-programming-guide}.
Perinteisen serialisoinnin, kuten Java serialisoinnin, sijaan, k‰ytet‰‰n erikoistunutta enkooderia olioiden serialisointiin.
Serialisaatiolla tarkoitetaan olion muuntamista tavuiksi, jolloin olion muistijalanj‰lki pienenee.
Yleisesti serialisointia tarvitaan datan prosessointiin tai verkon yli l‰hett‰miseen.
Molempia, sek‰ enkoodereita ja serialisointia k‰ytet‰‰n olioiden muuntamiseen tavuiksi, mutta koodi luo enkooderit dynaamisesti.
Enkooderit k‰ytt‰v‰t sellaista muotoa, ett‰ Spark kykenee suorittamaan monenlaisia operaatioita, kuten suodattamista, j‰rjest‰mist‰ ja hajautusta (hashing), ilman ett‰ tavuja tarvitsee deserialisoida takaisin objektiksi. \cite{spark-programming-guide}

Seuraavassa koodilistauksessa luodaan uusi Datase lukemalla $json$ tiedosto tiedostoj‰rjestelm‰st‰.
Seuraavaksi luodaan uusi Dataset muunnoksen kautta.
Objektin kloonaamiseksi k‰ytet‰‰n case luokan $copy$ metodia, koska $people$ Dataset oli m‰‰ritelty muuttumattomaksi.
Lopuksi fyysinen suunnitelma tulostetaan konsoliin k‰ytt‰m‰ll‰ $explain$ funktiota uudelle Dataset objektille.

\begin{lstlisting}[caption=Dataset olion fyysisen suunnitelman n‰ytt‰minen]

val people = spark.read.json("./people.json").as[Person]
val peopleWithDoubleSalary = people.map { person => 
	person.copy(salary = person.salary * 2)
}
peopleWithDoubleSalary.explain

== Physical Plan ==
*SerializeFromObject [staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, assertnotnull(input[0, $line62.$read$$iw$$iw$Person, true], top level Product input object).name, true) AS name#212, staticinvoke(class org.apache.spark.sql.types.Decimal$, DecimalType(38,0), apply, assertnotnull(input[0, $line62.$read$$iw$$iw$Person, true], top level Product input object).salary, true) AS salary#213]
+- *MapElements <function1>, obj#211: $line62.$read$$iw$$iw$Person
+- *DeserializeToObject newInstance(class $line62.$read$$iw$$iw$Person), obj#210: $line62.$read$$iw$$iw$Person
+- *FileScan json [name#200,salary#201L] Batched: false, Format: JSON, Location: InMemoryFileIndex[file:/home/joonne/Documents/GitHub/thesis-code/people.json], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<name:string,salary:bigint>

\end{lstlisting}

\section{DataFrame API}

DataFrame on pohjimmiltaan nimettyihin sarakkeisiin j‰rjestetty Dataset.
Se on k‰sitteellisesti yhtenev‰ relaatiotietokannan taulun tai R/Python kielten tietokehyksen (data frame) kanssa, mutta DataFrame omaa rikkaammat optimoinnit konepellin alla.
DataFrame voidaan rakentaa useammalla tavalla, kuten esimerkiksi j‰sennellyist‰ tiedostoista, Hive tauluista, ulkoisista tietokannoista tai olemassaolevista RDD olioista.
DataFrame rajapinta on saatavilla Scala, Java, Python ja R -ohjelmointikielille.
Scala rajapinnassa DataFrame on riveist‰ rakentuva Dataset, se on siis yksinkertaisesti tyyppialias Dataset[Row]. \cite{spark-programming-guide}

\begin{lstlisting}[caption=DataFrame luominen k‰ytt‰en read funktiota]

val people = spark.read.json("./people.json")

\end{lstlisting}

DataFrame objektia luotaessa, Spark arvaa luodun objektin sis‰isen mallin.

\begin{figure}[h]
	\caption{DataFrame}
	\centering
	\includegraphics[scale=1.0]{dataframe}
\end{figure}

\section{Matriisin tekijˆihinjako}

Matriisin tekijˆihinjako on toimi, jossa matriisi hajoitetaan matriisien tuloksi.
Matriisi voidaan hajoittaa tekijˆihins‰ usealla eri tavalla.
Seuraava kappale kuvailee matriisin tekijˆihinjakoa yleisell‰ tasolla sek‰ vuorottelevien pienenmpien neliˆiden (Alternating Least Squares, ALS) algoritmia. ALS on Sparkin toteuttama matriisin tekijˆihinjako algoritmi ja se perustuu samalle ajatukselle Netflix prize kilpailun voittajan, matriisin tekijˆihinjako mallin kanssa.

Matriisin tekijˆihinjako kuuluu suureen algoritmien luokkaan nimelt‰ piilevien tekijˆiden mallit (Latent-factor models).
Piilevien tekijˆiden mallit yritt‰v‰t selitt‰‰ usean k‰ytt‰j‰n ja tuotteen v‰lill‰ havaittuja vuorovaikutuksia k‰ytt‰m‰ll‰ suhteellisen pient‰ m‰‰r‰‰ havaitsemattomia, piilevi‰ syit‰.
Voidaan esimerkiksi yritt‰‰ selitt‰‰ miksi ihminen ostaisi tietyn albumin lukemattomien mahdolisuuksien joukosta kuvailemalla k‰ytt‰ji‰ ja tuotteita mieltymysten perusteella, joista ei ole mahdollista saada tietoa. \cite{ryza15}
Piilev‰‰ tekij‰‰ ei ole mahdollista tarkastella sellaisenaan.
Ihmisen terveys on esimerkki piilev‰st‰ tekij‰st‰, sill‰
sit‰ ei ole mahdollista mitata kuten esimerkiksi verenpainetta.

\begin{figure}[h]
	\caption{Matrix factorization \cite{ryza15}}
	\centering
	\includegraphics[scale=0.8]{matrix_factorization}
\end{figure}

Matriisin tekijˆihinjako algoritmit k‰sittelev‰t k‰ytt‰j‰- ja tuotetietoja suurena matriisina A.
Jokainen riviss‰ $i$ sek‰ sarakkeessa $j$ sijaitseva kohta esitt‰‰ arvostelua jonka k‰ytt‰j‰ on antanut tietylle tuotteelle. \cite{ryza15}

Yleens‰ $A$ on harva (sparse), jolla tarkoitetaan ett‰ useimmat $A$:n alkiot sis‰lt‰v‰t 0.
T‰m‰ johtuu siit‰, ett‰ usein vain muutama k‰ytt‰j‰-tuote kombinaatio on olemassa kaikista mahdollisuuksista.

Matriisin tekijˆihinjako mallintaa $A$:n kahden pienemm‰n matriisin $X$ ja $Y$ tulona, jotka ovat varsin pieni‰.
Koska $A$:ssa on monta rivi‰ ja saraketta, $X$ ja $Y$ sis‰lt‰v‰t paljon rivej‰ mutta vain muutaman $(k)$ sarakkeen.
N‰m‰ $k$ saraketta vastaavat piilevi‰ tekijˆit‰ joita k‰ytet‰‰n kuvailemaan tiedossa sijaitsevia vuorovaikutuksia.
Hajotelma (factorization) on ainoastaan arvio, sill‰ $k$ on pieni. \cite{ryza15}

Tavanomainen l‰hestymistapa matriisin tekijˆihinjakoon perustuvassa yhteisˆllisess‰ suodatuksessa on kohdella k‰ytt‰j‰-tuote matriisin alkioita k‰ytt‰jien antamina t‰sm‰llisin‰ arvosteluina.
Eksplisiittist‰ tietoa on esimerkiksi k‰ytt‰j‰n antama arvio tuotteelle.
Spark ALS kykenee k‰sittelem‰‰n sek‰ implisiittist‰ ett‰ eksplisiittist‰ tietoa.
Implisiittist‰ tietoa on esimerkiksi sivujen katselukerrat tai tieto siit‰, onko k‰ytt‰j‰ kuunnellut tietty‰ artistia.
\cite{spark14} \cite{ryza15}

Usein monissa tosiel‰m‰n k‰yttˆtapauksissa on k‰ytett‰viss‰ ainoastaan implisiittist‰ tieto kuten katselukerrat, klikkaukset, ostos, tykk‰ykset tai jakamiset.
Spark MLlib kohtelee tietoa numeroina jotka esitt‰v‰t havaintojen vahvuutta kuten klikkausten m‰‰r‰ tai kumulatiivinen aika joka k‰ytet‰‰n elokuvan katseluun, sen sijaan ett‰ mallinnettaisiin arviomatriisia suoraan.
Ekplisiittisten arvioioiden sijaan, n‰m‰ numerot liittyv‰t havaittujen k‰ytt‰j‰mieltymysten varmuuteen.
T‰m‰n tiedon perusteella malli koettaa etsi‰ piilevi‰ tekijˆit‰ joiden avulla voidaan ennustaa k‰ytt‰j‰n odotettu arvio tuotteelle. \cite{spark14}

N‰ihin algoritmeihin viitataan joskus matriisin t‰yttˆ algoritmeina.
T‰m‰ johtuu siit‰ ett‰ alkuper‰inen matriisi $A$ saattaa olla harva vaikka matriisitulo $XY^T$ on tihe‰.
Vaikka tulosmatriisi sis‰lt‰‰ arvon kaikille alkioille, se on kuitenkin vain arvio $A$:sta. \cite{ryza15}

\subsection{Alternating Least Squares (ALS)}

Yhteisˆllist‰ suodatusta k‰ytet‰‰n usein suosittelijaj‰rjestelmiss‰.
N‰m‰ tekniikat pyrkiv‰t t‰ytt‰m‰‰n k‰ytt‰j‰-tuote assosiaatiomatriisin puuttuvat kohdat.
Spark MLlib tukee mallipohjaista yhteisˆsuodatusta, jossa k‰ytt‰ji‰ ja tuotteita kuvaillaan pienell‰ m‰‰r‰ll‰ piilevi‰ tekijˆit‰, joita voidaan k‰ytt‰‰ puuttuvien kohtien ennustamiseen.
Spark MLlib k‰ytt‰‰ vaihtelevien pienimpien neliˆiden (Alternating Least Squares, ALS) algoritmia n‰iden piilevien tekijˆiden oppimiseen. \cite{spark14}

Spark ALS yritt‰‰ arvioida arvostelumatriisin $A$ kahden alemman arvon matriisin, $X$ ja $Y$, tulona. \cite{als14}

\begin{equation}
A = XY^T
\end{equation}

Tyypillisesti n‰ihin arvioihin viitataan tekij‰matriiseina.
Perinteinen l‰hestymistapa on iteratiivinen.
Jokaisen iteraation aikana, toista tekij‰matriisia pidet‰‰n vakiona ja toinen ratkaistaan k‰ytt‰en pienimpien summien algoritmia.
Juuri ratkaistua tekij‰matriisia pidet‰‰n vuorostaan vakiona kun ratkaistaan toista tekij‰matriisia. \cite{als14}
Spark ALS mahdollistaa massiivisen rinnakkaistamisen sill‰ algoritmia voidaan suorittaa erikseen.
T‰m‰ on erinomainen ominaisuus laajamittaiselle (large-scale) laskenta-algoritmille. \cite{ryza15}

Spark ALS on lohkotettu versio ALS tekijˆihinjako algoritmista.
Ajatuksena on ryhmitt‰‰ kaksi tekij‰ryhm‰‰, $k‰ytt‰j‰t$ ja $tuotteet$, lohkoihin.
Ryhmitt‰mist‰ seuraa kommunikaation v‰hent‰minen l‰hett‰m‰ll‰ jokaiseen tuotelohkoon vain yksi kopio jokaisesta k‰ytt‰j‰vektorista iteraation aikana.
Vain ne k‰ytt‰j‰ vektorit l‰hetet‰‰n, joita tarvitaan tuotelohkoissa.
V‰hennetty kommunikaatio saavutetaan valmiiksi laskemalla joitain tietoja suositusmatriisista jotta voidaan p‰‰tell‰ jokaisen k‰ytt‰j‰n ulostulot ja jokaisen tuotteen sis‰‰ntulot.
Ulostulolla tarkoitetaan niit‰ tuotelohkoja, joihin k‰ytt‰j‰ tulee myˆt‰vaikuttamaan.
Sis‰‰ntulolla tarkoitetaan niit‰ ominaisuusvektoreita jotka jokainen tuote ottaa vastaan niilt‰ k‰ytt‰j‰lohkoilta joista ne ovat riippuvaisia.
T‰m‰ mahdollistaa sen, ett‰ voidaan l‰hett‰‰ vain taulukollisen ominaisuusvektoreita jokaisen k‰ytt‰j‰- ja tuotelohkon v‰lill‰.
Vastaavasti tuotelohko lˆyt‰‰ k‰ytt‰j‰n arviot ja p‰ivitt‰‰ tuotteita n‰iden viestien perusteella. \cite{als14}

Sen sijaan ett‰ etsitt‰isiin, alemman tason arviot suositusmatriisille $A$, etsit‰‰nkin arviot mieltymysmatriisi $P$:lle, jossa $P$:n alkiot saavat arvon 1 kun $r > 0$ ja arvon 0 kun $r< = 0$.
Eksplisiittisen tuotearvion sijaan arvostelut kuvaavat k‰ytt‰j‰n mieltymyksen vahvuuden luottamusarvoa. \cite{als14}

\begin{equation}
A_iY(Y^T Y)^{-1} = X_i
\end{equation}

ALS operoi kiinnitt‰m‰ll‰ yhden tuntemattomista $u_i$ ja $v_j$ ja vaihtelemalla t‰t‰ kiinnitt‰mist‰.
Kun toinen on kiinnitetty, toinen voidaan laskea ratkaisemalla pienimpien neliˆiden ongelma.
T‰m‰ l‰hestymistapa on hyˆdyllinen, koska se muuttaa aiemman, ei-konveksin, ongelman kvadraattiseksi, eli neliˆm‰iseksi, jolloin se voidaan ratkaista optimaalisesti. \cite{aberger14}
Alla on \cite{aberger14} mukainen yleinen kuvaus ALS algoritmista:

\begin{lstlisting}[caption=Vaihtelevien pienimpien neliˆiden algoritmi (ALS) \cite{aberger14}]

1. Alusta matriisi V asettamalla ensimm‰iseksi riviksi elokuvan keskim‰‰r‰inen arvio ja pieni satunnaisluku j‰ljell‰oleviin alkioihin.

2. Kiinnit‰ V, ratkaise U minimoimalla RMSE funktio.

3. Kiinnit‰ U, ratkaise V minimoimalla RMSE funktio.

4. Toista askeleita 2 ja 3 konvergenssiin asti.

\end{lstlisting}

RMSE (Root Mean Square Error) on kenties suosituin ennustettujen arvosteluiden tarkkuuden evaluiointiin k‰ytetty metriikka.
Sit‰ k‰ytet‰‰n yleisesti regressioalgoritmien avulla luotujen mallien evaluointiin.
L‰heinen metriikka on MSE (Mean Square Error).
Regressioalgoritmien yhteydess‰ virheell‰ tarkoitetaan havainnon todellisen sek‰ ennustetun numeroarvon v‰list‰ eroa.
Kuten nimi viittaa, MSE on virheiden neliˆiden keskiarvo.
Se voidaan laskea neliˆim‰ll‰ jokaisen havainnon virhe ja laskemalla virheiden neliˆiden keskiarvo.
RMSE voidaan puolestaan laskea ottamalla neliˆjuuri MSE:st‰.
RMSE sek‰ MSE edustavat opetusvirhett‰.
Ne ilmoittavat kuinka hyvin malli sovittuu opetusdataan.
Niiden avulla saadaan selville havaintojen sek‰ ennustettujen arvojen v‰linen poikkeavuus.
Alhaisemman MSE:n tai RMSE:n omaavan mallin sanotaan sovittuvan paremmin opetusdataan kuin korkeammat virhearvot omaavan mallin. \cite{guller15}

Suositteluj‰rjestelm‰ luo ennustettuja arvosteluita $\hat{r}_{ui}$ testiaineistolle $\tau$ k‰ytt‰j‰-tuote pareja $(u,i)$ joille todelliset arviot $r$ tunnetaan.
Ennustettujen ja todellisten arvioiden v‰linen RMSE saadaan laskettua seuraavasti:

\begin{equation}
RMSE=\sqrt{\frac{1}{|\tau|} \sum_{(u,i)\in\tau}(\hat{r}_{ui}-r_{ui})^2}
\end{equation}

Konvergenssilla tarkoitetaan jonkin ilmiˆn l‰hestymist‰ ajan kuluessa jotain tietty‰ arvoa, t‰ss‰ tapauksessa sit‰, ett‰ RMSE ei en‰‰n pienene tarpeeksi.

\end{document}