\documentclass[main.tex]{thesis.tex}
\begin{document}

\chapter{Apache Spark}

Apache Spark on avoimen l‰hdekoodin sovelluskehys, joka yhdist‰‰ hajautettujen ohjelmien kirjoittamiseen tarkoitetun j‰rjestelm‰n sek‰ elegantin mallin ohjelmien kirjoittamiseen \cite{ryza15}.
Spark tarjoaa korkean tason rajapinnat Java, Scala, Python sek‰ R ohjelmointikielille.

Jokainen Spark sovelus koostuu driver-ohjelmasta sek‰ yhdest‰ tai useammasta executor-ohjelmasta.
Driver on ohjelma, joka ajaa k‰ytt‰j‰n p‰‰ohjelmaa ja suorittaa erilaisia rinnakkaisia operaatioita klusterissa.
Executor on yksi kone klusterissa. \cite{ryza15}

Spark voidaan esitell‰ kuvailemalla sen edelt‰j‰‰, MapReduce:a, ja sen tarjoamia etuja.
MapReduce tarjosi yksinkertaisen mallin ohjelmien kirjoittamiseen ja pystyi suorittamaan kirjoitettua ohjelmaa rinnakkain sadoilla tietokoneilla.
MapReduce skaalautuu l‰hes lineaarisesti datan koon kasvaessa.
Suoritusaikaa hallitaan lis‰‰m‰ll‰ lis‰‰ tietokoneita suorittamaan teht‰v‰‰. \cite{ryza15}

Apache Spark s‰ilytt‰‰ MapReduce:n lineaarisen skaalautuvuuden ja vikasietokyvyn mutta laajentaa sit‰ kolmella merkitt‰v‰ll‰ tavalla.
Ensiksi, MapReducessa map- ja reduce-teht‰vien v‰liset tulokset t‰ytyy kirjoittaa levylle kun taas Spark kykenee v‰litt‰m‰‰n tulokset suoraan putkiston (pipeline) seuraavalle vaiheelle.
Toiseksi, Apache Spark:in voidaan ajatella kohtelevan kehitt‰ji‰ paremmin tarjoamalla rikkaan joukon muunnoksia (transformations) joiden avulla voidaan muutamalla koodirivill‰ ilmaista monimutkaisia putkistoja. (ESIMERKKI?)
Kolmanneksi, Spark esittelee muistissa tapahtuvan prosessoinnin tarjoamalla abstraktion nimelt‰ Resilient Distributed Dataset (RDD). RDD tarjoaa kehitt‰j‰lle mahdollisuuden materialisoida mink‰ tahansa askeleen putkistossa ja tallentaa sen muistiin.
T‰m‰ tarkoittaa sit‰, ett‰ tulevien askelien ei tarvitse laskea aiempia tuloksia uudelleen ja t‰llˆin on mahdollista jatkaa juuri k‰ytt‰j‰n haluamasta askeleesta.
Aiemmin t‰m‰nkaltaista ominaisuutta ei ole ollut saatavilla hajautetun laskennan j‰rjestelmiss‰. \cite{ryza15}

Spark ohjelmia voidaan kirjottaa Java, Scala, Python tai R-ohjelmointikielell‰.
Scalan k‰ytt‰misell‰ saavutetaan kuitenkin muutamia etuja, joita muut kielet eiv‰t tarjoa.
Tehokkuus saatta parantua, sill‰ datan siirt‰minen eri kerrosten v‰lill‰ tai muunnosten suorittaminen datalle voi johtaa heikompaan tehokkuuteen.
Spark on kirjoitettu Scala-ohjelmointikielell‰, joten viimeisimm‰t ja parhaimmat ominaisuudet ovat aina k‰ytˆss‰, eik‰ niiden k‰‰nnˆst‰ tarvitse odotella.
Spark ohjelmoinnin filosofia on helpompi ymm‰rt‰‰ kun Sparkia k‰ytet‰‰n kielell‰, jolla se on rakennettu.
Suurin hyˆty, jonka Scalan k‰ytt‰minen tarjoaa, on kuitenkin kehitt‰j‰kokemus joka tulee saman ohjelmointikielen k‰ytt‰misest‰ kaikkeen.
Datan tuonti, manipulointi ja koodin l‰hett‰minen klustereihin hoituvat samalla kielell‰. \cite{ryza15}

Spark-jakelun mukana toimitetaan luku-evaluointi-tulostus-silmukka, komentorivityˆkalu, (Read eval print loop, REPL), joka mahdollistaa uusien asioiden nopean testailun konsolissa, eik‰ sovelluksista tarvitse rakentaa itsen‰isi‰ (self-contained) alusta asti.
Kun REPLiss‰ kehitetyn sovelluksen tai sovelluksen osan voidaan katsoa olevan tarpeeksi valmis, on j‰rkev‰‰ tehd‰ siit‰ koottu kirjasto (JAR).
N‰in varmistutaan ettei ohjelmakoodia tai tuloksia p‰‰se katoamaan, vaikkakin REPL tarjoaa samantapaisen muistin komentohistoriasta kuin perinteinen komentorivikin.

JAR eli Java ARchive on suosittuun ZIP tiedostoformaattiin perustuva alustariippumaton tiedostoformaatti, jota k‰ytet‰‰n kokoamaan monta tiedostoa yhdeksi tiedostoksi. \cite{JAR}

JVM (Java Virtual Machine, Java-virtuaalikone) on abstrakti laskentakone (computing machine). Kuten oikea laskentakone, se omaa k‰skykannan ja muokkaa useita muistialueita ajon aikana. JVM ei tied‰ mit‰‰n ohjelmointikielist‰, kuten Scala tai Java, vaan se operoi ainoastaan class-tiedostoilla, jotka ovat tietynlaisia bin‰‰ritiedostoja. Class-tiedosto sis‰lt‰‰ JVM k‰skyt sek‰ symbolitaulun. \cite{JVM}

\section{Scala}

Scala on moniparadigmainen ohjelmointikieli, joka tukee sek‰ olio- ett‰ funktionaalista ohjelmointia.
Funktionaalista ohjelmointia varten Scalasta lˆytyy tuki funktionaalisen ohjelmoinnin konsepteille kuten muuttumattomat tietorakenteet ja funktiot ensimm‰isen luokan kansalaisina.
Olio-ohjelmointia varten Scalasta lˆytyy tuki konsepteille kuten luokat, oliot ja piirre (trait).
Scala tukee myˆs kapselointia, perint‰‰, moniperint‰‰ ja muita t‰rkeit‰ olio-ohjelmoinnin konsepteja.
Scala on staattisesti tyypitetty kieli ja sill‰ kirjoitetut ohjelmat k‰‰nnet‰‰n Scala k‰‰nt‰j‰‰ k‰ytt‰en.
Scala on JVM-perustainen (Java Virtual Machine, Java-virtuaalikone) kieli, joten Scala k‰‰nt‰j‰ k‰‰nt‰‰ sovelluksen Java-tavukoodiksi, joka voidaan ajaa miss‰ tahansa Java-virtuaalikoneessa.
Tavukooditasolla Scala ohjelmaa ei voida erottaa Java sovelluksesta.
Scalan ollessa JVM-perustainen, Scala on t‰ysin yhteensopiva Javan kanssa ja n‰in ollen Java-kirjastoja voidaan k‰ytt‰‰ suoraan Scala-koodissa.
T‰st‰ syyst‰ Scala-sovellukset hyˆtyv‰t suuresta Java-koodin m‰‰r‰st‰.
Vaikka Scala tukee sek‰ olio- ett‰ funktionaalista ohjelmointia, funktionaalista ohjelmointia suositaan. \cite{guller15}

\subsection{Perustyypit}

Scalan perustyypit numeroiden esitt‰miseen ovat Byte, Short, Int, Long, Float ja Double.
Lis‰ksi Scalassa on perustyypit Char, String ja Boolean.
Char on 16 bittinen etumerkitˆn Unicode merkki.
String on jono Char:eja.
Boolean esitt‰‰ totuusarvoa tosi (true) tai ep‰tosi (false). \cite{guller15}

Javasta poiketen Scalassa ei ole ollenkaan primitiivisi‰ tyyppej‰ vaan jokainen tyyppi on toteutettu luokkana.
K‰‰nnˆksen aikana k‰‰nt‰j‰ tarvittaessa automaattisesti muuntaa Scala tyypit Javan primitiivisiksi tyypeiksi. \cite{guller15}

\subsection{Muuttujat}

Scalassa on kahdentyyppisi‰ muuttujia: muuttuvia ja vakioita.
Muuttuva muuttuja m‰‰ritell‰‰n avainsanan $var$ avulla.
Muuttuvaa muuttujaa ei voida asettaa uudelleen luomisen j‰lkeen.
Var:ien k‰yttˆ‰ ei suositella, mutta joskus niiden k‰ytt‰misell‰ saadaan aikaan yksinkertaisempaa ohjelmakoodia ja t‰st‰ syyst‰ Scala tukee myˆs muuttuvia muuttujia. \cite{guller15}

Syntaksi $var$:in luomiseksi on

\begin{lstlisting}[caption=Muuttuvan muuttujan luominen ja uudelleen asettaminen]
var x = 10
x = 20
\end{lstlisting}

Muuttumatonta muuttujaa, $val$, ei sen sijaan voida antaa uudelleen luomisen j‰lkeen.
Syntaksi $val$:in luomiseksi on

\begin{lstlisting}[caption=Muuttumattoman muuttujan luominen]
val y = 10
\end{lstlisting}

Mik‰li muuttumatonta muuttujaa koitetaan antaa uudelleen myˆhemmin ohjelmassa, k‰‰nt‰j‰ antaa virheen.
Huomionarvoista yll‰olevassa syntaksissa on se, ett‰ Scala k‰‰nt‰j‰ ei pakota m‰‰rittelem‰‰n muuttujan tyyppi‰ sillon kuin k‰‰nt‰j‰ pystyy p‰‰ttelem‰‰n sen.

\begin{lstlisting}[caption=Muuttujan luominen tyyppim‰‰rittelyn avulla]
var x: Int = 10
val y: Int = 10
\end{lstlisting}

\subsection{Funktiot}

Funktio on lohko suoritettavaa koodia joka palauttaa arvon.
Se on konseptuaalisesti samankaltainen kuin matematiikassa: funktio ottaa sis‰‰ntulon ja palauttaa ulostulon. \cite{guller15}

Scalan funktiot ovat ensimm‰isen luokan kansalaisia, jolla tarkoitetaan ett‰ funktiota voidaan:

\begin{itemize}
	\item k‰ytt‰‰ kuten muuttujaa
	\item antaa syˆtteen‰ toiselle funktiolle
	\item m‰‰ritell‰ nimettˆm‰n‰ funktioliteraalina
	\item asettaa muuttujaan
	\item m‰‰ritell‰ toisen funktion sis‰ll‰
	\item palauttaa toisen funktion ulostulona
\end{itemize}

\cite{guller15}

Scalassa funktio m‰‰ritell‰‰n avainsanalla $def$.
Funktion m‰‰rittely aloitetaan funktion nimell‰, jota seuraa sulkeissa olevat, pilkulla erotetut, parametrit tyyppim‰‰rittelyineen.
Parametrien j‰lkeen funktiom‰‰rittelyyn tulee kaksoispiste, funktion ulostulon tyyppi, yht‰suuruusmerkki sek‰ funktion runko joko aaltosulkeissa tai ilman. \cite{guller15}

\lstset{language=Scala,tabsize=2}

\begin{lstlisting}[caption=Funktio]
def add(first: Int, second: Int): Int = {
	val sum = first + second
	return sum
}
\end{lstlisting}

Yll‰olevassa esimerkiss‰ funktion nimi on $add$ ja se ottaa kaksi $Int$ tyyppist‰ sis‰‰ntuloa.
Funktio palauttaa $Int$ tyyppisen arvon jonka se muodostaa lis‰‰m‰ll‰ annetut sis‰‰ntulot yhteen ja palauttamalla tuloksen.

Scala sallii myˆs lyhyemm‰n version samasta funktiosta:

\lstset{language=Scala,tabsize=2}

\begin{lstlisting}[caption=Funktio]
def add(first: Int, second: Int): Int = first + second
\end{lstlisting}

Toinen versio tekee t‰sm‰lleen saman asian kuin ensimm‰inenkin, mutta se on vain kirjoitetty k‰ytt‰en lyhyemp‰‰ syntaksia.
Paluuarvon tyyppi on j‰tetty antamatta, sill‰ k‰‰nt‰j‰ pystyy p‰‰ttelem‰‰n sen koodista.
Paluuarvo suositellaan kuitenkin annettavan aina.
Aaltosulkeet on myˆskin j‰tetty pois, sill‰ ne ovat pakolliset vain kun funktion runko sis‰lt‰‰ useamman kuin yhden k‰skyn.
Lis‰ksi, $return$ avainsana on ohitettu, sill‰ se on vapaaehtoinen.
Scalassa kaikki lausekkeet ovat arvon palauttavia lausekkeita, joten funktion rungon viimeisen lausekkeen arvosta tulee funktion paluuarvo. \cite{guller15}

\section{Resilient Distributed Dataset (RDD)}

Resilient Distributed Dataset (RDD) on Sparkin tarjoama p‰‰abstraktio.
RDD on muuttumaton, partitioitu elementtikokoelma, joka voidaan hajauttaa klusterin useiden koneiden v‰lill‰. \cite{spark-rdd}

RDD:t ovat laiskasti evaluoituvia, jolla tarkoitetaan sit‰, ett‰ lausekkeen evaluointia viivytet‰‰n siihen asti kun sen arvoa tarvitaan.
Kun uusi RDD luodaan, mit‰‰n laskentaa ei oikeasti viel‰ tapahdu, vaan Spark tiet‰‰ miss‰ data sijaitsee tai miten data saadaan laskettua kun tulee aika tehd‰ sille jotain.

RDD voidaan luoda kahdella tavalla, rinnakkaistamalla (parallelize) tai viittaamalla ulkoiseen aineistoon.
Rinnakkaistamisessa olemassaoleva Scala kokoelma voidaan rinnakkaistaa RDD:ksi.
Ulkoiseen ainestoon viittaamisella tarkoitetaan viittaamista aineistoon ulkoisessa varastointij‰rjestelm‰ss‰ kuten HDFS:s‰, HBase:ssa tai miss‰ tahansa Hadoopin tuntemassa tiedostoj‰rjestelm‰ss‰. \cite{spark-programming-guide}

RDD:t voidaan tallentaa muistiin, jolloin ohjelmistokehitt‰j‰ voi uudelleenk‰ytt‰‰ niit‰ tehokkaasti rinnakkaisissa operaatioissa.
RDD:t voivat palautua solmuvirheist‰ automaattisesti k‰ytt‰en Directed Acyclic Graph (DAG) moottoria.
DAG tukee asyklist‰ datavirtaa, jolla tarkoitetaan sit‰, ett‰ jokainen graafin kaari kulkee topologisessa j‰rjestyksess‰ aiemmasta myˆhemp‰‰n.
Jokaista Spark-tyˆt‰ kohti luodaan DAG klusterissa suoritettavan teht‰v‰n tasoista.
Verrattuna MapReduceen, joka luo DAGin kahdesta ennaltam‰‰r‰tyst‰ tilasta (Map ja Reduce), Sparkin luomat DAGit voivat sis‰lt‰‰ mink‰ tahansa m‰‰r‰n tasoja.
T‰st‰ syyst‰ jotkin tyˆt voivat valmistua nopeammin kuin ne valmistuisivat MapReducessa. TODO
Yksinkertaisimmat tyˆt voivat valmistua vain yhden tason j‰lkeen ja monimutkaisemmat teht‰v‰t valmistuvat yhden monitasoisen ajon j‰lkeen, ilman ett‰ niit‰ t‰ytyy pilkkoa useampiin tˆihin. \cite{mapRSpark}

\begin{figure}[h]
	\caption{Directed Acyclic Graph \cite{dag-image}}
	\centering
	\includegraphics[scale=1.0]{directed_acyclic_graph}
\end{figure}

\section{Dataset API}

Dataset (DS) on vahvasti tyypitetty kokoelma aluespesifisi‰ objekteja, jotka voidaan muuntaa rinnakkain k‰ytt‰en funktionaalisia tai relaatio-operaatioita.
DS on RDD:n korvaaja Sparkissa.
Dataset:ille olemassa olevat operaatiot on jaettu \textit{muunnoksiin} (transformations) ja \textit{toimiin} (actions).
Muunnokset ovat operaatioita, jotka luovat uusia Dataset objekteja, kuten map, filter, select, aggregate.
Toimet ovat operaatioita jotka suorittavat laskentaa ja palauttavat tuloksia.
Toimia ovat esimerkiksi count, show tai datan kirjoittaminen tiedostoj‰rjestelm‰‰n. \cite{spark-dataset}

Dataset-instanssit ovat laiskasti evaluoituvia, jolla tarkoitetaan sit‰, ett‰ laskenta aloitetaan vasta kun toimintoa kutsutaan tai instanssin arvoa tarvitaan.
Dataset on pohjimmiltaan looginen suunnitelma, jolla kuvataan datan tuottamiseen tarvittava laskenta.
Toimea kutsuttaessa, Sparkin kyselyoptimoija (query optimizer) optimoi loogisen suunnitelman ja generoi fyysisen suunnitelman.
Fyysinen suunnitelma takaa rinnakkaisesti ja hajautetusti tapahtuvan tehokkaan suorituksen.
Loogista suunnitelmaa, kuten myˆs optimoitua fyysist‰ suunnitelmaa, voidaan tutkia k‰ytt‰m‰ll‰ DS:n $explain$ funktiota. \cite{spark-dataset}

Domain-spesifisten olioiden tehokkaaseen tukemiseen tarvitaan enkooderia.
Enkooderilla tarkoitetaan ohjelmaa, joka muuntaa tietoa jonkin algoritmin mukaisesti ja t‰ss‰ tapauksessa sit‰ k‰ytet‰‰n yhdist‰m‰‰n domain-spesifinen tyyppi $T$ Sparkin sis‰iseen tyyppij‰rjestelm‰‰n.
Esimerkiksi luokan $Person$ tapauksessa, joka sis‰lt‰‰ kent‰t nimi (merkkijono) ja ik‰ (kokonaisluku), enkooderia voidaan k‰ytt‰‰ k‰skem‰‰n Sparkia luomaan koodia ajon aikana joka sarjallistaa $Person$ olion bin‰‰rirakenteeksi.
Generoidulla bin‰‰rirakenteella on usein pienempi muistijalanj‰lki ja se on myˆs optimoitu tehokkaaseen dataprosessointiin.
Datan bin‰‰riesitys voidaan tarkistaa k‰ytt‰m‰ll‰ DS:n tarjoamaa $schema$ funktiota. \cite{spark-dataset}

Dataset voidaan luoda tyypillisesti kahdella eri tavalla.
Yleisin tapa on k‰ytt‰‰ $SparkSession$:in tarjoamaa $read$ funktiota ja osoittaa Spark joihinkin tiedostoihin tiedostoj‰rjestelm‰ss‰, kuten seuraavaan $json$ tiedostoon.

\lstset{
	string=[s]{"}{"},
	stringstyle=\color{blue},
	comment=[l]{:},
	commentstyle=\color{black},
}

\begin{lstlisting}[caption=Esimerkki JSON tiedosto]

[{
  "name": "Matt",
  "salary": 5400
}, {
  "name": "George",
  "salary": 6000
}]

\end{lstlisting}


\lstset{
	frame=0,
	language=Scala,
	breaklines=true,
}

Dataset voidaan luoda myˆs tekem‰ll‰ muutoksia olemassaoleville Dataset olioille:

\begin{lstlisting}[caption=Creating a new Dataset through a transformation]

val people: Dataset<Person> = Dataset(Person())
val names = people.map(_.name)

\end{lstlisting}

\begin{lstlisting}[caption=Uuden Dataset olion luominen k‰ytt‰en read funktiota]

val people = spark.read.json("./people.json").as[Person]

\end{lstlisting}

jossa $Person$ olisi Scala case-luokka, esimerkiksi:

\begin{lstlisting}[caption=case class Person]

case class Person(id: BigInt, firstName: String, lastName: String)

\end{lstlisting}

Case-luokat ovat tavallisia Scala-luokkia jotka ovat:

\begin{itemize}
	\item Oletustarvoisesti muuttumattomia (immutable)
	\item Hajoitettavia (decomposable) hahmonsovitusta hyv‰ksik‰ytt‰en
	\item Vertailtavissa viitteiden sijasta rakenteellisen samankaltaisuuden mukaan
	\item Lyhyit‰ luoda (instantiate) ja k‰ytt‰‰
\end{itemize}

Mik‰li tyyppimuunnos (casting) j‰tett‰isiin tekem‰tt‰, p‰‰dytt‰isiin luomaan DataFrame olio, jonka sis‰inen mallin (schema) Spark pyrkisi arvaamaan.
DataFrame rajapintaa k‰sitell‰‰n seuraavassa aliluvussa.
Tyyppimuunnos tehd‰‰n k‰ytt‰m‰ll‰ $as$ avainsanaa.

\begin{lstlisting}[caption=SparkSession kontekstin luominen]
val spark = SparkSession
.builder
.appName("MovieLensALS")
.config("spark.executor.memory", "2g")
.getOrCreate()
\end{lstlisting}

SparkSession on Spark ohjelmoinnin l‰htˆkohta, kun halutaan k‰ytt‰‰ Dataset ja DataFrame rajapintoja.
Yll‰olevassa koodinp‰tk‰ss‰ luodaan $SparkSession$ ketjuttamalla rakentajan kutsuja.

\cite{spark-dataset}

Dataset oliot ovat samankaltaisia kuin RDD:t, sill‰ nekin tarjoavat vahvan tyypityksen ja mahdollisuuden k‰ytt‰‰ voimakkaita lambda-funktioita \cite{spark-sql-programming-guide}.
Lambda-funktiolla tarkoitetaan yleisesti anonyymi‰ funktiota, jota ei olla sidottu muuttujaan.
Perinteisen sarjallistamisen, kuten Java-sarjallistamisen, sijaan k‰ytet‰‰n erikoistunutta enkooderia olioiden sarjallistamiseen.
Serialisaatiolla tarkoitetaan olion muuntamista tavuiksi, jolloin olion muistijalanj‰lki pienenee.
Yleisesti sarjallistamista tarvitaan datan prosessointiin tai verkon yli l‰hett‰miseen.
Molempia, sek‰ enkoodereita ett‰ sarjallistamista k‰ytet‰‰n olioiden muuntamiseen tavuiksi, mutta enkooderit luodaan dynaamisesti koodissa.
Enkooderit k‰ytt‰v‰t sellaista muotoa, ett‰ Spark kykenee suorittamaan monenlaisia operaatioita, kuten suodattamista, j‰rjest‰mist‰ ja hajautusta (hashing), ilman ett‰ tavuja tarvitsee purkaa takaisin objektiksi. \cite{spark-programming-guide}

Seuraavassa koodilistauksessa luodaan uusi Dataset lukemalla $json$-tiedosto tiedostoj‰rjestelm‰st‰.
Seuraavaksi luodaan uusi Dataset muunnoksen kautta.
Objektin kloonaamiseksi k‰ytet‰‰n case luokan $copy$ metodia, koska $people$ Dataset oli m‰‰ritelty muuttumattomaksi.
Lopuksi fyysinen suunnitelma tulostetaan konsoliin kutsumalla $explain$ funktiota uudelle Dataset objektille.

\begin{lstlisting}[caption=Dataset olion loogisen ja fyysisen suunnitelman n‰ytt‰minen]

val people = spark.read.json("./people.json").as[Person]

val peopleWithDoubleSalary = people.map { person => 
	person.copy(salary = person.salary * 2)
}

peopleWithDoubleSalary.explain(true)

\end{lstlisting}

\begin{lstlisting}[caption=Dataset olion looginen suunnitelma]

== Optimized Logical Plan ==
SerializeFromObject [staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, assertnotnull(input[0, $line32.$read$$iw$$iw$Person, true], top level Product input object).name, true) AS name#67, staticinvoke(class org.apache.spark.sql.types.Decimal$, DecimalType(38,0), apply, assertnotnull(input[0, $line32.$read$$iw$$iw$Person, true], top level Product input object).salary, true) AS salary#68]
+- MapElements <function1>, class $line32.$read$$iw$$iw$Person, [StructField(name,StringType,true), StructField(salary,DecimalType(38,0),true)], obj#66: $line32.$read$$iw$$iw$Person
+- DeserializeToObject newInstance(class $line32.$read$$iw$$iw$Person), obj#65: $line32.$read$$iw$$iw$Person
+- Relation[name#55,salary#56L] json

\end{lstlisting}

\begin{lstlisting}[caption=Dataset olion fyysinen suunnitelma]

== Physical Plan ==
*SerializeFromObject [staticinvoke(class org.apache.spark.unsafe.types.UTF8String, StringType, fromString, assertnotnull(input[0, $line32.$read$$iw$$iw$Person, true], top level Product input object).name, true) AS name#67, staticinvoke(class org.apache.spark.sql.types.Decimal$, DecimalType(38,0), apply, assertnotnull(input[0, $line32.$read$$iw$$iw$Person, true], top level Product input object).salary, true) AS salary#68]
+- *MapElements <function1>, obj#66: $line32.$read$$iw$$iw$Person
+- *DeserializeToObject newInstance(class $line32.$read$$iw$$iw$Person), obj#65: $line32.$read$$iw$$iw$Person
+- *FileScan json [name#55,salary#56L] Batched: false, Format: JSON, Location: InMemoryFileIndex[file:/home/joonne/Documents/GitHub/thesis-code/people.json], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<name:string,salary:bigint>


\end{lstlisting}

\section{DataFrame API}

DataFrame on nimettyihin sarakkeisiin j‰rjestetty Dataset.
Se on k‰sitteellisesti yhtenev‰ relaatiotietokannan taulun tai R/Python kielten tietokehyksen (data frame) kanssa, mutta DataFrame on optimoitu tehokkaammin.
DataFrame voidaan rakentaa useammalla tavalla, kuten esimerkiksi j‰sennellyist‰ tiedostoista, ulkoisista tietokannoista tai olemassaolevista RDD-olioista.
DataFrame-rajapinta on saatavilla Scala, Java, Python ja R-ohjelmointikielille.
Scala-toteutuksessa DataFrame on riveist‰ rakentuva Dataset, se on siis yksinkertaisesti tyyppialias Dataset[Row] (kuten C++:n typedef). \cite{spark-programming-guide}

\begin{lstlisting}[caption=DataFrame luominen k‰ytt‰en read funktiota]

val people = spark.read.json("./people.json")

\end{lstlisting}

DataFrame oliota luotaessa, Spark arvaa luodun objektin sis‰isen mallin.

\begin{figure}[h]
	\caption{DataFrame}
	\centering
	\includegraphics[scale=1.0]{dataframe}
\end{figure}

\subsection{Alternating Least Squares (ALS)}

Yhteisˆllist‰ suodatusta k‰ytet‰‰n usein suosittelijaj‰rjestelmiss‰.
N‰m‰ tekniikat pyrkiv‰t t‰ytt‰m‰‰n k‰ytt‰j‰-tuote assosiaatiomatriisin puuttuvat kohdat.
Spark MLlib tukee mallipohjaista yhteisˆsuodatusta, jossa k‰ytt‰ji‰ ja tuotteita kuvaillaan pienell‰ m‰‰r‰ll‰ piilevi‰ tekijˆit‰, joita voidaan k‰ytt‰‰ puuttuvien kohtien ennustamiseen.
Spark MLlib k‰ytt‰‰ vaihtelevien pienimpien neliˆiden (Alternating Least Squares, ALS) algoritmia n‰iden piilevien tekijˆiden oppimiseen. \cite{spark14}

Spark ALS yritt‰‰ arvata arvostelumatriisin $A$ kahden alemman arvon matriisin, $X$ ja $Y$, tulona. \cite{als14}

\begin{equation}
A = XY^T
\end{equation}

Tyypillisesti n‰ihin arvioihin viitataan tekij‰matriiseina.
Perinteinen l‰hestymistapa on iteratiivinen.
Jokaisen iteraation aikana, toista tekij‰matriisia pidet‰‰n vakiona ja toinen ratkaistaan k‰ytt‰en pienimpien summien algoritmia.
Juuri ratkaistua tekij‰matriisia pidet‰‰n vuorostaan vakiona kun ratkaistaan toista tekij‰matriisia. \cite{als14}
Spark ALS mahdollistaa massiivisen rinnakkaistamisen sill‰ algoritmia voidaan suorittaa erikseen.
T‰m‰ on erinomainen ominaisuus laajamittaiselle (large-scale) laskenta-algoritmille. \cite{ryza15}

Spark ALS on lohkotettu versio ALS tekijˆihinjako algoritmista.
Ajatuksena on ryhmitt‰‰ kaksi tekij‰ryhm‰‰, $k‰ytt‰j‰t$ ja $tuotteet$, lohkoihin.
Ryhmitt‰mist‰ seuraa kommunikaation v‰hent‰minen l‰hett‰m‰ll‰ jokaiseen tuotelohkoon vain yksi kopio jokaisesta k‰ytt‰j‰vektorista iteraation aikana.
Vain ne k‰ytt‰j‰ vektorit l‰hetet‰‰n, joita tarvitaan tuotelohkoissa.
V‰hennetty kommunikaatio saavutetaan valmiiksi laskemalla joitain tietoja suositusmatriisista jotta voidaan p‰‰tell‰ jokaisen k‰ytt‰j‰n ulostulot ja jokaisen tuotteen sis‰‰ntulot.
Ulostulolla tarkoitetaan niit‰ tuotelohkoja, joihin k‰ytt‰j‰ tulee myˆt‰vaikuttamaan.
Sis‰‰ntulolla tarkoitetaan niit‰ ominaisuusvektoreita jotka jokainen tuote ottaa vastaan niilt‰ k‰ytt‰j‰lohkoilta joista ne ovat riippuvaisia.
T‰m‰ mahdollistaa sen, ett‰ voidaan l‰hett‰‰ vain taulukollinen ominaisuusvektoreita jokaisen k‰ytt‰j‰- ja tuotelohkon v‰lill‰.
Vastaavasti tuotelohko lˆyt‰‰ k‰ytt‰j‰n arviot ja p‰ivitt‰‰ tuotteita n‰iden viestien perusteella. \cite{als14}

Sen sijaan ett‰ etsitt‰isiin alemman tason arviot suositusmatriisille $A$, etsit‰‰nkin arviot mieltymysmatriisi $P$:lle, jossa $P$:n alkiot saavat arvon 1 kun $r > 0$ ja arvon 0 kun $r< = 0$.
Eksplisiittisen tuotearvion sijaan arvostelut kuvaavat k‰ytt‰j‰n mieltymyksen vahvuuden luottamusarvoa. \cite{als14}

\begin{equation}
A_iY(Y^T Y)^{-1} = X_i
\end{equation}

ALS operoi kiinnitt‰m‰ll‰ yhden tuntemattomista $u_i$ ja $v_j$ ja vaihtelemalla t‰t‰ kiinnitt‰mist‰.
Kun toinen on kiinnitetty, toinen voidaan laskea ratkaisemalla pienimpien neliˆiden ongelma.
T‰m‰ l‰hestymistapa on hyˆdyllinen, koska se muuttaa aiemman, ei-konveksin, ongelman neliˆm‰iseksi, jolloin se voidaan ratkaista optimaalisesti. \cite{aberger14}
Alla on \cite{aberger14} mukainen yleinen kuvaus ALS algoritmista:

\begin{lstlisting}[caption=Vaihtelevien pienimpien neliˆiden algoritmi (ALS) \cite{aberger14}]

1. Alusta matriisi V asettamalla ensimm‰iseksi riviksi elokuvan keskim‰‰r‰inen arvio ja pieni satunnaisluku j‰ljell‰oleviin alkioihin.

2. Kiinnit‰ V, ratkaise U minimoimalla RMSE funktio.

3. Kiinnit‰ U, ratkaise V minimoimalla RMSE funktio.

4. Toista askeleita 2 ja 3 konvergenssiin asti.

\end{lstlisting}

RMSE (Root Mean Square Error) on kenties suosituin ennustettujen arvosteluiden tarkkuuden evaluiointiin k‰ytetty metriikka.
Sit‰ k‰ytet‰‰n yleisesti regressioalgoritmien avulla luotujen mallien evaluointiin.
Regressioalgoritmien yhteydess‰ virheell‰ tarkoitetaan havainnon todellisen sek‰ ennustetun numeroarvon v‰list‰ eroa.
RMSE:n tuntemiseksi tulee tuntea ensin MSE (Mean Square Error).
Kuten nimi viittaa, MSE on virheiden neliˆiden keskiarvo ja se voidaan laskea neliˆim‰ll‰ jokaisen havainnon virhe ja laskemalla virheiden neliˆiden keskiarvo.
RMSE voidaan puolestaan laskea ottamalla neliˆjuuri MSE:st‰.
Sek‰ RMSE ett‰ MSE edustavat opetusvirhett‰ ja ne ilmoittavat kuinka hyvin malli sovittuu opetusdataan.
Niiden avulla saadaan selville havaintojen sek‰ ennustettujen arvojen v‰linen poikkeavuus.
Alhaisemman MSE:n tai RMSE:n omaavan mallin sanotaan sovittuvan paremmin opetusdataan kuin korkeammat virhearvot omaavan mallin. \cite{guller15}

Suositteluj‰rjestelm‰ luo ennustettuja arvosteluita $\hat{r}_{ui}$ testiaineistolle $\tau$ k‰ytt‰j‰-tuote pareja $(u,i)$ joille todelliset arviot $r$ tunnetaan. \cite{guller15}
Ennustettujen ja todellisten arvioiden v‰linen RMSE saadaan laskettua seuraavasti:

\begin{equation}
RMSE=\sqrt{\frac{1}{|\tau|} \sum_{(u,i)\in\tau}(\hat{r}_{ui}-r_{ui})^2}
\end{equation}

Konvergenssilla tarkoitetaan jonkin ilmiˆn l‰hestymist‰ ajan kuluessa jotain tietty‰ arvoa, t‰ss‰ tapauksessa sit‰, ett‰ RMSE ei en‰‰n pienene tarpeeksi.

\end{document}