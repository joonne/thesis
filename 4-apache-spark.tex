\documentclass[main.tex]{thesis.tex}
\begin{document}
	
\lstset{
	columns=flexible,
	breaklines=true,
	tabsize=2,
	language=Scala,
	commentstyle=\color{scalacomment},
	keywordstyle=\color{blue},
	numbers=left,
	numbersep=15pt,
}

\chapter{Apache Spark}

Apache Spark on sovelluskehys hajautettujen ohjelmien kirjoittamiseen. \cite{ryza15}.
Spark-ohjelmia voidaan kirjoittaa Java-, Scala-, Python- sekä R-ohjelmointikielillä.
Jokainen Spark-sovellus koostuu driver-ohjelmasta sekä yhdestä tai useammasta executor-ohjelmasta.
Driver on ohjelma, joka ajaa käyttäjän pääohjelmaa ja hajauttaa laskennan klusteriin.
Executor on yksi kone klusterissa. \cite{ryza15}

Spark voidaan esitellä kuvailemalla sen edeltäjää, MapReduce:a, ja sen tarjoamia etuja.
MapReduce tarjosi yksinkertaisen mallin ohjelmien kirjoittamiseen ja pystyi suorittamaan kirjoitettua ohjelmaa rinnakkain sadoilla tietokoneilla.
MapReduce skaalautuu lähes lineaarisesti datan koon kasvaessa.
Suoritusaikaa hallitaan lisäämällä lisää tietokoneita suorittamaan tehtävää. \cite{ryza15}

Apache Spark säilyttää MapReduce:n lineaarisen skaalautuvuuden ja vikasietokyvyn mutta laajentaa sitä kolmella merkittävällä tavalla.
Ensiksi, MapReducessa map- ja reduce-tehtävien väliset tulokset täytyy kirjoittaa levylle kun taas Spark kykenee välittämään tulokset suoraan liukuhihnan (pipeline) seuraavalle vaiheelle.
Toiseksi, Apache Sparkin voidaan ajatella kohtelevan kehittäjiä paremmin tarjoamalla monipuolisen joukon transformataatioita, joiden avulla voidaan muutamalla koodirivillä ilmaista monimutkaisia liukuhihnoja (pipelines, ohjelmistoja).
Kolmanneksi, Spark esittelee muistissa tapahtuvan prosessoinnin tarjoamalla abstraktion nimeltä \textit{Resilient Distributed Dataset (RDD)}.
RDD tarjoaa kehittäjälle mahdollisuuden materialisoida minkä tahansa askeleen liukuhihnassa ja tallentaa sen muistiin.
Tämä tarkoittaa sitä, että tulevien askelien ei tarvitse laskea aiempia tuloksia uudelleen ja tällöin on mahdollista jatkaa juuri käyttäjän haluamasta askeleesta.
Aiemmin tämänkaltaista ominaisuutta ei ole ollut saatavilla hajautetun laskennan järjestelmissä. \cite{ryza15}

Vaikka Spark-ohjelmia voidaankin kirjottaa usealla ohjelmointikielellä, Scalan käyttämisellä saavutetaan kuitenkin muutamia etuja, joita muut kielet eivät tarjoa.
Tehokkuus saattaa parantua, sillä datan siirtäminen eri kerrosten välillä tai muunnosten suorittaminen datalle voi johtaa heikompaan tehokkuuteen.
Spark on kirjoitettu Scala-ohjelmointikielellä, joten viimeisimmät ja parhaimmat (latest and greatest) ominaisuudet ovat aina käytössä, eikä niiden käännöstä tarvitse odotella.
Spark ohjelmoinnin filosofia on helpompi ymmärtää kun Sparkia käytetään kielellä, jolla se on rakennettu.
Suurin hyöty, jonka Scalan käyttäminen tarjoaa, on kuitenkin kehittäjäkokemus joka tulee saman ohjelmointikielen käyttämisestä kaikkeen.
Datan tuonti, manipulointi ja koodin lähettäminen klustereihin hoituvat samalla kielellä. \cite{ryza15}

Spark-jakelun mukana toimitetaan luku-evaluointi-tulostus-silmukka (Read Eval Print Loop, REPL).
REPL on komentorivityökalu, joka mahdollistaa uusien asioiden nopean testailun konsolissa, eikä sovelluksista tarvitse rakentaa itsenäisiä (self-contained) alusta asti.
Kun REPLissä kehitetyn sovelluksen tai sovelluksen osan voidaan katsoa olevan tarpeeksi valmis, on järkevää tehdä siitä koottu kirjasto (JAR).
Näin varmistutaan ettei ohjelmakoodia tai tuloksia pääse katoamaan, vaikkakin REPL tarjoaa samantapaisen muistin komentohistoriasta kuin perinteinen komentorivikin.

JAR eli Java ARchive on suosittuun ZIP-tiedostoformaattiin perustuva alustariippumaton tiedostoformaatti, jota käytetään kokoamaan monta tiedostoa yhdeksi tiedostoksi. \cite{JAR}
JVM (Java Virtual Machine, Java-virtuaalikone) on abstrakti tietokone.
Kuten oikea tietokone, se omaa käskykannan ja muokkaa useita muistialueita ajon aikana.
JVM ei tiedä mitään ohjelmointikielistä, kuten Scala tai Java, vaan se operoi ainoastaan \textit{class}-tiedostoilla, jotka ovat binääritiedostoja.
Class-tiedosto sisältää JVM-käskyt sekä symbolitaulun. \cite{JVM}

\section{Resilient Distributed Dataset API (RDD API)}

Resilient Distributed Dataset (RDD) on Sparkin tarjoama pääabstraktio.
RDD on muuttumaton, partitioitu elementtikokoelma, joka voidaan hajauttaa klusterin useiden koneiden välillä. \cite{spark-rdd}

RDD:t tukevat kahdenlaisia operaatioita: transformaatioita ja toimia (actions).
Transformaatioilla luodaan uusia datasettejä olemassaolevista dataseteistä.
Toimet palauttavat arvon driver-ohjelmaan laskennan valmistuttua.
$map$-operaatio on esimerkki transformaatiosta ja $reduce$-operaatio on esimerkki toimesta.
Molemmat operaatiot käyvät annetun datasetin läpi ja kohdistavat annetun funktion jokaiselle elementille, mutta $map$ palauttaa uuden RDD:n ja $reduce$ palauttaa tuloksensa driver-ohjelmalle. \cite{spark-programming-guide}

Kaikki transformaatiot Sparkissa ovat laiskasti evaluoituvia, jolla tarkoitetaan sitä, että lausekkeen evaluointia viivytetään siihen asti kun sen arvoa tarvitaan.
Kun uusi RDD luodaan, mitään laskentaa ei oikeasti vielä tapahdu, vaan Spark tietää missä data sijaitsee tai miten data saadaan laskettua kun tulee aika tehdä sille jotain.
Tämä suunnittelu mahdollistaa Sparkin tehokkaamman toiminnan.
Esimerkiksi $map$ ja $reduce$ -operaatioista koostuva liukuhihna suoriutuu tehokkaammin, sillä $map$-transformaation luoma datasetti voidaan jättää palauttamatta driver-ohjelmaan, koska $reduce$-toimen vastaus palautuu.
\cite{spark-sql-programming-guide}

RDD voidaan luoda kahdella tavalla, rinnakkaistamalla (parallelize) tai viittaamalla ulkoiseen aineistoon.
Rinnakkaistamisessa olemassaoleva Scala-kokoelma saadaan muunnettua RDD:ksi.
Ulkoiseen ainestoon viittaamisella tarkoitetaan viittaamista aineistoon ulkoisessa varastointijärjestelmässä kuten \textit{HDFS}:sä tai \textit{HBase}:ssa. \cite{spark-programming-guide}

\begin{figure}[h]
	\caption{Directed Acyclic Graph (muokattu lähteestä \cite{dag-image})}
	\centering
	\includegraphics[scale=0.17]{directed_acyclic_graph2}
\end{figure}

RDD:t voidaan tallentaa muistiin, jolloin ohjelmistokehittäjä voi uudelleenkäyttää niitä tehokkaasti rinnakkaisissa operaatioissa.
RDD:t voivat palautua solmuvirheistä automaattisesti käyttäen Directed Acyclic Graph (DAG) -moottoria.
DAG tukee asyklistä datavirtaa, jolla tarkoitetaan sitä, että jokainen graafin kaari kulkee topologisessa järjestyksessä aiemmasta myöhempään.
Jokaista Spark-työtä kohti luodaan DAG klusterissa suoritettavan tehtävän tasoista.
Verrattuna MapReduceen, joka luo DAGin kahdesta ennaltamäärätystä tilasta (Map ja Reduce), Sparkin luomat DAGit voivat sisältää minkä tahansa määrän tasoja.
Tästä syystä jotkin työt voivat valmistua nopeammin kuin ne valmistuisivat MapReducessa.
Yksinkertaisimmat työt voivat valmistua vain yhden tason jälkeen ja monimutkaisemmat tehtävät valmistuvat yhden monitasoisen ajon jälkeen, ilman että niitä täytyy pilkkoa useampiin töihin. \cite{mapRSpark}

\section{Dataset API}

Dataset (DS) on vahvasti tyypitetty kokoelma domain spesifisiä objekteja, jotka voidaan muuntaa rinnakkain käyttäen funktionaalisia tai relaatio-operaatioita.
DS on kehitetty korvaamaan RDD Sparkissa.
Dataset:ille olemassa olevat operaatiot on jaettu \textit{transformaatioihin} (transformations) ja \textit{toimiin} (actions).
Transformaatiot ovat operaatioita, jotka luovat uusia Dataset-objekteja, kuten $map$, $filter$, $select$ ja $aggregate$.
Toimet ovat operaatioita jotka suorittavat laskentaa ja palauttavat tuloksia.
Toimia ovat esimerkiksi \textit{count}, \textit{show} tai datan kirjoittaminen tiedostojärjestelmään. \cite{spark-dataset}

Kuten RDD:t, Dataset-instanssit ovat laiskasti evaluoituvia, jolla tarkoitetaan sitä, että laskenta aloitetaan vasta kun toimintoa kutsutaan tai instanssin arvoa tarvitaan.
Dataset on pohjimmiltaan looginen suunnitelma, jolla kuvataan datan tuottamiseen tarvittava laskenta.
Toimea kutsuttaessa, Sparkin kyselyoptimoija (query optimizer) optimoi loogisen suunnitelman ja generoi fyysisen suunnitelman.
Fyysinen suunnitelma takaa rinnakkaisesti ja hajautetusti tapahtuvan tehokkaan suorituksen.
Loogista suunnitelmaa, kuten myös optimoitua fyysistä suunnitelmaa, voidaan tutkia käyttämällä DS:n $explain$ funktiota. \cite{spark-dataset}

Domain-spesifisten olioiden tehokkaaseen tukemiseen tarvitaan enkooderia.
Enkooderilla tarkoitetaan ohjelmaa, joka muuntaa tietoa jonkin algoritmin mukaisesti ja tässä tapauksessa sitä käytetään yhdistämään domain-spesifinen tyyppi $T$ Sparkin sisäiseen tyyppijärjestelmään.
Esimerkiksi luokan $Person$ tapauksessa, joka sisältää kentät nimi (merkkijono) ja ikä (kokonaisluku), enkooderia voidaan käyttää käskemään Sparkia luomaan koodia ajon aikana joka sarjallistaa $Person$ olion binäärirakenteeksi.
Generoidulla binäärirakenteella on usein pienempi muistijalanjälki ja se on myös optimoitu tehokkaaseen dataprosessointiin.
Datan binääriesitys voidaan tarkistaa käyttämällä DS:n tarjoamaa $schema$ funktiota. \cite{spark-dataset}

Dataset voidaan luoda tyypillisesti kahdella eri tavalla.
Yleisin tapa on käyttää $SparkSession$:in tarjoamaa $read$ funktiota ja osoittaa Spark joihinkin tiedostoihin tiedostojärjestelmässä, kuten seuraavaan $json$ tiedostoon:

\newpage

\lstset{
	string=[s]{"}{"},
	stringstyle=\color{blue},
	comment=[l]{:},
	commentstyle=\color{black},
}

\begin{lstlisting}[caption=Esimerkki JSON-tiedosto]
[{
  "name": "Matt",
  "salary": 5400
}, {
  "name": "George",
  "salary": 6000
}]

\end{lstlisting}


\lstset{
	frame=0,
	language=Scala,
	breaklines=true,
}

Dataset voidaan luoda myös tekemällä transformaatio olemassaoleville Dataset-olioille:

\begin{lstlisting}[caption=Uuden Dataset-olion luominen transformaatiolla]

val people: Dataset[Person] = Dataset(Person())
val names = people.map(_.name)

\end{lstlisting}

\begin{lstlisting}[caption=Uuden Dataset-olion luominen käyttäen read funktiota]

case class Person(id: BigInt, firstName: String, lastName: String)
val people = spark.read.json("./people.json").as[Person]

\end{lstlisting}

Case-luokat ovat tavallisia Scala-luokkia, jotka ovat:

\begin{itemize}
	\item Oletustarvoisesti muuttumattomia (immutable)
	\item Hajoitettavia (decomposable) hahmonsovituksen (pattern matching) avulla
	\item Vertailtavissa viitteiden sijasta rakenteellisen samankaltaisuuden mukaan
	\item Yksinkertaisempia luoda (instantiate) ja käyttää
\end{itemize}

Mikäli tyyppimuunnos (casting) jätettäisiin tekemättä, päädyttäisiin luomaan DataFrame-olio, jonka sisäinen mallin (schema) Spark pyrkisi arvaamaan.
DataFrame rajapintaa käsitellään seuraavassa aliluvussa.
Tyyppimuunnos tehdään käyttämällä $as$ avainsanaa.

\begin{lstlisting}[caption=SparkSession-kontekstin luominen]
val spark = SparkSession
	.builder
	.appName("MovieLensALS")
	.getOrCreate()
\end{lstlisting}

\textit{SparkSession} on Spark-ohjelmoinnin lähtökohta, kun halutaan käyttää Dataset- ja DataFrame-rajapintoja.
Ylläolevassa koodinpätkässä luodaan SparkSession ketjuttamalla rakentajan kutsuja. \cite{spark-dataset}

Dataset-oliot ovat samankaltaisia kuin RDD:t, sillä nekin tarjoavat vahvan tyypityksen ja mahdollisuuden käyttää voimakkaita lambda-funktioita \cite{spark-sql-programming-guide}.
Lambda-funktiolla tarkoitetaan yleisesti anonyymiä funktiota, jota ei olla sidottu muuttujaan.
Perinteisen sarjallistamisen, kuten Java-sarjallistamisen, sijaan käytetään erikoistunutta enkooderia olioiden sarjallistamiseen.
Sarjallistamisella tarkoitetaan olion muuntamista tavuiksi, jolloin olion muistijalanjälki pienenee.
Yleisesti sarjallistamista tarvitaan datan prosessointiin tai verkon yli lähettämiseen.
Molempia, sekä enkoodereita että sarjallistamista käytetään olioiden muuntamiseen tavuiksi, mutta enkooderit luodaan dynaamisesti koodissa.
Enkooderit käyttävät sellaista muotoa, että Spark kykenee suorittamaan monenlaisia operaatioita, kuten suodattamista, järjestämistä ja hajautusta (hashing), ilman että tavuja tarvitsee purkaa takaisin olioksi. \cite{spark-programming-guide}

Seuraavassa koodilistauksessa luodaan uusi Dataset lukemalla $json$-tiedosto tiedostojärjestelmästä.
Seuraavaksi luodaan uusi Dataset muunnoksen kautta.
Olion kloonaamiseksi käytetään case class :n $copy$ metodia, koska case class instanssit ovat muuttumattomia ja näin ollen copy-metodi on ainoa tapa kloonata (luoda uusi muuttumaton, immutable instanssi) siitä.
Lopuksi looginen- ja fyysinen suunnitelma tulostetaan konsoliin kutsumalla $explain$-funktiota uudelle Dataset-oliolle.

\begin{lstlisting}[caption=Dataset-esimerkki]
val people = spark.read.json("./people.json").as[Person]

val peopleWithDoubleSalary = people.map { person => 
	person.copy(salary = person.salary * 2)
}

peopleWithDoubleSalary.explain(true)
\end{lstlisting}

\section{DataFrame API}

\textit{DataFrame} on nimettyihin sarakkeisiin järjestetty Dataset.
Se on käsitteellisesti yhtenevä relaatiotietokannan taulun tai R/Python kielten tietokehyksen (data frame) kanssa. \cite{spark-sql-programming-guide}

\begin{figure}[h]
	\caption{DataFrame}
	\centering
	\includegraphics[scale=1.0]{dataframe}
\end{figure}

DataFrame voidaan rakentaa useammalla tavalla, kuten esimerkiksi jäsennellyistä tiedostoista, hakemalla ja viittaamalla ulkoisista tietokannoista tai olemassaolevista RDD-olioista.
Se mahdollistaa datan prosessoinnin kilobittien suuruusluokasta aina petabitteihin asti ja klusterin yksittäisen solmun klustereista suuriin klustereihin.
DataFrame-rajapinta on saatavilla Scala-, Java-, Python- ja R-ohjelmointikielille.
Kooditasolla Scala-toteutuksessa DataFrame on riveistä rakentuva Dataset ($Dataset[Row]$). \cite{spark-programming-guide} \cite{spark_sql_dataframes}

Alla olevassa ohjelmassa luodaan uusi DataFrame tietorakenne kohdistamalla \textit{spark.read} -funktio JSON-tiedostoon.
Tämän seurauksena Spark pyrkii päättelemään uuden DataFrame:n sisäisen mallin.

\begin{lstlisting}[caption=DataFrame luominen käyttäen read-funktiota]
val people = spark.read.json("./people.json")
\end{lstlisting}

DataFrame API tarjoaa domain-spesifisen kielen (DSL, domain-specific language) jäsennellyn datan, kuten JSON:in manipulointiin:

\begin{lstlisting}[caption=Sarakkeen valitseminen ja tulostaminen DataFrame DSL:n avulla]
people.select("name").show()
\end{lstlisting}

Yllä olevassa esimerkissä valitaan luodusta DataFrame:sta vain $name$ -sarake ja tulostetaan se konsoliin. \cite{spark-sql-programming-guide}

\section{Mllib}

MLlib on Sparkin koneoppimiskirjasto.
Projektin tavoitteena on tehdä käytännönläheisestä koneoppimisesta skaalautuvaa ja helppoa.
MLlib tarjoaa muun muassa seuraavanlaisia palveluita:

\begin{itemize}
	\item \textbf{Koneoppimisalgoritmit} oppimisalgoritmeja kuten luokittelua, regressiota, klusterointia ja yhteisösuodatusta
	\item \textbf{Featurization} piirreirroituis (feature extraction), transformaatiot, dimensioiden vähentäminen (dimensionality reduction) -ja valitseminen
	\item \textbf{Liukuhihnat} työkalut koneoppimisliukuhihnojen rakentamiseen, arviointiin ja muokkaamiseen
	\item \textbf{Persistointi} algoritmien, mallien ja liukuhihnojen tallentaminen ja lataaminen
	\item \textbf{Apuohjelmat} esimerkiksi lineaari algebra, statistiikka ja datan käsittely
\end{itemize}

\cite{mllib}

Tässä työssä tutustutaan MLlib-kirjaston tarjoamaan ALS-kirjastoon, jolla suositukset tarjoava koneoppimismalli opetetaan.

\end{document}