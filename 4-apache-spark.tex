\documentclass[main.tex]{thesis.tex}
\begin{document}
	
\lstset{
	columns=flexible,
	breaklines=true,
	tabsize=2,
	language=Scala,
	commentstyle=\color{scalacomment},
	keywordstyle=\color{blue},
	numbers=left,
	numbersep=15pt,
}

\chapter{Apache Spark}

Apache Spark on sovelluskehys hajautettujen ohjelmien kirjoittamiseen. \cite{ryza15}.
Spark-ohjelmia voidaan kirjoittaa Java-, Scala-, Python- sek‰ R-ohjelmointikielill‰.
Jokainen Spark-sovellus koostuu driver-ohjelmasta sek‰ yhdest‰ tai useammasta executor-ohjelmasta.
Driver on ohjelma, joka ajaa k‰ytt‰j‰n p‰‰ohjelmaa ja hajauttaa laskennan klusteriin.
Executor on yksi kone klusterissa. \cite{ryza15}

Spark voidaan esitell‰ kuvailemalla sen edelt‰j‰‰, MapReduce:a, ja sen tarjoamia etuja.
MapReduce tarjosi yksinkertaisen mallin ohjelmien kirjoittamiseen ja pystyi suorittamaan kirjoitettua ohjelmaa rinnakkain sadoilla tietokoneilla.
MapReduce skaalautuu l‰hes lineaarisesti datan koon kasvaessa.
Suoritusaikaa hallitaan lis‰‰m‰ll‰ lis‰‰ tietokoneita suorittamaan teht‰v‰‰. \cite{ryza15}

Apache Spark s‰ilytt‰‰ MapReduce:n lineaarisen skaalautuvuuden ja vikasietokyvyn mutta laajentaa sit‰ kolmella merkitt‰v‰ll‰ tavalla.
Ensiksi, MapReducessa map- ja reduce-teht‰vien v‰liset tulokset t‰ytyy kirjoittaa levylle kun taas Spark kykenee v‰litt‰m‰‰n tulokset suoraan liukuhihnan (pipeline) seuraavalle vaiheelle.
Toiseksi, Apache Sparkin voidaan ajatella kohtelevan kehitt‰ji‰ paremmin tarjoamalla monipuolisen joukon transformataatioita, joiden avulla voidaan muutamalla koodirivill‰ ilmaista monimutkaisia liukuhihnoja (pipelines, ohjelmistoja).
Kolmanneksi, Spark esittelee muistissa tapahtuvan prosessoinnin tarjoamalla abstraktion nimelt‰ \textit{Resilient Distributed Dataset (RDD)}. RDD tarjoaa kehitt‰j‰lle mahdollisuuden materialisoida mink‰ tahansa askeleen liukuhihnassa ja tallentaa sen muistiin.
T‰m‰ tarkoittaa sit‰, ett‰ tulevien askelien ei tarvitse laskea aiempia tuloksia uudelleen ja t‰llˆin on mahdollista jatkaa juuri k‰ytt‰j‰n haluamasta askeleesta.
Aiemmin t‰m‰nkaltaista ominaisuutta ei ole ollut saatavilla hajautetun laskennan j‰rjestelmiss‰. \cite{ryza15}

Vaikka Spark-ohjelmia voidaankin kirjottaa usealla ohjelmointikielell‰, Scalan k‰ytt‰misell‰ saavutetaan kuitenkin muutamia etuja, joita muut kielet eiv‰t tarjoa.
Tehokkuus saattaa parantua, sill‰ datan siirt‰minen eri kerrosten v‰lill‰ tai muunnosten suorittaminen datalle voi johtaa heikompaan tehokkuuteen.
Spark on kirjoitettu Scala-ohjelmointikielell‰, joten viimeisimm‰t ja parhaimmat (latest and greatest) ominaisuudet ovat aina k‰ytˆss‰, eik‰ niiden k‰‰nnˆst‰ tarvitse odotella.
Spark ohjelmoinnin filosofia on helpompi ymm‰rt‰‰ kun Sparkia k‰ytet‰‰n kielell‰, jolla se on rakennettu.
Suurin hyˆty, jonka Scalan k‰ytt‰minen tarjoaa, on kuitenkin kehitt‰j‰kokemus joka tulee saman ohjelmointikielen k‰ytt‰misest‰ kaikkeen.
Datan tuonti, manipulointi ja koodin l‰hett‰minen klustereihin hoituvat samalla kielell‰. \cite{ryza15}

Spark-jakelun mukana toimitetaan luku-evaluointi-tulostus-silmukka (Read Eval Print Loop, REPL).
REPL on komentorivityˆkalu, joka mahdollistaa uusien asioiden nopean testailun konsolissa, eik‰ sovelluksista tarvitse rakentaa itsen‰isi‰ (self-contained) alusta asti.
Kun REPLiss‰ kehitetyn sovelluksen tai sovelluksen osan voidaan katsoa olevan tarpeeksi valmis, on j‰rkev‰‰ tehd‰ siit‰ koottu kirjasto (JAR).
N‰in varmistutaan ettei ohjelmakoodia tai tuloksia p‰‰se katoamaan, vaikkakin REPL tarjoaa samantapaisen muistin komentohistoriasta kuin perinteinen komentorivikin.

JAR eli Java ARchive on suosittuun ZIP-tiedostoformaattiin perustuva alustariippumaton tiedostoformaatti, jota k‰ytet‰‰n kokoamaan monta tiedostoa yhdeksi tiedostoksi. \cite{JAR}

JVM (Java Virtual Machine, Java-virtuaalikone) on abstrakti tietokone.
Kuten oikea tietokone, se omaa k‰skykannan ja muokkaa useita muistialueita ajon aikana.
JVM ei tied‰ mit‰‰n ohjelmointikielist‰, kuten Scala tai Java, vaan se operoi ainoastaan \textit{class}-tiedostoilla, jotka ovat bin‰‰ritiedostoja.
Class-tiedosto sis‰lt‰‰ JVM-k‰skyt sek‰ symbolitaulun. \cite{JVM}

\section{Scala}

Scala on moniparadigmainen ohjelmointikieli, joka tukee sek‰ olio- ett‰ funktionaalista ohjelmointia.
Funktionaalista ohjelmointia varten Scalasta lˆytyy tuki funktionaalisen ohjelmoinnin k‰sitteille, kuten muuttumattomat tietorakenteet ja funktiot ensimm‰isen luokan kansalaisina.
Olio-ohjelmointia varten Scalasta lˆytyy tuki k‰sitteille kuten luokat, oliot ja piirre (trait).
Scala tukee myˆs kapselointia, perint‰‰, moniperint‰‰ ja muita t‰rkeit‰ olio-ohjelmoinnin konsepteja.
Scala on staattisesti tyypitetty kieli ja sill‰ kirjoitetut ohjelmat k‰‰nnet‰‰n Scala-k‰‰nt‰j‰‰ k‰ytt‰en.
Scala on JVM-perustainen (Java Virtual Machine, Java-virtuaalikone) kieli, joten Scala k‰‰nt‰j‰ k‰‰nt‰‰ sovelluksen Java-tavukoodiksi, joka voidaan ajaa miss‰ tahansa Java-virtuaalikoneessa.
Tavukooditasolla Scala ohjelmaa ei voida erottaa Java sovelluksesta.
Scalan ollessa JVM-perustainen, Scala on t‰ysin yhteensopiva Javan kanssa ja n‰in ollen Java-kirjastoja voidaan k‰ytt‰‰ suoraan Scala-koodissa.
T‰st‰ syyst‰ Scala-sovellukset hyˆtyv‰t suuresta Java-koodin m‰‰r‰st‰.
Vaikka Scala tukee sek‰ olio- ett‰ funktionaalista ohjelmointia, funktionaalista ohjelmointia suositaan. \cite{guller15}

\subsection{Perustyypit}

Scalan perustyypit numeroiden esitt‰miseen ovat Byte, Short, Int, Long, Float ja Double.
Lis‰ksi Scalassa on perustyypit Char, String ja Boolean.
Char on 16 bittinen etumerkitˆn Unicode-merkki.
String on jono Char:eja.
Boolean esitt‰‰ totuusarvoa tosi (true) tai ep‰tosi (false). \cite{guller15}

Javasta poiketen Scalassa ei ole ollenkaan primitiivisi‰ tyyppej‰ vaan jokainen tyyppi on toteutettu luokkana.
K‰‰nnˆksen aikana k‰‰nt‰j‰ tarvittaessa automaattisesti muuntaa Scala tyypit Javan primitiivisiksi tyypeiksi. \cite{guller15}

\subsection{Muuttujat}

Scalassa on kahdentyyppisi‰ muuttujia: muuttuvia ja vakioita.
Muuttuva muuttuja m‰‰ritell‰‰n avainsanan $var$ avulla.
Muuttuvaa muuttujaa ei voida asettaa uudelleen luomisen j‰lkeen.
Var:ien k‰yttˆ‰ ei suositella, mutta joskus niiden k‰ytt‰misell‰ saadaan aikaan yksinkertaisempaa ohjelmakoodia ja t‰st‰ syyst‰ Scala tukee myˆs muuttuvia muuttujia.
Vakiota, $val$, ei sen sijaan voida antaa uudelleen luomisen j‰lkeen. \cite{guller15}

Syntaksi $val$:in ja $var$:in luomiseksi on

\begin{lstlisting}[caption=Muuttujien luominen ja uudelleen asettaminen]
var x = 10
x = 20
val y = 10
\end{lstlisting}

Mik‰li vakiota yritett‰‰n uudelleenm‰‰ritt‰‰ myˆhemmin ohjelmassa, k‰‰nt‰j‰ antaa virheen.
Huomionarvoista yll‰olevassa syntaksissa on se, ett‰ Scala k‰‰nt‰j‰ ei pakota m‰‰rittelem‰‰n muuttujan tyyppi‰ sillon kuin k‰‰nt‰j‰ pystyy p‰‰ttelem‰‰n (type deduction) sen.

\begin{lstlisting}[caption=Muuttujan luominen tyyppim‰‰rittelyn avulla]
var x: Int = 10
val y: Int = 10
\end{lstlisting}

\subsection{Funktiot}

Funktio on lohko suoritettavaa koodia joka palauttaa arvon.
Se on konseptuaalisesti samankaltainen kuin matematiikassa: funktio ottaa sis‰‰ntulon ja palauttaa ulostulon. \cite{guller15}

Scalan funktiot ovat ensimm‰isen luokan kansalaisia, jolla tarkoitetaan, ett‰ funktiota voidaan:

\begin{itemize}
	\item k‰ytt‰‰ kuten muuttujaa
	\item antaa syˆtteen‰ toiselle funktiolle
	\item m‰‰ritell‰ nimettˆm‰n‰ funktioliteraalina
	\item asettaa muuttujaan
	\item m‰‰ritell‰ toisen funktion sis‰ll‰
	\item palauttaa toisen funktion ulostulona
\end{itemize}

\cite{guller15}

Scalassa funktio m‰‰ritell‰‰n avainsanalla $def$.
Funktion m‰‰rittely aloitetaan funktion nimell‰, jota seuraa sulkeissa olevat, pilkulla erotetut, parametrit tyyppim‰‰rittelyineen.
Parametrien j‰lkeen funktiom‰‰rittelyyn tulee kaksoispiste, funktion ulostulon tyyppi, yht‰suuruusmerkki sek‰ funktion runko joko aaltosulkeissa tai ilman. \cite{guller15}

\lstset{language=Scala,tabsize=2}

\begin{lstlisting}[caption=Funktio]
def add(first: Int, second: Int): Int = {
	val sum = first + second
	return sum
}
\end{lstlisting}

Yll‰olevassa esimerkiss‰ funktion nimi on $add$ ja se ottaa kaksi $Int$ tyyppist‰ sis‰‰ntuloa.
Funktio palauttaa $Int$ tyyppisen arvon jonka se muodostaa lis‰‰m‰ll‰ annetut sis‰‰ntulot yhteen ja palauttamalla tuloksen.

Scala sallii myˆs lyhyemm‰n version samasta funktiosta:

\lstset{language=Scala,tabsize=2}

\begin{lstlisting}[caption=Funktio]
def add(first: Int, second: Int) = first + second
\end{lstlisting}

Toinen versio tekee t‰sm‰lleen saman asian kuin ensimm‰inenkin, mutta se on vain kirjoitettu lyhyemmin.
Paluuarvon tyyppi on j‰tetty antamatta, sill‰ k‰‰nt‰j‰ pystyy p‰‰ttelem‰‰n sen koodista.
Paluuarvon tyyppi suositellaan kuitenkin annettavan aina.
Aaltosulkeet on myˆskin j‰tetty pois, sill‰ ne ovat pakolliset vain kun funktion runko sis‰lt‰‰ useamman kuin yhden lausekkeen.
Lis‰ksi, $return$ avainsana on ohitettu, sill‰ se on vapaaehtoinen.
Scalassa kaikki lausekkeet ovat arvon palauttavia lausekkeita, joten funktion rungon viimeisen lausekkeen arvosta tulee funktion paluuarvo. \cite{guller15}

\section{Resilient Distributed Dataset (RDD)}

Resilient Distributed Dataset (RDD) on Sparkin tarjoama p‰‰abstraktio.
RDD on muuttumaton, partitioitu elementtikokoelma, joka voidaan hajauttaa klusterin useiden koneiden v‰lill‰. \cite{spark-rdd}

RDD:t tukevat kahdenlaisia operaatioita: transformaatioita ja toimia (actions).
Transformaatioilla luodaan uusia datasettej‰ olemassaolevista dataseteist‰.
Toimet palauttavat arvon driver-ohjelmaan laskennan valmistuttua.
$map$-operaatio on esimerkki transformaatiosta ja $reduce$-operaatio on esimerkki toimesta.
Molemmat operaatiot k‰yv‰t annetun datasetin l‰pi ja kohdistavat annetun funktion jokaiselle elementille, mutta $map$ palauttaa uuden RDD:n ja $reduce$ palauttaa tuloksensa driver-ohjelmalle. \cite{spark-programming-guide}

Kaikki transformaatiot Sparkissa ovat laiskasti evaluoituvia, jolla tarkoitetaan sit‰, ett‰ lausekkeen evaluointia viivytet‰‰n siihen asti kun sen arvoa tarvitaan.
Kun uusi RDD luodaan, mit‰‰n laskentaa ei oikeasti viel‰ tapahdu, vaan Spark tiet‰‰ miss‰ data sijaitsee tai miten data saadaan laskettua kun tulee aika tehd‰ sille jotain.
T‰m‰ suunnittelu mahdollistaa Sparkin tehokkaamman toiminnan.
Esimerkiksi $map$ ja $reduce$ -operaatioista koostuva liukuhihna suoriutuu tehokkaammin, sill‰ $map$-transformaation luoma datasetti voidaan j‰tt‰‰ palauttamatta driver-ohjelmaan, koska $reduce$-toimen vastaus palautuu.
\cite{spark-sql-programming-guide}

RDD voidaan luoda kahdella tavalla, rinnakkaistamalla (parallelize) tai viittaamalla ulkoiseen aineistoon.
Rinnakkaistamisessa olemassaoleva Scala-kokoelma saadaan muunnettua RDD:ksi.
Ulkoiseen ainestoon viittaamisella tarkoitetaan viittaamista aineistoon ulkoisessa varastointij‰rjestelm‰ss‰ kuten \textit{HDFS}:s‰ tai \textit{HBase}:ssa. \cite{spark-programming-guide}

RDD:t voidaan tallentaa muistiin, jolloin ohjelmistokehitt‰j‰ voi uudelleenk‰ytt‰‰ niit‰ tehokkaasti rinnakkaisissa operaatioissa.
RDD:t voivat palautua solmuvirheist‰ automaattisesti k‰ytt‰en Directed Acyclic Graph (DAG) -moottoria.
DAG tukee asyklist‰ datavirtaa, jolla tarkoitetaan sit‰, ett‰ jokainen graafin kaari kulkee topologisessa j‰rjestyksess‰ aiemmasta myˆhemp‰‰n.
Jokaista Spark-tyˆt‰ kohti luodaan DAG klusterissa suoritettavan teht‰v‰n tasoista.
Verrattuna MapReduceen, joka luo DAGin kahdesta ennaltam‰‰r‰tyst‰ tilasta (Map ja Reduce), Sparkin luomat DAGit voivat sis‰lt‰‰ mink‰ tahansa m‰‰r‰n tasoja.
T‰st‰ syyst‰ jotkin tyˆt voivat valmistua nopeammin kuin ne valmistuisivat MapReducessa.
Yksinkertaisimmat tyˆt voivat valmistua vain yhden tason j‰lkeen ja monimutkaisemmat teht‰v‰t valmistuvat yhden monitasoisen ajon j‰lkeen, ilman ett‰ niit‰ t‰ytyy pilkkoa useampiin tˆihin. \cite{mapRSpark}

\begin{figure}[h]
	\caption{Directed Acyclic Graph \cite{dag-image}}
	\centering
	\includegraphics[scale=1.0]{directed_acyclic_graph}
\end{figure}

\section{Dataset API}

Dataset (DS) on vahvasti tyypitetty kokoelma domain spesifisi‰ objekteja, jotka voidaan muuntaa rinnakkain k‰ytt‰en funktionaalisia tai relaatio-operaatioita.
DS on kehitetty korvaamaan RDD Sparkissa.
Dataset:ille olemassa olevat operaatiot on jaettu \textit{transformaatioihin} (transformations) ja \textit{toimiin} (actions).
Transformaatiot ovat operaatioita, jotka luovat uusia Dataset-objekteja, kuten $map$, $filter$, $select$ ja $aggregate$.
Toimet ovat operaatioita jotka suorittavat laskentaa ja palauttavat tuloksia.
Toimia ovat esimerkiksi \textit{count}, \textit{show} tai datan kirjoittaminen tiedostoj‰rjestelm‰‰n. \cite{spark-dataset}

Kuten RDD:t, Dataset-instanssit ovat laiskasti evaluoituvia, jolla tarkoitetaan sit‰, ett‰ laskenta aloitetaan vasta kun toimintoa kutsutaan tai instanssin arvoa tarvitaan.
Dataset on pohjimmiltaan looginen suunnitelma, jolla kuvataan datan tuottamiseen tarvittava laskenta.
Toimea kutsuttaessa, Sparkin kyselyoptimoija (query optimizer) optimoi loogisen suunnitelman ja generoi fyysisen suunnitelman.
Fyysinen suunnitelma takaa rinnakkaisesti ja hajautetusti tapahtuvan tehokkaan suorituksen.
Loogista suunnitelmaa, kuten myˆs optimoitua fyysist‰ suunnitelmaa, voidaan tutkia k‰ytt‰m‰ll‰ DS:n $explain$ funktiota. \cite{spark-dataset}

Domain-spesifisten olioiden tehokkaaseen tukemiseen tarvitaan enkooderia.
Enkooderilla tarkoitetaan ohjelmaa, joka muuntaa tietoa jonkin algoritmin mukaisesti ja t‰ss‰ tapauksessa sit‰ k‰ytet‰‰n yhdist‰m‰‰n domain-spesifinen tyyppi $T$ Sparkin sis‰iseen tyyppij‰rjestelm‰‰n.
Esimerkiksi luokan $Person$ tapauksessa, joka sis‰lt‰‰ kent‰t nimi (merkkijono) ja ik‰ (kokonaisluku), enkooderia voidaan k‰ytt‰‰ k‰skem‰‰n Sparkia luomaan koodia ajon aikana joka sarjallistaa $Person$ olion bin‰‰rirakenteeksi.
Generoidulla bin‰‰rirakenteella on usein pienempi muistijalanj‰lki ja se on myˆs optimoitu tehokkaaseen dataprosessointiin.
Datan bin‰‰riesitys voidaan tarkistaa k‰ytt‰m‰ll‰ DS:n tarjoamaa $schema$ funktiota. \cite{spark-dataset}

Dataset voidaan luoda tyypillisesti kahdella eri tavalla.
Yleisin tapa on k‰ytt‰‰ $SparkSession$:in tarjoamaa $read$ funktiota ja osoittaa Spark joihinkin tiedostoihin tiedostoj‰rjestelm‰ss‰, kuten seuraavaan $json$ tiedostoon:

\newpage

\lstset{
	string=[s]{"}{"},
	stringstyle=\color{blue},
	comment=[l]{:},
	commentstyle=\color{black},
}

\begin{lstlisting}[caption=Esimerkki JSON tiedosto]

[{
  "name": "Matt",
  "salary": 5400
}, {
  "name": "George",
  "salary": 6000
}]

\end{lstlisting}


\lstset{
	frame=0,
	language=Scala,
	breaklines=true,
}

Dataset voidaan luoda myˆs tekem‰ll‰ muutoksia olemassaoleville Dataset olioille:

\begin{lstlisting}[caption=Uuden Dataset-olion luominen transformaatiolla]

val people: Dataset<Person> = Dataset(Person())
val names = people.map(_.name)

\end{lstlisting}

\begin{lstlisting}[caption=Uuden Dataset-olion luominen k‰ytt‰en read funktiota]

case class Person(id: BigInt, firstName: String, lastName: String)
val people = spark.read.json("./people.json").as[Person]

\end{lstlisting}

Case-luokat ovat tavallisia Scala-luokkia, jotka ovat:

\begin{itemize}
	\item Oletustarvoisesti muuttumattomia (immutable)
	\item Hajoitettavia (decomposable) hahmonsovituksen (pattern matching) avulla
	\item Vertailtavissa viitteiden sijasta rakenteellisen samankaltaisuuden mukaan
	\item Yksinkertaisempia luoda (instantiate) ja k‰ytt‰‰
\end{itemize}

Mik‰li tyyppimuunnos (casting) j‰tett‰isiin tekem‰tt‰, p‰‰dytt‰isiin luomaan DataFrame-olio, jonka sis‰inen mallin (schema) Spark pyrkisi arvaamaan.
DataFrame rajapintaa k‰sitell‰‰n seuraavassa aliluvussa.
Tyyppimuunnos tehd‰‰n k‰ytt‰m‰ll‰ $as$ avainsanaa.

\begin{lstlisting}[caption=SparkSession kontekstin luominen]
val spark = SparkSession
.builder
.appName("MovieLensALS")
.getOrCreate()
\end{lstlisting}

\textit{SparkSession} on Spark-ohjelmoinnin l‰htˆkohta, kun halutaan k‰ytt‰‰ Dataset ja DataFrame -rajapintoja.
Yll‰olevassa koodinp‰tk‰ss‰ luodaan SparkSession ketjuttamalla rakentajan kutsuja. \cite{spark-dataset}

Dataset-oliot ovat samankaltaisia kuin RDD:t, sill‰ nekin tarjoavat vahvan tyypityksen ja mahdollisuuden k‰ytt‰‰ voimakkaita lambda-funktioita \cite{spark-sql-programming-guide}.
Lambda-funktiolla tarkoitetaan yleisesti anonyymi‰ funktiota, jota ei olla sidottu muuttujaan.
Perinteisen sarjallistamisen, kuten Java-sarjallistamisen, sijaan k‰ytet‰‰n erikoistunutta enkooderia olioiden sarjallistamiseen.
Sarjallistamisella tarkoitetaan olion muuntamista tavuiksi, jolloin olion muistijalanj‰lki pienenee.
Yleisesti sarjallistamista tarvitaan datan prosessointiin tai verkon yli l‰hett‰miseen.
Molempia, sek‰ enkoodereita ett‰ sarjallistamista k‰ytet‰‰n olioiden muuntamiseen tavuiksi, mutta enkooderit luodaan dynaamisesti koodissa.
Enkooderit k‰ytt‰v‰t sellaista muotoa, ett‰ Spark kykenee suorittamaan monenlaisia operaatioita, kuten suodattamista, j‰rjest‰mist‰ ja hajautusta (hashing), ilman ett‰ tavuja tarvitsee purkaa takaisin objektiksi. \cite{spark-programming-guide}

Seuraavassa koodilistauksessa luodaan uusi Dataset lukemalla $json$-tiedosto tiedostoj‰rjestelm‰st‰.
Seuraavaksi luodaan uusi Dataset muunnoksen kautta.
Objektin kloonaamiseksi k‰ytet‰‰n case-luokan $copy$ metodia, koska $people$-Dataset oli m‰‰ritelty vakioksi.
Lopuksi looginen- ja fyysinen suunnitelma tulostetaan konsoliin kutsumalla $explain$-funktiota uudelle Dataset-objektille.

\newpage

\begin{lstlisting}[caption=Dataset olion loogisen ja fyysisen suunnitelman n‰ytt‰minen]

val people = spark.read.json("./people.json").as[Person]

val peopleWithDoubleSalary = people.map { person => 
	person.copy(salary = person.salary * 2)
}

peopleWithDoubleSalary.explain(true)

\end{lstlisting}

\section{DataFrame API}

\textit{DataFrame} on nimettyihin sarakkeisiin j‰rjestetty Dataset.
Se on k‰sitteellisesti yhtenev‰ relaatiotietokannan taulun tai R/Python kielten tietokehyksen (data frame) kanssa.
DataFrame voidaan rakentaa useammalla tavalla, kuten esimerkiksi j‰sennellyist‰ tiedostoista, ulkoisista tietokannoista tai olemassaolevista RDD-olioista.
DataFrame-rajapinta on saatavilla Scala-, Java-, Python- ja R-ohjelmointikielille.
Scala-toteutuksessa DataFrame on riveist‰ rakentuva Dataset ($Dataset[Row]$). \cite{spark-programming-guide}

\begin{lstlisting}[caption=DataFrame luominen k‰ytt‰en read-funktiota]
val people = spark.read.json("./people.json")
\end{lstlisting}

DataFrame-oliota luotaessa Spark arvaa luodun objektin sis‰isen mallin.

\begin{figure}[h]
	\caption{DataFrame}
	\centering
	\includegraphics[scale=1.0]{dataframe}
\end{figure}

\end{document}