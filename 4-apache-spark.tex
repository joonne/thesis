\documentclass[main.tex]{thesis.tex}
\begin{document}
	
\lstset{
	columns=flexible,
	breaklines=true,
	tabsize=2,
	language=Scala,
	commentstyle=\color{scalacomment},
	keywordstyle=\color{blue},
	numbers=left,
	numbersep=15pt,
}

\chapter{Apache Spark}

Apache Spark on sovelluskehys hajautettujen ohjelmien kirjoittamiseen. \cite{ryza15}.
Spark-ohjelmia voidaan kirjoittaa Java-, Scala-, Python- sek‰ R-ohjelmointikielill‰.
Jokainen Spark-sovellus koostuu driver-ohjelmasta sek‰ yhdest‰ tai useammasta executor-ohjelmasta.
Driver on ohjelma, joka ajaa k‰ytt‰j‰n p‰‰ohjelmaa ja hajauttaa laskennan klusteriin.
Executor on yksi kone klusterissa. \cite{ryza15}

Spark voidaan esitell‰ kuvailemalla sen edelt‰j‰‰, MapReduce:a, ja sen tarjoamia etuja.
MapReduce tarjosi yksinkertaisen mallin ohjelmien kirjoittamiseen ja pystyi suorittamaan kirjoitettua ohjelmaa rinnakkain sadoilla tietokoneilla.
MapReduce skaalautuu l‰hes lineaarisesti datan koon kasvaessa.
Suoritusaikaa hallitaan lis‰‰m‰ll‰ lis‰‰ tietokoneita suorittamaan teht‰v‰‰. \cite{ryza15}

Apache Spark s‰ilytt‰‰ MapReduce:n lineaarisen skaalautuvuuden ja vikasietokyvyn mutta laajentaa sit‰ kolmella merkitt‰v‰ll‰ tavalla.
Ensiksi, MapReducessa map- ja reduce-teht‰vien v‰liset tulokset t‰ytyy kirjoittaa levylle kun taas Spark kykenee v‰litt‰m‰‰n tulokset suoraan liukuhihnan (pipeline) seuraavalle vaiheelle.
Toiseksi, Apache Sparkin voidaan ajatella kohtelevan kehitt‰ji‰ paremmin tarjoamalla monipuolisen joukon transformataatioita, joiden avulla voidaan muutamalla koodirivill‰ ilmaista monimutkaisia liukuhihnoja (pipelines, ohjelmistoja).
Kolmanneksi, Spark esittelee muistissa tapahtuvan prosessoinnin tarjoamalla abstraktion nimelt‰ \textit{Resilient Distributed Dataset (RDD)}.
RDD tarjoaa kehitt‰j‰lle mahdollisuuden materialisoida mink‰ tahansa askeleen liukuhihnassa ja tallentaa sen muistiin.
T‰m‰ tarkoittaa sit‰, ett‰ tulevien askelien ei tarvitse laskea aiempia tuloksia uudelleen ja t‰llˆin on mahdollista jatkaa juuri k‰ytt‰j‰n haluamasta askeleesta.
Aiemmin t‰m‰nkaltaista ominaisuutta ei ole ollut saatavilla hajautetun laskennan j‰rjestelmiss‰. \cite{ryza15}

Vaikka Spark-ohjelmia voidaankin kirjottaa usealla ohjelmointikielell‰, Scalan k‰ytt‰misell‰ saavutetaan kuitenkin muutamia etuja, joita muut kielet eiv‰t tarjoa.
Tehokkuus saattaa parantua, sill‰ datan siirt‰minen eri kerrosten v‰lill‰ tai muunnosten suorittaminen datalle voi johtaa heikompaan tehokkuuteen.
Spark on kirjoitettu Scala-ohjelmointikielell‰, joten viimeisimm‰t ja parhaimmat (latest and greatest) ominaisuudet ovat aina k‰ytˆss‰, eik‰ niiden k‰‰nnˆst‰ tarvitse odotella.
Spark ohjelmoinnin filosofia on helpompi ymm‰rt‰‰ kun Sparkia k‰ytet‰‰n kielell‰, jolla se on rakennettu.
Suurin hyˆty, jonka Scalan k‰ytt‰minen tarjoaa, on kuitenkin kehitt‰j‰kokemus joka tulee saman ohjelmointikielen k‰ytt‰misest‰ kaikkeen.
Datan tuonti, manipulointi ja koodin l‰hett‰minen klustereihin hoituvat samalla kielell‰. \cite{ryza15}

Spark-jakelun mukana toimitetaan luku-evaluointi-tulostus-silmukka (Read Eval Print Loop, REPL).
REPL on komentorivityˆkalu, joka mahdollistaa uusien asioiden nopean testailun konsolissa, eik‰ sovelluksista tarvitse rakentaa itsen‰isi‰ (self-contained) alusta asti.
Kun REPLiss‰ kehitetyn sovelluksen tai sovelluksen osan voidaan katsoa olevan tarpeeksi valmis, on j‰rkev‰‰ tehd‰ siit‰ koottu kirjasto (JAR).
N‰in varmistutaan ettei ohjelmakoodia tai tuloksia p‰‰se katoamaan, vaikkakin REPL tarjoaa samantapaisen muistin komentohistoriasta kuin perinteinen komentorivikin.

JAR eli Java ARchive on suosittuun ZIP-tiedostoformaattiin perustuva alustariippumaton tiedostoformaatti, jota k‰ytet‰‰n kokoamaan monta tiedostoa yhdeksi tiedostoksi. \cite{JAR}
JVM (Java Virtual Machine, Java-virtuaalikone) on abstrakti tietokone.
Kuten oikea tietokone, se omaa k‰skykannan ja muokkaa useita muistialueita ajon aikana.
JVM ei tied‰ mit‰‰n ohjelmointikielist‰, kuten Scala tai Java, vaan se operoi ainoastaan \textit{class}-tiedostoilla, jotka ovat bin‰‰ritiedostoja.
Class-tiedosto sis‰lt‰‰ JVM-k‰skyt sek‰ symbolitaulun. \cite{JVM}

\section{Resilient Distributed Dataset API (RDD API)}

Resilient Distributed Dataset (RDD) on Sparkin tarjoama p‰‰abstraktio.
RDD on muuttumaton, partitioitu elementtikokoelma, joka voidaan hajauttaa klusterin useiden koneiden v‰lill‰. \cite{spark-rdd}

RDD:t tukevat kahdenlaisia operaatioita: transformaatioita ja toimia (actions).
Transformaatioilla luodaan uusia datasettej‰ olemassaolevista dataseteist‰.
Toimet palauttavat arvon driver-ohjelmaan laskennan valmistuttua.
$map$-operaatio on esimerkki transformaatiosta ja $reduce$-operaatio on esimerkki toimesta.
Molemmat operaatiot k‰yv‰t annetun datasetin l‰pi ja kohdistavat annetun funktion jokaiselle elementille, mutta $map$ palauttaa uuden RDD:n ja $reduce$ palauttaa tuloksensa driver-ohjelmalle. \cite{spark-programming-guide}

Kaikki transformaatiot Sparkissa ovat laiskasti evaluoituvia, jolla tarkoitetaan sit‰, ett‰ lausekkeen evaluointia viivytet‰‰n siihen asti kun sen arvoa tarvitaan.
Kun uusi RDD luodaan, mit‰‰n laskentaa ei oikeasti viel‰ tapahdu, vaan Spark tiet‰‰ miss‰ data sijaitsee tai miten data saadaan laskettua kun tulee aika tehd‰ sille jotain.
T‰m‰ suunnittelu mahdollistaa Sparkin tehokkaamman toiminnan.
Esimerkiksi $map$ ja $reduce$ -operaatioista koostuva liukuhihna suoriutuu tehokkaammin, sill‰ $map$-transformaation luoma datasetti voidaan j‰tt‰‰ palauttamatta driver-ohjelmaan, koska $reduce$-toimen vastaus palautuu.
\cite{spark-sql-programming-guide}

RDD voidaan luoda kahdella tavalla, rinnakkaistamalla (parallelize) tai viittaamalla ulkoiseen aineistoon.
Rinnakkaistamisessa olemassaoleva Scala-kokoelma saadaan muunnettua RDD:ksi.
Ulkoiseen ainestoon viittaamisella tarkoitetaan viittaamista aineistoon ulkoisessa varastointij‰rjestelm‰ss‰ kuten \textit{HDFS}:s‰ tai \textit{HBase}:ssa. \cite{spark-programming-guide}

\begin{figure}[h]
	\caption{Directed Acyclic Graph (muokattu l‰hteest‰ \cite{dag-image})}
	\centering
	\includegraphics[scale=0.17]{directed_acyclic_graph2}
\end{figure}

RDD:t voidaan tallentaa muistiin, jolloin ohjelmistokehitt‰j‰ voi uudelleenk‰ytt‰‰ niit‰ tehokkaasti rinnakkaisissa operaatioissa.
RDD:t voivat palautua solmuvirheist‰ automaattisesti k‰ytt‰en Directed Acyclic Graph (DAG) -moottoria.
DAG tukee asyklist‰ datavirtaa, jolla tarkoitetaan sit‰, ett‰ jokainen graafin kaari kulkee topologisessa j‰rjestyksess‰ aiemmasta myˆhemp‰‰n.
Jokaista Spark-tyˆt‰ kohti luodaan DAG klusterissa suoritettavan teht‰v‰n tasoista.
Verrattuna MapReduceen, joka luo DAGin kahdesta ennaltam‰‰r‰tyst‰ tilasta (Map ja Reduce), Sparkin luomat DAGit voivat sis‰lt‰‰ mink‰ tahansa m‰‰r‰n tasoja.
T‰st‰ syyst‰ jotkin tyˆt voivat valmistua nopeammin kuin ne valmistuisivat MapReducessa.
Yksinkertaisimmat tyˆt voivat valmistua vain yhden tason j‰lkeen ja monimutkaisemmat teht‰v‰t valmistuvat yhden monitasoisen ajon j‰lkeen, ilman ett‰ niit‰ t‰ytyy pilkkoa useampiin tˆihin. \cite{mapRSpark}

\section{Dataset API}

Dataset (DS) on vahvasti tyypitetty kokoelma domain spesifisi‰ objekteja, jotka voidaan muuntaa rinnakkain k‰ytt‰en funktionaalisia tai relaatio-operaatioita.
DS on kehitetty korvaamaan RDD Sparkissa.
Dataset:ille olemassa olevat operaatiot on jaettu \textit{transformaatioihin} (transformations) ja \textit{toimiin} (actions).
Transformaatiot ovat operaatioita, jotka luovat uusia Dataset-objekteja, kuten $map$, $filter$, $select$ ja $aggregate$.
Toimet ovat operaatioita jotka suorittavat laskentaa ja palauttavat tuloksia.
Toimia ovat esimerkiksi \textit{count}, \textit{show} tai datan kirjoittaminen tiedostoj‰rjestelm‰‰n. \cite{spark-dataset}

Kuten RDD:t, Dataset-instanssit ovat laiskasti evaluoituvia, jolla tarkoitetaan sit‰, ett‰ laskenta aloitetaan vasta kun toimintoa kutsutaan tai instanssin arvoa tarvitaan.
Dataset on pohjimmiltaan looginen suunnitelma, jolla kuvataan datan tuottamiseen tarvittava laskenta.
Toimea kutsuttaessa, Sparkin kyselyoptimoija (query optimizer) optimoi loogisen suunnitelman ja generoi fyysisen suunnitelman.
Fyysinen suunnitelma takaa rinnakkaisesti ja hajautetusti tapahtuvan tehokkaan suorituksen.
Loogista suunnitelmaa, kuten myˆs optimoitua fyysist‰ suunnitelmaa, voidaan tutkia k‰ytt‰m‰ll‰ DS:n $explain$ funktiota. \cite{spark-dataset}

Domain-spesifisten olioiden tehokkaaseen tukemiseen tarvitaan enkooderia.
Enkooderilla tarkoitetaan ohjelmaa, joka muuntaa tietoa jonkin algoritmin mukaisesti ja t‰ss‰ tapauksessa sit‰ k‰ytet‰‰n yhdist‰m‰‰n domain-spesifinen tyyppi $T$ Sparkin sis‰iseen tyyppij‰rjestelm‰‰n.
Esimerkiksi luokan $Person$ tapauksessa, joka sis‰lt‰‰ kent‰t nimi (merkkijono) ja ik‰ (kokonaisluku), enkooderia voidaan k‰ytt‰‰ k‰skem‰‰n Sparkia luomaan koodia ajon aikana joka sarjallistaa $Person$ olion bin‰‰rirakenteeksi.
Generoidulla bin‰‰rirakenteella on usein pienempi muistijalanj‰lki ja se on myˆs optimoitu tehokkaaseen dataprosessointiin.
Datan bin‰‰riesitys voidaan tarkistaa k‰ytt‰m‰ll‰ DS:n tarjoamaa $schema$ funktiota. \cite{spark-dataset}

Dataset voidaan luoda tyypillisesti kahdella eri tavalla.
Yleisin tapa on k‰ytt‰‰ $SparkSession$:in tarjoamaa $read$ funktiota ja osoittaa Spark joihinkin tiedostoihin tiedostoj‰rjestelm‰ss‰, kuten seuraavaan $json$ tiedostoon:

\newpage

\lstset{
	string=[s]{"}{"},
	stringstyle=\color{blue},
	comment=[l]{:},
	commentstyle=\color{black},
}

\begin{lstlisting}[caption=Esimerkki JSON-tiedosto]
[{
  "name": "Matt",
  "salary": 5400
}, {
  "name": "George",
  "salary": 6000
}]

\end{lstlisting}


\lstset{
	frame=0,
	language=Scala,
	breaklines=true,
}

Dataset voidaan luoda myˆs tekem‰ll‰ transformaatio olemassaoleville Dataset-olioille:

\begin{lstlisting}[caption=Uuden Dataset-olion luominen transformaatiolla]

val people: Dataset[Person] = Dataset(Person())
val names = people.map(_.name)

\end{lstlisting}

\begin{lstlisting}[caption=Uuden Dataset-olion luominen k‰ytt‰en read funktiota]

case class Person(id: BigInt, firstName: String, lastName: String)
val people = spark.read.json("./people.json").as[Person]

\end{lstlisting}

Case-luokat ovat tavallisia Scala-luokkia, jotka ovat:

\begin{itemize}
	\item Oletustarvoisesti muuttumattomia (immutable)
	\item Hajoitettavia (decomposable) hahmonsovituksen (pattern matching) avulla
	\item Vertailtavissa viitteiden sijasta rakenteellisen samankaltaisuuden mukaan
	\item Yksinkertaisempia luoda (instantiate) ja k‰ytt‰‰
\end{itemize}

Mik‰li tyyppimuunnos (casting) j‰tett‰isiin tekem‰tt‰, p‰‰dytt‰isiin luomaan DataFrame-olio, jonka sis‰inen mallin (schema) Spark pyrkisi arvaamaan.
DataFrame rajapintaa k‰sitell‰‰n seuraavassa aliluvussa.
Tyyppimuunnos tehd‰‰n k‰ytt‰m‰ll‰ $as$ avainsanaa.

\begin{lstlisting}[caption=SparkSession-kontekstin luominen]
val spark = SparkSession
	.builder
	.appName("MovieLensALS")
	.getOrCreate()
\end{lstlisting}

\textit{SparkSession} on Spark-ohjelmoinnin l‰htˆkohta, kun halutaan k‰ytt‰‰ Dataset- ja DataFrame-rajapintoja.
Yll‰olevassa koodinp‰tk‰ss‰ luodaan SparkSession ketjuttamalla rakentajan kutsuja. \cite{spark-dataset}

Dataset-oliot ovat samankaltaisia kuin RDD:t, sill‰ nekin tarjoavat vahvan tyypityksen ja mahdollisuuden k‰ytt‰‰ voimakkaita lambda-funktioita \cite{spark-sql-programming-guide}.
Lambda-funktiolla tarkoitetaan yleisesti anonyymi‰ funktiota, jota ei olla sidottu muuttujaan.
Perinteisen sarjallistamisen, kuten Java-sarjallistamisen, sijaan k‰ytet‰‰n erikoistunutta enkooderia olioiden sarjallistamiseen.
Sarjallistamisella tarkoitetaan olion muuntamista tavuiksi, jolloin olion muistijalanj‰lki pienenee.
Yleisesti sarjallistamista tarvitaan datan prosessointiin tai verkon yli l‰hett‰miseen.
Molempia, sek‰ enkoodereita ett‰ sarjallistamista k‰ytet‰‰n olioiden muuntamiseen tavuiksi, mutta enkooderit luodaan dynaamisesti koodissa.
Enkooderit k‰ytt‰v‰t sellaista muotoa, ett‰ Spark kykenee suorittamaan monenlaisia operaatioita, kuten suodattamista, j‰rjest‰mist‰ ja hajautusta (hashing), ilman ett‰ tavuja tarvitsee purkaa takaisin olioksi. \cite{spark-programming-guide}

Seuraavassa koodilistauksessa luodaan uusi Dataset lukemalla $json$-tiedosto tiedostoj‰rjestelm‰st‰.
Seuraavaksi luodaan uusi Dataset muunnoksen kautta.
Olion kloonaamiseksi k‰ytet‰‰n case class :n $copy$ metodia, koska case class instanssit ovat muuttumattomia ja n‰in ollen copy-metodi on ainoa tapa kloonata (luoda uusi muuttumaton, immutable instanssi) siit‰.
Lopuksi looginen- ja fyysinen suunnitelma tulostetaan konsoliin kutsumalla $explain$-funktiota uudelle Dataset-oliolle.

\begin{lstlisting}[caption=Dataset-esimerkki]
val people = spark.read.json("./people.json").as[Person]

val peopleWithDoubleSalary = people.map { person => 
	person.copy(salary = person.salary * 2)
}

peopleWithDoubleSalary.explain(true)
\end{lstlisting}

\section{DataFrame API}

\textit{DataFrame} on nimettyihin sarakkeisiin j‰rjestetty Dataset.
Se on k‰sitteellisesti yhtenev‰ relaatiotietokannan taulun tai R/Python kielten tietokehyksen (data frame) kanssa. \cite{spark-sql-programming-guide}

\begin{figure}[h]
	\caption{DataFrame}
	\centering
	\includegraphics[scale=1.0]{dataframe}
\end{figure}

DataFrame voidaan rakentaa useammalla tavalla, kuten esimerkiksi j‰sennellyist‰ tiedostoista, hakemalla ja viittaamalla ulkoisista tietokannoista tai olemassaolevista RDD-olioista.
Se mahdollistaa datan prosessoinnin kilobittien suuruusluokasta aina petabitteihin asti ja klusterin yksitt‰isen solmun klustereista suuriin klustereihin.
DataFrame-rajapinta on saatavilla Scala-, Java-, Python- ja R-ohjelmointikielille.
Kooditasolla Scala-toteutuksessa DataFrame on riveist‰ rakentuva Dataset ($Dataset[Row]$). \cite{spark-programming-guide} \cite{spark_sql_dataframes}

Alla olevassa ohjelmassa luodaan uusi DataFrame tietorakenne kohdistamalla \textit{spark.read} -funktio JSON-tiedostoon.
T‰m‰n seurauksena Spark pyrkii p‰‰ttelem‰‰n uuden DataFrame:n sis‰isen mallin.

\begin{lstlisting}[caption=DataFrame luominen k‰ytt‰en read-funktiota]
val people = spark.read.json("./people.json")
\end{lstlisting}

DataFrame API tarjoaa domain-spesifisen kielen (DSL, domain-specific language) j‰sennellyn datan, kuten JSON:in manipulointiin:

\begin{lstlisting}[caption=Sarakkeen valitseminen ja tulostaminen DataFrame DSL:n avulla]
people.select("name").show()
\end{lstlisting}

Yll‰ olevassa esimerkiss‰ valitaan luodusta DataFrame:sta vain $name$ -sarake ja tulostetaan se konsoliin. \cite{spark-sql-programming-guide}

\section{Mllib}

MLlib on Sparkin koneoppimiskirjasto.
Projektin tavoitteena on tehd‰ k‰yt‰nnˆnl‰heisest‰ koneoppimisesta skaalautuvaa ja helppoa.
MLlib tarjoaa muun muassa seuraavanlaisia palveluita:

\begin{itemize}
	\item \textbf{Koneoppimisalgoritmit} oppimisalgoritmeja kuten luokittelua, regressiota, klusterointia ja yhteisˆsuodatusta
	\item \textbf{Featurization} piirreirroituis (feature extraction), transformaatiot, dimensioiden v‰hent‰minen (dimensionality reduction) -ja valitseminen
	\item \textbf{Liukuhihnat} tyˆkalut koneoppimisliukuhihnojen rakentamiseen, arviointiin ja muokkaamiseen
	\item \textbf{Persistointi} algoritmien, mallien ja liukuhihnojen tallentaminen ja lataaminen
	\item \textbf{Apuohjelmat} esimerkiksi lineaari algebra, statistiikka ja datan k‰sittely
\end{itemize}

\cite{mllib}

T‰ss‰ tyˆss‰ tutustutaan MLlib-kirjaston tarjoamaan ALS-kirjastoon, jolla suositukset tarjoava koneoppimismalli opetetaan.

\end{document}