\documentclass[main.tex]{thesis.tex}
\begin{document}

\chapter{Implementation}

GroupLens Research has collected and made available rating data sets from the MovieLens web site.
The data sets were collected over various periods of time, depending on the size of the set.
MovieLens ml-latest-small dataset is a collection of 100,000 ratings to 9,000 movies by 700 users.
The downside of these datasets is that they will change over time thus they are not suitable for reporting research results.
The current version, released 10/2016 is available in the project repository of this thesis.
This dataset was chosen in order to be able to review movies that we have actually seen.
MovieLens ml-latest-small dataset consists of $movies.csv$ and $ratings.csv$ files.

\begin{tabular}{lll}
	movieId & title & genres \\ \hline
	1 & Toy Story (1995) & Adventure|Animation|Children \\
	2 & Jumanji (1995) & Adventure|Children|Fantasy \\
	3 & Grumpier Old Men (1995) & Comedy|Romance \\
	4 & Waiting to Exhale (1995) & Comedy|Drama|Romance \\
	5 & Father of the Bride Part II (1995) & Comedy \\
	6 & Heat (1995) & Action|Crime|Thriller \\
	7 & Sabrina (1995) & Comedy|Romance \\
	8 & Tom and Huck (1995) & Adventure|Children \\
	9 & Sudden Death (1995) & Action \\
	10 & GoldenEye (1995) & Action|Adventure|Thriller \\
\end{tabular}

\begin{tabular}{llll}
	userId & movieId & rating & timestamp \\ \hline
	1 & 31 & 2.5 & 1260759144 \\
	1 & 1029 & 3.0 & 1260759179 \\
	1 & 1061 & 3.0 & 1260759182 \\
	1 & 1129 & 2.0 & 1260759185 \\
	1 & 1172 & 4.0 & 1260759205 \\
	1 & 1263 & 2.0 & 1260759151 \\
	1 & 1287 & 2.0 & 1260759187 \\
	1 & 1293 & 2.0 & 1260759148 \\
	1 & 1339 & 3.5 & 1260759125 \\
\end{tabular}

We used RDD based API since dataset API is not yet fully functional in collaborative filtering tasks.
Loading data can be done with dataset API but the recommendation with ALS needs to be done still with RDD based API. Dataset API brings numerous improvements such as easier data loading.

\section{MovieLensRecommendation.scala}

First task, when writing a self contained spark application is to make a correct project structure and have a $<PROJECT_NAME>.sbt$ file which describes the dependencies of the application. A self contained spark application refers to a shippable jar (Java ARchive) file that can be distributed to a spark cluster and it contains your code and all the dependencies.

Applications can be launched on a cluster with the spark-submit. It can use all of Spark's supported cluster managers through a uniform interface so you don't have to configure your application specially for each one.

\begin{lstlisting}[caption=Creating assembly jar with sbt,language=sh]
sbt package
\end{lstlisting}

\begin{lstlisting}[caption=Launch an application on a cluster,language=sh]
spark-submit --class "MovieLensALS" --master local[4] movielens-recommendations_2.11-1.0.jar
\end{lstlisting}

\begin{lstlisting}[caption=Loading ratings with RDD API]
val ratings = sc.textFile("ml-latest-small/ratings.csv")
  .filter(x => !isHeader("userId", x))
  .map { line =>
    val fields = line.split(",")
    (fields(3).toLong % 10, Rating(fields(0).toInt, fields(1).toInt, fields(2).toDouble))
  }
\end{lstlisting}

\begin{lstlisting}[caption=Loading ratings with Dataset API]
val ratings = spark.read.csv("ml-latest-small/ratings.csv")
  .filter(arr => arr(0) != "userId")
  .map { fields =>
   	Rating(fields(0).asInstanceOf[String].toInt, fields(1).asInstanceOf[String].toInt, fields(2).asInstanceOf[String].toFloat, fields(3).asInstanceOf[String].toLong % 10)
  }
\end{lstlisting}

\begin{lstlisting}[caption=MovieLensALS.scala]

import org.apache.spark.mllib.recommendation.*

object MovieLensALS {
def main(args: Array[String]) {
Logger.getLogger("org.apache.spark").setLevel(Level.WARN)
Logger.getLogger("org.eclipse.jetty.server").setLevel(Level.OFF)

val conf = new SparkConf()
.setAppName("MovieLensALS")
.set("spark.executor.memory", "4g")
val sc = new SparkContext(conf)

// load personal ratings

val personalRatings = Source.fromFile("personalRatings.txt").getLines()
.map { line =>
val fields = line.split(",")
Rating(fields(0).toInt, fields(1).toInt, fields(2).toDouble)
}.filter(_.rating > 0.0).toSeq

val personalRatingsRDD = sc.parallelize(personalRatings, 1)

// load ratings and movie titles

val ratings = sc.textFile("ml-latest-small/ratings.csv")
.filter(x => !isHeader("userId", x))
.map { line =>
val fields = line.split(",")
// format: (timestamp % 10, Rating(userId, movieId, rating))
(fields(3).toLong % 10, Rating(fields(0).toInt, fields(1).toInt, fields(2).toDouble))
}

val movies = sc.textFile("ml-latest-small/movies.csv")
.filter(x => !isHeader("movieId", x))
.map { line =>
val fields = line.split(",")
// format: (movieId, movieName)
(fields(0).toInt, fields(1))
}.collect().toMap

val numRatings = ratings.count
val numUsers = ratings.map(_._2.user).distinct.count
val numMovies = ratings.map(_._2.product).distinct.count

println(s"Got $numRatings ratings from $numUsers users on $numMovies movies.")

val numPartitions = 4
val training = ratings.filter(x => x._1 < 6)
.values
.union(personalRatingsRDD)
.repartition(numPartitions)
.cache()
val validation = ratings.filter(x => x._1 >= 6 && x._1 < 8)
.values
.repartition(numPartitions)
.cache()
val test = ratings.filter(x => x._1 >= 8).values.cache()

val numTraining = training.count()
val numValidation = validation.count()
val numTest = test.count()

println(s"Training: $numTraining, validation: $numValidation, test: $numTest")

// TRAINING

val ranks = List(8, 12)
val lambdas = List(1.0, 10.0)
val numIters = List(10, 20)
var bestModel: Option[MatrixFactorizationModel] = None
var bestValidationRmse = Double.MaxValue
var bestRank = 0
var bestLambda = -1.0
var bestNumIter = -1
for (rank <- ranks; lambda <- lambdas; numIter <- numIters) {
val model = ALS.train(training, rank, numIter, lambda)
val validationRmse = computeRmse(model, validation, numValidation)
println(s"RMSE (validation) = $validationRmse for the model trained with rank = $rank, lambda = $lambda, and numIter = $numIter.")
if (validationRmse < bestValidationRmse) {
bestModel = Some(model)
bestValidationRmse = validationRmse
bestRank = rank
bestLambda = lambda
bestNumIter = numIter
}
}

val testRmse = computeRmse(bestModel.get, test, numTest)

println(s"The best model was trained with rank = $bestRank and lambda = $bestLambda and numIter = $bestNumIter and its RMSE on the test set is $testRmse .")

val myRatedMovieIds = personalRatings.map(_.product).toSet
val candidates = sc.parallelize(movies.keys.filter(!myRatedMovieIds.contains(_)).toSeq)
val recommendations = bestModel.get
.predict(candidates.map((0, _)))
.collect()
.sortBy(- _.rating)
.take(10)

var i = 1
println("Movies recommended for you:")
recommendations.foreach { r =>
println("%2d".format(i) + ": " + movies(r.product))
i += 1
}

// clean up
sc.stop()
}

def isHeader(headerId: String, line: String): Boolean = line.contains(headerId)

/** Compute RMSE (Root Mean Squared Error). */
def computeRmse(model: MatrixFactorizationModel, data: RDD[Rating], n: Long): Double = {
val predictions: RDD[Rating] = model.predict(data.map(x => (x.user, x.product)))
val predictionsAndRatings = predictions.map(x => ((x.user, x.product), x.rating))
.join(data.map(x => ((x.user, x.product), x.rating)))
.values
math.sqrt(predictionsAndRatings.map(x => (x._1 - x._2) * (x._1 - x._2)).reduce(_ + _) / n)
}
}
\end{lstlisting}

\end{document}