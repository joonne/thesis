\documentclass[main.tex]{thesis.tex}
\begin{document}

\chapter{Toteutus}

T‰ss‰ luvussa esitet‰‰n tyˆn toteutuksen oleelliset osat.
Opetusdata, sen lataaminen ja siistiminen Sparkia varten.
EMR-klusterin pystytt‰minen.
Projektin rakenne.
Mallin opettaminen.
Ennustusten ker‰‰minen mallin avulla.

\section{EMR-klusterin konfigurointi}

T‰ss‰ osiossa esitet‰‰n EMR-klusterin konfigurointi sek‰ tyˆss‰ k‰ytetyll‰ tavalla ett‰ vaihtoehtoisesti komentorivin kautta tapahtuvalla keinolla.
EMR-klusteri on mahdollista pystytt‰‰ myˆs suoraan Spark-sovelluksesta.
Klusterin konfigurointi piti tehd‰ k‰yt‰nnˆss‰ kolmannen osapuolen \cite{levelup} videon perusteella, sill‰ AWS:n omat ohjeistukset eiv‰t olleet riitt‰v‰t, tai ainakin ne olivat vaikeaselkoiset.

\subsection{WEB-k‰yttˆliittym‰n avulla}

EMR-klusteri voidaan pystytt‰‰ k‰tev‰sti myˆs graafisen k‰yttˆliittym‰n kautta AWS:n konsolissa.
Ensin klusterille annetaan nimi.
Seuraavaksi voidaan valita, halutaanko tallentaa lokit S3-buckettiin.
Spark-sovellus tuotaa yll‰tt‰v‰n paljon lokitusta ja t‰m‰n valinnan kanssa kannattaakin olla tarkkana, erityisesti jos haluaa operoida Amazon Free Tier:in puitteissa, sill‰ jokainen kirjoitus syˆ osansa t‰st‰ rajallisesta m‰‰r‰st‰.
T‰m‰n j‰lkeen valitaan \textit{Launch mode}, joka tarkoittaa k‰yt‰nnˆss‰ sit‰, ett‰ halutaanko liukuhihna ajaa vain kerran vai useammin.
Seuraavaksi valitaan haluttu EMR-versio sek‰ tarvittavat ohjelmistokomponentit.
T‰m‰n tyˆn tarpeisiin tulee valita luonnollisesti Spark.
Seuraavaksi valitaan tarvittavien virtuaalikoneiden suorituskyky ja lukum‰‰r‰.
Viimeinen kohta k‰sittelee salausta ja p‰‰synhallintaa.
K‰ytt‰j‰n tulee luoda EC2-avainpari, jotta on mahdollista muodostaa SSH-yhteys klusterin p‰‰virtuaalikoneelle.

\newpage

\begin{figure}[h]
	\caption{EMR-klusterin luominen}
	\centering
	\includegraphics[scale=0.5]{emr_create}
\end{figure}

Yll‰ olevassa kuvassa on esitetty klusterin konfiguroinnin www-k‰yttˆliittym‰, johon p‰‰st‰‰n k‰siksi AWS:n konsolin kautta.

\subsection{Komentorivin avulla}

\lstset{
	columns=flexible,
	breaklines=true,
	tabsize=2,
	language=sh,
	keywordstyle=,
	numbers=left,
	numbersep=15pt,
}

\begin{lstlisting}[caption=Klusterin luominen komentorivilt‰ \cite{create_cluster},language=sh]
aws emr create-cluster \
	--name "cluster" \
	--release-label emr-5.17.0 \
	--applications Name=Spark \
	--ec2-attributesKeyName=myKey \
	--instance-type m4.large \
	--instance-count 3 \
	--use-default-roles
\end{lstlisting}

Yll‰ olevassa esimerkiss‰ pystytet‰‰n EMR klusteri AWS:n komentorivityˆkalun avulla.
Rivinvaihtomerkit \textit{\mybs} on lis‰tty selkeyden takia, joten ne eiv‰t ole merkityksellisi‰ komennon kannalta. \cite{create_cluster}
Valinnat omat samat komentoriviversiossa, kuin k‰yttˆliittym‰nkin kautta.

\begin{lstlisting}[caption=Sovelluksen ajaminen AWS EMR:ss‰]
> sbt package
> aws s3 cp movielens.jar s3://movielens/movielens.jar
> aws s3 cp s3://movielens/movielens.jar .
> spark-submit ./movielens.jar
\end{lstlisting}

Yll‰ olevassa ohjelmassa esitet‰‰n askeleet, joilla sovellus saadaan paketoitua, l‰hetetty‰ EC2-virtuaalikoneelle ja ajettua EMR-klusterissa.
Ensin ohjelma paketoidaan $jar$-tiedostoksi.
Seuraavaksi $jar$-tiedosto l‰hetet‰‰n S3-palveluun, jossa sijaitsee \textit{movielens} niminen bucket.
T‰m‰n j‰lkeen otetaan SSH-yhteys EC2-virtuaalikoneelle ja ladataan $jar$-tiedosto samaisesta bucketista.
Lopuksi k‰ytet‰‰n \textit{spark-submit} -tyˆkalua sovelluksen ajamiseen EMR-klusterissa.

\section{Opetusdata}

GroupLens Research on ker‰nnyt ja laittanut saataville aineistoja MovieLens-sivustolta.
Aineistot on ker‰tty useiden aikajaksojen aikana, riippuen aineiston koosta.
MovieLens 20M aineisto sis‰lt‰‰ 20 000 000 (kaksikymment‰ miljoonaa) arviota, jotka ovat antaneet 138 000 k‰ytt‰j‰‰ 27 000 elokuvalle.
MovieLens 20M aineisto koostuu $movies.csv$ and $ratings.csv$ tiedostoista. \cite{movieLens17}

\captionof{table}{N‰yte \textit{movies.csv} tiedostosta}
\begin{tabular}{lll}
	movieId & title & genres \\ \hline
	1 & Toy Story (1995) & Adventure|Animation|Children \\
	2 & Jumanji (1995) & Adventure|Children|Fantasy \\
	3 & Grumpier Old Men (1995) & Comedy|Romance \\
	4 & Waiting to Exhale (1995) & Comedy|Drama|Romance \\
	5 & Father of the Bride Part II (1995) & Comedy \\
	6 & Heat (1995) & Action|Crime|Thriller \\
	7 & Sabrina (1995) & Comedy|Romance \\
	8 & Tom and Huck (1995) & Adventure|Children \\
	9 & Sudden Death (1995) & Action \\
	10 & GoldenEye (1995) & Action|Adventure|Thriller \\
\end{tabular}

\captionof{table}{N‰yte \textit{ratings.csv} tiedostosta}
\begin{tabular}{llll}
	userId & movieId & rating & timestamp \\ \hline
	1 & 31 & 2.5 & 1260759144 \\
	1 & 1029 & 3.0 & 1260759179 \\
	1 & 1061 & 3.0 & 1260759182 \\
	1 & 1129 & 2.0 & 1260759185 \\
	1 & 1172 & 4.0 & 1260759205 \\
	1 & 1263 & 2.0 & 1260759151 \\
	1 & 1287 & 2.0 & 1260759187 \\
	1 & 1293 & 2.0 & 1260759148 \\
	1 & 1339 & 3.5 & 1260759125 \\
\end{tabular}

~

Toteutuksessa k‰ytettiin RDD-pohjaista rajapintaa, sill‰ Dataset-pohjainen rajapinta ei ollut viel‰ t‰ysin toiminnallinen yhteisˆsuodatuksen ongelmille.
Aineiston lataaminen oli mahdollista toteuttaa Dataset-rajapintaa hyˆdynt‰en, mutta varsinainen suositus t‰ytyi tehd‰ RDD-rajapintaa k‰ytt‰en.
Dataset-rajapinta tarjoaa useita parannuksia, kuten esimerkiksi yksinkertaisemman tiedon lataamisen.
Mik‰li tyˆ teht‰isiin alusta, olisi Dataset-rajapinta oikea vaihtoehto toteutukselle.

\section{Projektin rakenne}

Ensimm‰inen askel itsen‰isen Spark-sovelluksen rakentamisessa on tehd‰ oikeanlainen kansiorakenne ja luoda tiedosto, jossa kuvaillaan sovelluksen riippuvuudet.
Itsen‰isell‰ Spark-sovelluksella tarkoitetaan k‰yttˆvalmista $JAR$-tiedostoa (Java ARchive), joka voidaan jakaa Spark-klusterille ja se sis‰lt‰‰ sek‰ koodin ett‰ kaikki riippuvuudet.
T‰t‰ varten l‰hdekoodi tulee saada paketoitua ja t‰m‰ saavutetaan SBT-tyˆkalulla.
SBT (Scala Build Tool) on k‰‰nnˆstyˆkalu Scala, Java ja C++-kielille, jonka avulla l‰hdekoodi saadaan sek‰ k‰‰nnetty‰ ett‰ paketoitua JAR:iksi \cite{sbt}.

Sovelluksia voidaan ottaa k‰yttˆˆn klusterissa \textit{spark-submit} tyˆkalun avulla, joka mahdollistaa kaikkien Sparkin tukemien klusterinhallintatyˆkalujen k‰ytt‰misen yhten‰isen k‰yttˆliittym‰n kautta.
T‰m‰ ominaisuus osoittautui eritt‰in hyˆdylliseksi kun sovellusta ajettiin EMR-klusterissa, sill‰ \textit{spark-submit}-tyˆkalu ottaa syˆtteen‰ vain k‰‰nnetyn JAR-tiedoston ja alkaa ajamaan sovellusta.

\begin{lstlisting}[caption=Sovelluksen paketointi ja ajaminen klusterissa,language=sh]
sbt package
spark-submit movielens-recommendations_2.11-1.0.jar
\end{lstlisting}

~

Alla olevassa esimerkiss‰ ladataan tyˆss‰ k‰ytetyt suositukset RDD rajapintaa k‰ytt‰en.

\lstset{
	columns=flexible,
	breaklines=true,
	tabsize=2,
	language=Scala,
	commentstyle=\color{scalacomment},
	keywordstyle=\color{blue},
	numbers=left,
	numbersep=15pt,
}

\begin{lstlisting}[caption=Suositusten lataaminen RDD rajapintaa k‰ytt‰en]
val ratings = sc.textFile("ml-latest-small/ratings.csv")
  .mapPartitionsWithIndex((i, it) => if (i == 0) it.drop(1) else it)
  .map { line =>
    val fields = line.split(",")
    val timestamp = fields(3).toLong % 10
    val userId = fields(0).toInt
    val movieId = fields(1).toInt
    val rating = fields(2).toDouble 

    (timestamp, Rating(userId, movieId, rating))
  }
\end{lstlisting}

~

Alla olevassa esimerkiss‰ ladataan tyˆss‰ k‰ytetyt suositukset Dataset-rajapintaa k‰ytt‰en.

\begin{lstlisting}[caption=Suositusten lataaminen Dataset rajapintaa k‰ytt‰en]
val ratings = spark.read.csv("ml-latest-small/ratings.csv")
	.mapPartitionsWithIndex((i, it) => if (i == 0) it.drop(1) else it)
	.map { fields =>
		val userId = fields(0).asInstanceOf[String].toInt
		val movieId = fields(1).asInstanceOf[String].toInt
		val rating = fields(2).asInstanceOf[String].toFloat
		val timestamp = fields(3).asInstanceOf[String].toDouble % 10

		Rating(userId, movieId, rating, timestamp)
  }
\end{lstlisting}

\section{Opetusdatan lataaminen Spark sovellukseen}

Alla olevassa koodilistauksessa on esimerkki opetusdatan lataamisesta S3:sta.

\begin{lstlisting}[caption=Aineiston lataaminen]
val ratings = sc.textFile("s3n://bucket/ratings.csv")
  .mapPartitionsWithIndex((i, it) => if (i == 0) it.drop(1) else it)
  .map { line =>
		val fields = line.split(",")
		val userId = fields(0).toInt
		val movieId = fields(1).toInt
		val rating = fields(2).toDouble

		Rating(userId, movieId, rating)
  }	
\end{lstlisting}

Riveill‰ 1-11 luodaan RDD $ratings$ lataamalla csv-tiedosto.
Tiedostosta suodatetaan ensin pois otsikkorivit ja t‰m‰n j‰lkeen tiedosto k‰yd‰‰n l‰pi rivi kerrallaan ja p‰tkit‰‰n pilkulla erotetut arvot taulukkoon k‰ytt‰en Scalan String-luokan sis‰‰nrakennettua $split$ funktiota.
T‰m‰n j‰lkeen taulukossa olevista arvoista muodostetaan \textit{Rating}-olioita.
Rating on Sparkin tarjoama apuluokka, jota ei en‰‰n k‰ytet‰ uudemman \textit{ml}-kirjaston kanssa, vaan ALS-malli ottaa syˆtteen‰‰n vain tupleja, joissa on oikeat arvot oikeilla paikoillaan.
Huomionarvoista on se, kuinka tiedostoihin voidaan viitata suoraan S3:n tiedoston nimell‰ ja Spark osaa hakea tiedostot suoraan S3-bucketista.
Opetusdataa ei juuri tarvinnut siisti‰, sill‰ opetukseen k‰ytettiin valmista, hyvin j‰sennelty‰ datasetti‰.

\section{Mallin opettaminen}

Alla olevassa koodilistauksessa on esitetty esimerkki ALS-mallin opettamisesta.

\begin{lstlisting}[caption=Mallin opettaminen]
for (rank <- ranks; lambda <- lambdas; numIter <- numIters) {
	val model = ALS.train(training, rank, numIter, lambda)
	val validationRmse =
		computeRmse(model, validation, numValidation)
	
	if (validationRmse < bestValidationRmse) {
		bestModel = Some(model)
		bestValidationRmse = validationRmse
		bestRank = rank
		bestLambda = lambda
		bestNumIter = numIter
	}
}

\end{lstlisting}

Yll‰ olevassa esimerkiss‰ suoritetaan varsinainen mallin opetus.
Opetus tapahtuu niin, ett‰ opetetaan muutama versio mallista ja valitaan malleista paras k‰ytt‰en RMSE-metriikkaa.
Koodin tasolla opetus tapahtuu k‰ytt‰en MLlib / ALS kirjaston funktiota $train$, joka ottaa sis‰‰ntulonaan $ratings$, $rank$, $iterations$ sek‰ $lambda$ parametrit:

\begin{itemize}
	\item $ratings$ on RDD Rating olioita, jotka sis‰lt‰v‰t k‰ytt‰j‰n tunnisteen, elokuvan tunnisteen ja suosituksen
	\item $rank$ on piilevien ominaisuuksien sis‰llytett‰v‰ m‰‰r‰
	\item $iterations$ on ALS algoritmin iteraatioiden m‰‰r‰
	\item $lambda$ on regularisaatio-parametri, jolla yritet‰‰n ehk‰ist‰ mallin ylioppimista
\end{itemize}

Tutkimuksessa \cite{miryala17} on tutkittu parhaita parametreja ALS-algoritmille ja p‰‰dytty lambda-arvoon 0.1 sek‰ iteraatioiden m‰‰r‰‰n 20.
Parhautta on tutkittu RMSE-metriikan kautta ja kyseisill‰ parametreilla RMSE saadaan pienimmilleen eli mallin voidaan sanoa sovittuvan parhaiten opetusdataan.
Tutkimuksessa oltiin p‰‰dytty arvoon 0.819942, kun taas paras itse opetettu malli p‰‰tyi RMSE arvoon 0.807167.
Omassa opetuksessa eroavaisuuksina olivat t‰m‰n hetken l‰hin vastaava datasetti, joka ei ollut aivan niin suuri kuin tutkimuksessa k‰ytetty, myˆs opetus datojen suhde oli hieman eri, sill‰ oman toteutuksen RMSE-validointi tarvitsi myˆs oman osansa datasta.
Validointiin olisi tietysti voitu k‰ytt‰‰ samaa opetusdataa, kuten oletettavasti tutkimuksessakin oli tehty tai sitten RMSE oli arvioitu eri menetelm‰‰ k‰ytt‰en.
Tutkimuksessa paras arvo saatiin k‰ytt‰en datasetti‰ 80-20 suhteessa ja omassa opetuksessa k‰ytˆss‰ oli datasetti, joka oli jaettu  60-20-20 suhteessa.

\section{Ennustaminen}

Alla olevassa koodilistauksessa on esitetty esimerkki suositusten ennustamisesta.

\begin{lstlisting}[caption=Suositusten ennustaminen]
val myRatedMovieIds = Set(personalRatings.map(_.product))
val candidates = movies.keys.filter(!myRatedMovieIds.contains(_)).toSeq
val candidatesRDD = sc.parallelize(candidates)

val recommendations = bestModel
	.get
	.predict(candidatesRDD)
	.collect()
	.sortBy(- _.rating)
	.take(10)
\end{lstlisting}

~

Yll‰ olevassa koodilistauksessa haetaan henkilˆkohtaiset suositukset k‰ytt‰m‰ll‰ mallin $predict$-metodia, joka ottaa parametrinaan mahdollisten elokuvien joukon.
Mahdollisilla elokuvilla tarkoitetaan elokuvia, joita k‰ytt‰j‰ ei ole viel‰ n‰hnyt, eli ne eiv‰t sis‰lly $personalRatings$-muuttujan sis‰lt‰miin elokuviin.
Muuttuja $bestModel$ sis‰lt‰‰ aiemmin talteen otetun, parhaan RMSE-arvon omaava malli.
Tarkemmin sanottuna se on mallin sis‰lt‰v‰ $Option$ muuttuja, jonka sis‰ll‰ olevaan malliin p‰‰st‰‰n k‰siksi $get$-funktion avulla.

\section{Opetusvirheen evaluointi}

Alla olevassa kooodilistauksessa esitet‰‰n apufunktio, jolla evaluoidaan opetetun mallin virhett‰.

\begin{lstlisting}[caption=RMSE]
def computeRmse(
	model: MatrixFactorizationModel,
	data: RDD[Rating],
	n: Long
): Double = {
	val predictions: RDD[Rating] =
		model.predict(data.map(x => (x.user, x.product)))
	val predictionsAndRatings = predictions
		.map(x => ((x.user, x.product), x.rating))
		.join(data.map(x => ((x.user, x.product), x.rating)))
		.values
	
	math.sqrt(
		predictionsAndRatings
			.map(x => (x._1 - x._2) * (x._1 - x._2))
			.reduce(_ + _) / n
	)
}
\end{lstlisting}

Yll‰ olevassa koodilistauksessa esitet‰‰n apufunktio $computeRMSE$, jonka avulla evaluoidaan opetetun mallin virhett‰.
Apufunktio on ehk‰ hieman v‰h‰inen nimitys, sill‰ kuten kappaleessa 2 todettiin, k‰ytet‰‰n RMSE:t‰ kriteerin‰ sille, ett‰ kuinka hyvin malli sovittuu opetusdataan.
Lis‰ksi vertailevassa tutkimuksessa \cite{miryala17} RMSE-arvoa k‰ytettiin kriteerin‰, jolla todettiin paras malli vaihtoehdoista.

\end{document}