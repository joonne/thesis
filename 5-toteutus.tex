\documentclass[main.tex]{thesis.tex}
\begin{document}

\chapter{Toteutus}

Tässä luvussa esitetään työn toteutuksen oleelliset osat.
Opetusdata, sen lataaminen ja siistiminen Sparkia varten.
Projektin rakenne.
Mallin opettaminen.
Ennustusten kerääminen mallin avulla.

\section{Opetusdata}

GroupLens Research on kerännyt ja laittanut saataville aineistoja MovieLens-sivustolta.
Aineistot on kerätty useiden aikajaksojen aikana, riippuen aineiston koosta.
MovieLens 20M aineisto sisältää 20 000 000 (kaksikymmentä miljoonaa) arviota, jotka ovat antaneet 138 000 käyttäjää 27 000 elokuvalle.
MovieLens 20M aineisto koostuu $movies.csv$ and $ratings.csv$ tiedostoista.

\captionof{table}{Näyte \textit{movies.csv} tiedostosta}
\begin{tabular}{lll}
	movieId & title & genres \\ \hline
	1 & Toy Story (1995) & Adventure|Animation|Children \\
	2 & Jumanji (1995) & Adventure|Children|Fantasy \\
	3 & Grumpier Old Men (1995) & Comedy|Romance \\
	4 & Waiting to Exhale (1995) & Comedy|Drama|Romance \\
	5 & Father of the Bride Part II (1995) & Comedy \\
	6 & Heat (1995) & Action|Crime|Thriller \\
	7 & Sabrina (1995) & Comedy|Romance \\
	8 & Tom and Huck (1995) & Adventure|Children \\
	9 & Sudden Death (1995) & Action \\
	10 & GoldenEye (1995) & Action|Adventure|Thriller \\
\end{tabular}

\captionof{table}{Näyte \textit{ratings.csv} tiedostosta}
\begin{tabular}{llll}
	userId & movieId & rating & timestamp \\ \hline
	1 & 31 & 2.5 & 1260759144 \\
	1 & 1029 & 3.0 & 1260759179 \\
	1 & 1061 & 3.0 & 1260759182 \\
	1 & 1129 & 2.0 & 1260759185 \\
	1 & 1172 & 4.0 & 1260759205 \\
	1 & 1263 & 2.0 & 1260759151 \\
	1 & 1287 & 2.0 & 1260759187 \\
	1 & 1293 & 2.0 & 1260759148 \\
	1 & 1339 & 3.5 & 1260759125 \\
\end{tabular}

Toteutuksessa käytettiin RDD-pohjaista rajapintaa, sillä dataset-pohjainen rajapinta ei ole vielä täysin toiminnallinen yhteisöllisen suodatuksen ongelmille.
Aineiston lataaminen voidaan tehdä dataset-rajapintaa hyödyntäen, mutta varsinainen suositus täytyy tehdä RDD-rajapintaa käyttäen.
Dataset-rajapinta tarjoaa useita parannuksia, kuten esimerkiksi yksinkertaisemman tiedon lataamisen.

\section{Projektin rakenne}

Ensimmäinen askel itsenäisen Spark-sovelluksen rakentamisessa on tehdä oikeanlainen kansiorakenne ja luoda $<PROJEKTI>.sbt$ niminen tiedosto, jossa kuvaillaan sovelluksen riippuvuudet.
Itsenäisellä Spark-sovelluksella tarkoitetaan käyttövalmista $JAR$-tiedostoa (Java ARchive), joka voidaan jakaa Spark-klusterille ja se sisältää sekä koodin että kaikki riippuvuudet.
Tiedostomuoto $.sbt$ viittaa SBT (Scala Build Tool) nimiseen ohjelmaan, joka on käännöstyökalu Scala, Java ja C++-kielille \cite{sbt}.
SBT:n avulla lähdekoodi saadaan sekä käännettyä että paketoitua JAR:iksi.

Sovelluksia voidaan ottaa käyttöön klusterissa \textit{spark-submit} työkalun avulla, joka mahdollistaa Sparkin kaikkien tuettujen klusterinhoitajien käyttämisen yhteinäisen käyttöliittymän kautta.
Tämä ominaisuus osoittautui erittäin hyödylliseksi kun sovellusta ajettiin EMR-klusterissa, sillä spark-submit työkalu otti parametrinaan vain käännetyn JAR:in ja alkoi ajamaan sovellusta.
Tässö työssä "klusteri" tulee sisältämään vain master noodin sekä yhden worker noodin, mutta periaatteessa kyseessä on kuitenkin klusteri, vain erittäin pieni sellainen.

\begin{lstlisting}[caption=Sovelluksen paketointi sbt-työkalulla,language=sh]
sbt package
\end{lstlisting}

\begin{lstlisting}[caption=Sovelluksen ajaminen klusterissa,language=sh]
spark-submit movielens-recommendations_2.11-1.0.jar
\end{lstlisting}

~

Alla olevassa esimerkissä 5.3 ladataan työssä käytetyt suositukset RDD rajapintaa käyttäen.

\lstset {
	language=Scala,
	basicstyle=\footnotesize,
	numbers=left,
	stepnumber=1,
	showstringspaces=false,
	tabsize=2,
	breaklines=true,
	breakatwhitespace=false,
}

\begin{lstlisting}[caption=Suositusten lataaminen RDD rajapintaa käyttäen]
val ratings = sc.textFile("ml-latest-small/ratings.csv")
  .filter(arr => arr(0) != "userId")
  .map { line =>
    val fields = line.split(",")
    val timestamp = fields(3).toLong % 10
    val userId = fields(0).toInt
    val movieId = fields(1).toInt
    val rating = fields(2).toDouble 

    (timestamp, Rating(userId, movieId, rating))
  }
\end{lstlisting}

~

Alla olevassa esimerkissä 5.4 ladataan työssä käytetyt suositukset Dataset-rajapintaa käyttäen.

\lstset {
	language=Scala,
	basicstyle=\footnotesize,
	numbers=left,
	stepnumber=1,
	showstringspaces=false,
	tabsize=2,
	breaklines=true,
	breakatwhitespace=false,
}

\begin{lstlisting}[caption=Suositusten lataaminen Dataset rajapintaa käyttäen]
val ratings = spark.read.csv("ml-latest-small/ratings.csv")
	.filter(arr => arr(0) != "userId")
	.map { fields =>
		val userId = fields(0).asInstanceOf[String].toInt
		val movieId = fields(1).asInstanceOf[String].toInt
		val rating = fields(2).asInstanceOf[String].toFloat
		val timestamp = fields(3).asInstanceOf[String].toDouble % 10

		Rating(userId, movieId, rating, timestamp)
  }
\end{lstlisting}

~

\section{Opetusdatan lataaminen Spark sovellukseen}

Alla olevassa koodilistauksessa 5.5 on esitetty opetusdatan lataaminen S3:sta.

\begin{lstlisting}[caption=Aineiston lataaminen]
// load personal ratings
val personalRatings = sc.textFile("s3n://bucket/personalRatings.txt")
	.map { line =>
	  val fields = line.split(",")
	  Rating(fields(0).toInt, fields(1).toInt, fields(2).toDouble)
	}.filter(_.rating > 0.0)
		
// load ratings
val ratings = sc.textFile("s3n://bucket/ratings.csv")
  .filter(!isHeader("userId")(_))
  .map { line =>
		val fields = line.split(",")
		val timestamp = fields(3).toLong % 10
		val userId = fields(0).toInt
		val movieId = fields(1).toInt
		val rating = fields(2).toDouble

		(timestamp, Rating(userId, movieId, rating)
  }
	
// load movies
val movies = sc.textFile("s3n://bucket/movies.csv")
	.filter(!isHeader("movieId")(_))
	.map { line =>
		val fields = line.split(",")
		(fields(0).toInt, fields(1))
	}.collect().toMap
	
\end{lstlisting}

Riveillä 2-6 luodaan RDD $personalRatings$ lataamalla henkilökohtaiset suositukset tekstitiedostosta nimeltä $personalRatings.txt$, pilkotaan tiedoston rivit pilkun kohdalta ja luodaan uusia $Rating$ -objekteja yhtä monta, kuin tiedostossa on rivejä.
Riveillä 22-36 luodaan RDD:t $ratings$ ja $movies$ lataamalla kaksi erillistä csv-tiedostoa.
Tiedostoista suodatetaan ensin pois otsikkorivit ja tämän jälkeen tiedosto käydään läpi rivi kerrallaan ja pätkitään pilkulla erotetut arvot taulukkoon käyttäen Scalan String luokan sisäänrakennettua $split$ funktiota.
Tämän jälkeen taulukossa olevista arvoista muodostetaan pareja (tuple).
Huomionarvoista on se, kuinka tiedostoihin voidaan viitata suoraan S3:n tiedoston nimellä ja Spark osaa hakea tiedostot suoraan S3 bucketista.
Opetusdataa ei juuri tarvinnut siistiä, sillä opetukseen käytettiin valmista, hyvin jäsenneltyä datasettiä.

\section{Mallin opettaminen}

Alla olevassa koodilistauksessa 5.6 on esitetty mallin opettaminen.

\begin{lstlisting}[caption=Mallin opettaminen]
val numPartitions = 4
val training = ratings.filter(x => x._1 < 6)
	.values
	.union(personalRatings)
	.repartition(numPartitions)
	.cache()
val validation = ratings.filter(x => x._1 >= 6 && x._1 < 8)
	.values
	.repartition(numPartitions)
	.cache()
val test = ratings.filter(x => x._1 >= 8).values.cache()

val ranks = List(8, 12)
val lambdas = List(1.0, 10.0)
val numIters = List(10, 20)
var bestModel: Option[MatrixFactorizationModel] = None
var bestValidationRmse = Double.MaxValue
var bestRank = 0
var bestLambda = -1.0
var bestNumIter = -1
for (rank <- ranks; lambda <- lambdas; numIter <- numIters) {
	val model = ALS.train(training, rank, numIter, lambda)
	val validationRmse =
		computeRmse(model, validation, numValidation)
	
	if (validationRmse < bestValidationRmse) {
		bestModel = Some(model)
		bestValidationRmse = validationRmse
		bestRank = rank
		bestLambda = lambda
		bestNumIter = numIter
	}
}

\end{lstlisting}

Riveillä 2-11 valmistellaan opetus-, validaatio- sekä testidatat.
Opetusdatan osuus koko aineistosta on 60-, validaatiodatan 20- ja testidatan 20-prosenttia.
Rivillä 4 opetusdataan lisätään omat henkilökohtaiset arvostelut käyttäen RDD:n union funktiota, joka yhdistää kaksi erillistä RDD:tä toisiinsa.
Riveillä 21-33 suoritetaan varsinainen mallin opetus.
Opetus tapahtuu niin, että opetetaan muutama versio mallista ja valitaan malleista paras käyttäen RMSE-metriikkaa.
Koodin tasolla opetus tapahtuu käyttäen MLlib / ALS kirjaston funktiota $train$, joka ottaa sisääntulonaan $ratings$, $rank$, $iterations$ sekä $lambda$ parametrit:

\begin{itemize}
	\item $ratings$ on RDD Rating olioita, jotka sisältävät käyttäjän tunnisteen, elokuvan tunnisteen ja suosituksen
	\item $rank$ on piilevien ominaisuuksien sisällytettävä määrä
	\item $iterations$ on ALS algoritmin iteraatioiden määrä
	\item $lambda$ on regularisaatio-parametri, jolla yritetään ehkäistä mallin ylioppimista
\end{itemize}

Eräässä tutkimuksessa \cite{miryala17} on tutkittu parhaita parametreja ALS-algoritmille ja päädytty lambda-arvoon 0.1 sekä iteraatioiden määrään 20.
Parhautta on tutkittu RMSE-metriikan kautta ja kyseisillä parametreilla RMSE saadaan pienimmilleen eli mallin voidaan sanoa sovittuvan parhaiten opetusdataan.
Tutkimuksessa oltiin päädytty arvoon 0.819942, kun taas paras itse opetettu malli päätyi RMSE arvoon 0.807167.
Omassa opetuksessa eroavaisuuksina olivat tämän hetken lähin vastaava datasetti, joka ei ollut aivan niin suuri kuin tutkimuksessa käytetty, myös opetus datojen suhde oli hieman eri, sillä oman toteutuksen RMSE-validointi tarvitsi oman osansa datasta, olisi tietysti voitu käyttää validointiin myös samaa dataa kuten ilmeisesti tutkimuksessa oli tehty tai sitten RMSE oli arvioitu eri tekniikkaa käyttäen.
Tutkimuksessa paras arvo saatiin 80-20 datasettiä käyttäen ja omassa opetuksessa käytössä oli 60-20-20 datasetti.
taulukko omien koulutusten tuloksista miten lambda omissa vaikutti?

\cite{miryala17}

\section{Ennustaminen}

Alla olevassa koodilistauksessa 5.7 on esitetty suositusten ennustaminen.

\begin{lstlisting}[caption=Suositusten ennustaminen]
		
val myRatedMovieIds = personalRatings.map(_.product).toSet
val candidates = sc.parallelize(
		movies.keys.filter(!myRatedMovieIds.contains(_)).toSeq
)
val recommendations = bestModel.get
	.predict(candidates.map((0, _)))
	.collect()
	.sortBy(- _.rating)
	.take(10)

var i = 1
println("Movies recommended for you:")
recommendations.foreach { r =>
	println("%2d".format(i) + ": " + movies(r.product))
	i += 1
}
\end{lstlisting}

Yllä olevassa koodilistauksessa haetaan henkilökohtaiset suositukset käyttämällä mallin $predict$ metodia, joka ottaa parametrinaan mahdollisten elokuvien joukon.
Mahdollisilla elokuvilla tarkoitetaan elokuvia joita käyttäjä ei ole vielä nähnyt, eli ne eivät sisälly $personalRatings$ muuttujan sisältämiin elokuviin.

\section{Apufunktiot}

Alla olevassa kooodilistauksessa 5.8 esitetään käytetyt apufunktiot.

\begin{lstlisting}[caption=Apufunktiot]
def computeRmse(
		model: MatrixFactorizationModel,
		data: RDD[Rating],
		n: Long
): Double = {
	val predictions: RDD[Rating] =
		model.predict(data.map(x => (x.user, x.product)))
	val predictionsAndRatings =
		predictions.map(x => ((x.user, x.product), x.rating))
			.join(data.map(x => ((x.user, x.product), x.rating)))
			.values
	
	math.sqrt(
		predictionsAndRatings
			.map(x => (x._1 - x._2) * (x._1 - x._2))
			.reduce(_ + _) / n
	)
}
\end{lstlisting}

Yllä olevassa koodilistauksessa esitetään apufunktio $computeRMSE$, jonka avulla evaluoidaan opetetun mallin virhettä.

\end{document}