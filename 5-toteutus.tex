\documentclass[main.tex]{thesis.tex}
\begin{document}

\chapter{Toteutus}

GroupLens Research on ker‰nnyt ja laittanut saataville vertailuaineistoja MovieLens -sivustolta.
Aineistot on ker‰tty useiden aikajaksojen aikana, riippuen aineiston koosta.
MovieLens 20M aineisto sis‰lt‰‰ 20 000 000 (kaksikymment‰ miljoonaa) arviota, jotka ovat antaneet 138 000 k‰ytt‰j‰‰ 27 000 elokuvalle.
MovieLens 20M aineisto koostuu $movies.csv$ and $ratings.csv$ tiedostoista.

\captionof{table}{movies.csv}
\begin{tabular}{lll}
	movieId & title & genres \\ \hline
	1 & Toy Story (1995) & Adventure|Animation|Children \\
	2 & Jumanji (1995) & Adventure|Children|Fantasy \\
	3 & Grumpier Old Men (1995) & Comedy|Romance \\
	4 & Waiting to Exhale (1995) & Comedy|Drama|Romance \\
	5 & Father of the Bride Part II (1995) & Comedy \\
	6 & Heat (1995) & Action|Crime|Thriller \\
	7 & Sabrina (1995) & Comedy|Romance \\
	8 & Tom and Huck (1995) & Adventure|Children \\
	9 & Sudden Death (1995) & Action \\
	10 & GoldenEye (1995) & Action|Adventure|Thriller \\
\end{tabular}

\captionof{table}{ratings.csv}
\begin{tabular}{llll}
	userId & movieId & rating & timestamp \\ \hline
	1 & 31 & 2.5 & 1260759144 \\
	1 & 1029 & 3.0 & 1260759179 \\
	1 & 1061 & 3.0 & 1260759182 \\
	1 & 1129 & 2.0 & 1260759185 \\
	1 & 1172 & 4.0 & 1260759205 \\
	1 & 1263 & 2.0 & 1260759151 \\
	1 & 1287 & 2.0 & 1260759187 \\
	1 & 1293 & 2.0 & 1260759148 \\
	1 & 1339 & 3.5 & 1260759125 \\
\end{tabular}

Toteutuksessa k‰ytettiin RDD-pohjaista rajapintaa, sill‰ dataset-pohjainen rajapinta ei ole viel‰ t‰ysin toiminnallinen yhteisˆllisen suodatuksen ongelmille.
Aineiston lataaminen voidaan tehd‰ dataset-rajapintaa hyˆdynt‰en, mutta varsinaisen suositus t‰ytyy tehd‰ RDD-rajapintaa k‰ytt‰en.
Dataset-rajapinta tarjoaa useita parannuksia, kuten esimerkiksi yksinkertaisemman tiedon lataamisen.

\section{MovieLensRecommendation.scala}

Ensimm‰inen askel itsen‰isen Spark-sovelluksen rakentamisessa on tehd‰ oikeanlainen kansiorakenne ja luoda $<PROJEKTI>.sbt$ niminen tiedosto, jossa kuvaillaan sovelluksen riippuvuudet.
Itsen‰isell‰ Spark-sovelluksella tarkoitetaan k‰yttˆvalmista $JAR$-tiedostoa (Java ARchive), joka voidaan jakaa Spark-klusterille ja se sis‰lt‰‰ sek‰ koodin ett‰ kaikki riippuvuudet.
Tiedostomuoto $.sbt$ viittaa SBT (Scala Build Tool) nimiseen ohjelmaan, joka on k‰‰nnˆstyˆkalu Scala, Java ja C++-kielille \cite{sbt}.
SBT:n avulla l‰hdekoodi saadaan sek‰ k‰‰nnetty‰ ett‰ paketoitua JAR:iksi.

Sovelluksia voidaan ottaa k‰yttˆˆn klusterissa spark-submit tyˆkalun avulla, joka mahdollistaa Sparkin kaikkien tuettujen klusterinhoitajien k‰ytt‰misen yhtein‰isen k‰yttˆliittym‰n kautta. ONKO TARPEELLINEN TIETO LAINKAAN?
T‰m‰n tyˆn toteutuksen "klusteri" tulee sis‰lt‰m‰‰n vain master noodin sek‰ yhden worker noodin, mutta periaatteessa kyseess‰ on kuitenkin klusteri, vain eritt‰in pieni sellainen.

\begin{lstlisting}[caption=Sovelluksen paketointi sbt tyˆkalulla,language=sh]
sbt package
\end{lstlisting}

\begin{lstlisting}[caption=Sovelluksen k‰yttˆˆnotto klusterissa,language=sh]
spark-submit movielens-recommendations_2.11-1.0.jar
\end{lstlisting}

~

Alla olevassa esimerkiss‰ 4.3 ladataan tyˆss‰ k‰ytetyt suositukset RDD rajapintaa k‰ytt‰en.

\lstset {
	language=Scala,
	basicstyle=\footnotesize,
	numbers=left,
	stepnumber=1,
	showstringspaces=false,
	tabsize=2,
	breaklines=true,
	breakatwhitespace=false,
}

\begin{lstlisting}[caption=Suositusten lataaminen RDD rajapintaa k‰ytt‰en]
val ratings = sc.textFile("ml-latest-small/ratings.csv")
  .filter(arr => arr(0) != "userId")
  .map { line =>
    val fields = line.split(",")
    val timestamp = fields(3).toLong % 10
    val userId = fields(0).toInt
    val movieId = fields(1).toInt
    val rating = fields(2).toDouble 

    (timestamp, Rating(userId, movieId, rating))
  }
\end{lstlisting}

~

Alla olevassa esimerkiss‰ 4.4 ladataan tyˆss‰ k‰ytetyt suositukset Dataset-rajapintaa k‰ytt‰en.

\lstset {
	language=Scala,
	basicstyle=\footnotesize,
	numbers=left,
	stepnumber=1,
	showstringspaces=false,
	tabsize=2,
	breaklines=true,
	breakatwhitespace=false,
}

\begin{lstlisting}[caption=Suositusten lataaminen Dataset rajapintaa k‰ytt‰en]
val ratings = spark.read.csv("ml-latest-small/ratings.csv")
	.filter(arr => arr(0) != "userId")
	.map { fields =>
		val userId = fields(0).asInstanceOf[String].toInt
		val movieId = fields(1).asInstanceOf[String].toInt
		val rating = fields(2).asInstanceOf[String].toFloat
		val timestamp = fields(3).asInstanceOf[String].toDouble % 10

		Rating(userId, movieId, rating, timestamp)
  }
\end{lstlisting}

~

Alla olevassa listauksessa on esitetty toteutetun suositteluj‰rjestelm‰n implementaatio.

\lstset {
	language=Scala,
	basicstyle=\footnotesize,
	numbers=left,
	stepnumber=1,
	showstringspaces=false,
	tabsize=2,
	breaklines=true,
	breakatwhitespace=false,
}

- datan lataaminen
	- siisti‰ ett‰ EMR klusterissa voi ladata suoraan S3 bucketista tiedostot
- siistiminen?
- opetus
	- parametrien arvoille selitys ja ett‰ paperin mukaan
- ennustaminen
- apufunktiot ja selitys jos tarpeen

\begin{lstlisting}[caption=Aineiston lataaminen]
// ladataan omat suositukset
val personalRatings = sc.textFile("s3n://bucket/personalRatings.txt")
	.map { line =>
	  val fields = line.split(",")
	  Rating(fields(0).toInt, fields(1).toInt, fields(2).toDouble)
	}.filter(_.rating > 0.0)
		
// ladataan suositukset
val ratings = sc.textFile("s3n://bucket/ratings.csv")
  .filter(!isHeader("userId")(_))
  .map { line =>
		val fields = line.split(",")
		val timestamp = fields(3).toLong % 10
		val userId = fields(0).toInt
		val movieId = fields(1).toInt
		val rating = fields(2).toDouble

		(timestamp, Rating(userId, movieId, rating)
  }
	
// ladataan elokuvat
val movies = sc.textFile("s3n://bucket/movies.csv")
	.filter(!isHeader("movieId")(_))
	.map { line =>
		val fields = line.split(",")
		(fields(0).toInt, fields(1))
	}.collect().toMap
	
\end{lstlisting}

Alla olevassa esimerkiss‰...

\begin{lstlisting}[caption=Aineiston lataaminen]

val numRatings = ratings.count
val numUsers = ratings.map(_._2.user).distinct.count
val numMovies = ratings.map(_._2.product).distinct.count

val numPartitions = 4
val training = ratings.filter(x => x._1 < 6)
	.values
	.union(personalRatingsRDD)
	.repartition(numPartitions)
	.cache()
val validation = ratings.filter(x => x._1 >= 6 && x._1 < 8)
	.values
	.repartition(numPartitions)
	.cache()
val test = ratings.filter(x => x._1 >= 8).values.cache()

val numTraining = training.count()
val numValidation = validation.count()
val numTest = test.count()

/* training */

val ranks = List(8, 12)
val lambdas = List(1.0, 10.0)
val numIters = List(10, 20)
var bestModel: Option[MatrixFactorizationModel] = None
var bestValidationRmse = Double.MaxValue
var bestRank = 0
var bestLambda = -1.0
var bestNumIter = -1
for (rank <- ranks; lambda <- lambdas; numIter <- numIters) {
	val model = ALS.train(training, rank, numIter, lambda)
	val validationRmse =
			computeRmse(model, validation, numValidation)
	
	if (validationRmse < bestValidationRmse) {
			bestModel = Some(model)
			bestValidationRmse = validationRmse
			bestRank = rank
			bestLambda = lambda
			bestNumIter = numIter
	}
}

val testRmse = computeRmse(bestModel.get, test, numTest)

\end{lstlisting}

Alla olevassa esimerkiss‰...

\begin{lstlisting}[caption=Aineiston lataaminen]

		
val myRatedMovieIds = personalRatings.map(_.product).toSet
val candidates = sc.parallelize(
		movies.keys.filter(!myRatedMovieIds.contains(_)).toSeq
)
val recommendations = bestModel.get
	.predict(candidates.map((0, _)))
	.collect()
	.sortBy(- _.rating)
	.take(10)

var i = 1
println("Movies recommended for you:")
recommendations.foreach { r =>
	println("%2d".format(i) + ": " + movies(r.product))
	i += 1
}
\\
\end{lstlisting}

Alla olevassa esimerkiss‰...

\begin{lstlisting}[caption=Apufunktiot]

	
def isHeader(id: String, line: String): Boolean = line.contains(id)

/** Compute RMSE */
def computeRmse(
		model: MatrixFactorizationModel,
		data: RDD[Rating],
		n: Long
): Double = {
	val predictions: RDD[Rating] =
		model.predict(data.map(x => (x.user, x.product)))
	val predictionsAndRatings =
		predictions.map(x => ((x.user, x.product), x.rating))
			.join(data.map(x => ((x.user, x.product), x.rating)))
			.values
	
	math.sqrt(
		predictionsAndRatings
			.map(x => (x._1 - x._2) * (x._1 - x._2))
			.reduce(_ + _) / n
	)
}
\end{lstlisting}

Rivill‰ 1 tuodaan saataville kaikki recommendation paketin sis‰lt‰m‰t kent‰t tai metodit k‰ytt‰en $import$ avainsanaa.
Rivill‰ 3 m‰‰ritell‰‰n MovieLensALS niminen objekti.
Objekti on nimetty instanssi joka sis‰lt‰‰ j‰seni‰ kuten kentti‰ (field) sek‰ metodeita (method).
Rivill‰ 4 on m‰‰ritelty $main$ funktio tarkoittaa sit‰, ett‰ m‰‰ritelty objekti $MovieLensALS$ on ohjelman aloituspiste (entry point) sill‰ $main$ funktio sis‰lt‰‰ tietynlaisen allekirjoituksen eli tietynlaiset parametrit.
Riveill‰ 6-9 luodaan $SparkConf$ objekti, jonka avulla luodaan ohjelman k‰yttˆˆn uusi $SparkContext$ objekti. $SparkContext$ objektin avulla p‰‰st‰‰n k‰siksi Sparkin sis‰isiin toiminnallisuuksiin.
Riveill‰ 11-17 ladataan henkilˆkohtaiset suositukset tekstitiedostosta nimelt‰ $personalRatings.txt$, pilkotaan tiedoston rivit pilkun kohdalta ja luodaan uusia $Rating$ objekteja yht‰ monta, kuin tiedostossa on rivej‰.
Rivill‰ 19 ladatut suositukset muutetaan viel‰ RDD (Resilient Distributed Dataset) muotoiseksi k‰ytt‰en $sc.parallelize$ funktiota.
Funktiolle annettava toinen parametri tarkoittaa hajautuksen m‰‰r‰‰, eli kuinka monelle solmulle klusterissa tiedosto halutaan hajauttaa.
Riveill‰ 22-36 luodaan RDD oliot $ratings$ ja $movies$ lataamalla kaksi erillist‰ csv tiedostoa.
Tiedostoista suodatetaan ensin pois otsikkorivit k‰ytt‰en $isHeader$ apufunktiota.
T‰m‰n j‰lkeen tiedosto k‰yd‰‰n l‰pi rivi kerrallaan ja p‰tkit‰‰n pilkulla erotetut arvot taulukkoon k‰ytt‰en Scalan String luokan sis‰‰nrakennettua $split$ funktiota.
T‰m‰n j‰lkeen taulukossa olevista arvoista muodostetaan Tupleja.
Riveill‰ 44-54 valmistellaan opetus, validaatio sek‰ testidatat.
Rivill‰ 47 opetusdataan lis‰t‰‰n omat henkilˆkohtaiset arvostelut k‰ytt‰en RDD:n union funktiota.
Riveill‰ 64-83 suoritetaan varsinainen mallin opetus.
Opetus suoritetaan niin, ett‰ opetetaan muutama versio mallista, ja lopuksi valitaan opetetuista malleista paras k‰ytt‰en RMSE-metriikkaa mittarina.
Varsinainen mallin opetus tehd‰‰n k‰ytt‰en ALS kirjaston funktiota $train$ ja tarkemmin sanottuna $train$ funktion ylikuormitettua versiota, joka ottaa sis‰‰ntulonaan $ratings$, $rank$, $iterations$ sek‰ $lambda$ parametrit.
Ratings on RDD Rating olioita, jotka sis‰lt‰v‰t k‰ytt‰j‰n id:n, elokuvan id:n ja suosituksen.
Rank tarkoittaa piilevien ominaisuuksien sis‰llytett‰v‰‰ m‰‰r‰‰.
Iterations tarkoittaa ALS algoritmin iteraatioiden m‰‰r‰‰.
Lambda tarkoittaa regularisaatio parametria, jolla yritet‰‰n ehk‰ist‰ mallin ylioppimista.
Riveill‰ 89-102 haetaan henkilˆkohtaiset suositukset k‰ytt‰m‰ll‰ mallin $predict$ metodia, joka ottaa parametrinaan mahdollisten elokuvien joukon.
Mahdollisilla elokuvilla tarkoitetaan elokuvia joita k‰ytt‰j‰ ei ole viel‰ n‰hnyt, eli ne eiv‰t sis‰lly $personalRatings$ muuttujan sis‰lt‰miin elokuviin.
Rivill‰ 105 kutsutaan lopuksi $sparkContext$ objektin $stop$ funktiota, jolla kerrotaan ett‰ laskenta on suoritettu loppuun.
Rivill‰ 108 m‰‰ritell‰‰n apufunktio $isHeader$, jota k‰ytet‰‰n apuna suodattamaan l‰htˆaineistosta ei halutut rivit pois.
Riveill‰ 111-128 m‰‰ritell‰‰n apufunktio $computeRMSE$, jonka avulla evaluoidaan opetetun mallin virhett‰.

\end{document}