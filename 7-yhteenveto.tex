\documentclass[main.tex]{thesis.tex}
\begin{document}

\chapter{Yhteenveto}

Tässä kappaleessa esitetään yhteenveto.

\section{Johtopäätökset}

Suosittelujärjestelmän rakentamiseen on olemassa monia mahdollisia toteutusvaihtoehtoja.
Apache Spark vaikutti mielenkiintoiselta opiskelukohteelta ja tulevaisuuden kannalta hyödylliseltä.
Scala ohjelmoinnin oppiminen vaikutti myöskin teknologian valintaan.
AWS-palveluiden tuntemus on varsin hyödyllinen taito kaikkinensa, ja tässä työssä pieni palanen tuli opittua sieltäkin maailmasta lisää.
Olemassaolevien suosittelujärjestelmien tai analytiikkajärjestelmien evaluointi tulisi suorittaa ennen suosittelujärjestelmän valintaa.
Suuremman datasetin käyttäminen, sekä isomman arvostelumäärän tarjoaminen järjestelmälle voisi parantaa tuloksia.

\section{Tulevaa työtä}

MLlib-kirjastoa voitaisiin tutkia uudestaan siinä vaiheessa, kun Dataset-rajapintaa voidaan käyttää MLlib:n kanssa.
Toteutusta yritettiin myös Dataset-rajapintaa hyväksikäyttäen, mutta kaikki ominaisuudet eivät olleet tuolloin vielä käytössä.

Yhteisöllistä suodatusta voisi tietenkin käyttää muuhunkin tarkoitukseen kuin elokuvien suositteluun, ja olisikin mielenkiintoista tutkia myös jotain muuta ongelmaa ja soveltaa siihen ALS-algoritmia.

Verratun tutkimuksen mukaista mallia voitaisiin vielä tutkia lisää jos selviäisi keino, jolla RMSE:tä arvioitiin.
Tällä tarkoitetaan sitä että oliko 60-40 datasetistä myös testidatasta osa käytetty validointiin kuten omassa toteutuksessa 60-20-20 jaolla.
Mikäli olisi rajattomat AWS resurssit käytössä X aikaa niin voisi helposti testailla montaa muutakin kombinaatiota.

spark-submit työkalun sielunelämää voisi tutkia lisää, olisi mielenkiintoista ymmärtää että miten klusterinhallinta toimii, miten spark jobeja jaetaan eri noodeille sekä erityisesti miten ne paljon puhutut sulkeumat toteutuvat käytännössä, voitaisiin testailla akkumulaattorin käyttöä noodien välillä.
Nykyisellään voisi olettaa, että koska spark kontekstin luomisessa ei käytetty ollenkaan parametreja klusterin kokoon liittyen, niin spark-submit haeskelee ne itse EMR klusterista, sillä se on kuitenkin hadoop klusteri.

\end{document}